# -*- coding: utf-8 -*-
"""BTCUSD  CAPSTONE PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xzbMI8x3UW7DJL0OkfAvknZv7ETneGwQ

# **Project Title:**

### Building AI-powered Trading Bot to Predict Bitcoin Price Movements Using Deep Sequential Models and Ensemble Learning
### A Comparative Study of LSTM, GRU, Transformer, and XGBoost

### **Problem Statement**

The extreme volatility and 24/7 nature of the Bitcoin (BTC/USD) market pose significant challenges for traders and investors, who must make rapid and informed decisions to capitalize on short-term price fluctuations.

 Traditional analytical tools often fall short in capturing complex market dynamics, resulting in missed opportunities or increased risk exposure.

This project aims to develop an AI-powered trading bot capable of accurately predicting the short-term direction and strength of Bitcoin price movements. By leveraging deep sequential models (LSTM, GRU, Transformer) and ensemble learning techniques (e.g., XGBoost), the system will generate real-time, multi-class trading signals.

 The ultimate goal is to enhance decision-making in crypto trading by combining financial data, technical indicators, and predictive modeling to support robust and adaptive trading strategies in highly dynamic market conditions.

### **Main Objective**

Design and implement an AI-driven trading system capable of forecasting near-term BTC/USD price changes and issuing real-time, actionable trade signals using advanced deep learning and ensemble techniques.

### **Specific Objectives**

***1.	Data Acquisition & Preparation***

Gather and standardize multi-timeframe (M15, H1, H4, Daily) historical price data for Bitcoin from 2017 to 2025, building a unified dataset suitable for machine learning tasks.

***2.	Feature Engineering***

Create and extract informative indicators such as technical metrics and time-lagged variables that effectively capture temporal relationships and price behavior patterns.

***3.	Model Development & Evaluation***

Train deep learning models (LSTM, GRU, Transformer) and machine learning algorithms (XGBoost), and assess their performance in predicting short-term market direction.

***4.	Model Integration via Ensembles***

Combine individual model outputs using ensemble strategies to boost prediction reliability, reduce error margins, and maintain performance across diverse market environments.

***5.	Back testing and Strategy Validation***

Simulate trading outcomes using tools like Back trader to validate the system’s predictive capability and profit potential under historical market conditions.

***6.	Live Deployment***

Integrate the trained ensemble into MetaTrader 5 and run it on a VPS for continuous real-time prediction, signal generation, and trade execution.

***7.	Monitoring & Concept Drift Handling***

Track the bot’s performance after deployment and apply adjustments when data patterns shift due to evolving market conditions.

***8.	Automation & Retraining (MLOps)***

Implement an automated pipeline to regularly retrain models, manage versions, and ensure consistency in trading logic over time.

***9.	Practical Value Demonstration***

Showcase how the system enhances trade quality, profitability, and decision-making resilience especially during volatile or trending markets.

## **Business Understanding**

Cryptocurrency markets, known for their volatility and unpredictability, present both substantial opportunities and risks for traders and investors.
Even minor improvements in prediction accuracy can lead to significant financial gains or reduced risks.

The primary objective of this project is to predict short-term Bitcoin (BTC/USD) price movements using machine learning and deep learning models.

Specifically, the project aims to accurately forecast the direction and extent of Bitcoin price changes in the near future to facilitate improved trading decisions.

This research directly applies to the fintech industry, with particular relevance to algorithmic trading, cryptocurrency markets, and quantitative finance. The intended audience comprises individual crypto traders and investors, algorithmic trading firms, financial analysts, and data scientists who focus on financial applications of machine learning.

If successful, the predictive model developed through this research could significantly automate cryptocurrency trading strategies, minimize human error, and enhance profitability.

The results could also serve as a core component in broader AI-powered trading systems, thereby facilitating more informed, efficient, and rapid decision-making within financial markets.

This project is informed by prior research in time series modeling, deep sequential data learning, and financial forecasting. It specifically leverages studies comparing LSTM and GRU models for stock prediction, the application of Transformer models to financial time series data, and the effectiveness of gradient boosting methods such as XGBoost in volatile market contexts.

Moreover, it integrates domain knowledge related to technical indicators, cryptocurrency market behaviors, and relevant evaluation metrics in financial prediction tasks.

The underlying motivation stems from the challenges and potential rewards presented by cryptocurrency markets, where traditional forecasting models often struggle due to the inherently chaotic and non-linear nature of the price data.

This project seeks to utilize advanced deep learning and ensemble methods to gain predictive advantages in this complex and dynamic market environment.


## **Data Understanding**

The project utilizes historical OHLCV (Open, High, Low, Close, Volume) data for BTC/USD, collected across multiple timeframes including 15-minute (M15), hourly (H1), 4-hour (H4), and daily intervals.

Data spans from May 8, 2017, to March 31, 2025. The dataset initially consists of separate dataframes for each timeframe, which will be merged into a unified dataset containing approximately 239,000 rows.

Raw data is sourced directly from the Tickstory database through a structured extraction process. The primary columns available in the dataset are: timestamp, Open, High, Low, Close, and Volume.

Future phases of the project intend to integrate fundamental news data via financial news APIs.

The data is stored in CSV format. Variables include timestamps (datetime format) and OHLCV features (floats). Exploratory data analysis (EDA) will focus on understanding class balance, analyzing volatility patterns, and identifying feature correlations.

Preprocessing steps include merging datasets across different timeframes, handling missing values, generating additional technical indicators, creating lag features, encoding temporal variables, and scaling numeric features.

## **Algo Evaluation, Performance & Metrics of Success.**

***1. Profitability Metrics***

(Quantifies financial gains/losses)
•                  Total Return (%):

Net profit/loss over the trading period, expressed as a percentage of initial capital.

•                  Annualized Return (%):

Total return adjusted to an annual rate for benchmark comparison (e.g., vs. Bitcoin’s buy-and-hold return).

•                  Sharpe Ratio:

Risk-adjusted return (return per unit of volatility). A ratio >1 indicates strong performance in volatile markets.

•                  Sortino Ratio:

Risk-adjusted return focusing on downside volatility (critical for Bitcoin’s asymmetric risk).

•                  Maximum Drawdown (%):

Largest peak-to-trough decline in capital (e.g., a 40% drawdown signals high risk).

***2. Model Performance Metrics***

(Evaluates predictive accuracy of trading signals)

•                  Precision:

 % of correct "buy" signals out of all predicted "buy" signals. High precision reduces false positives (e.g., avoiding unprofitable trades).

•                  Recall:

% of actual "buy" opportunities correctly identified. High recall minimizes missed profitable trades.

•                  F1-Score:

Harmonic mean of precision and recall. Balances trade-offs between false positives and missed opportunities.

•                  Accuracy:

Overall % of correct predictions. Less meaningful in imbalanced markets (use with caution).

***3. Risk Management Metrics***

(Measures strategy resilience)

•                  Volatility (Standard Deviation):

Dispersion of returns; Bitcoin’s inherent volatility (~80% annualized) demands tight control.

•                  Win Rate (%):

% of profitable trades. A high rate (>60%) requires a favorable risk-reward ratio (>1.5).

•                  Value at Risk (VaR):

Maximum potential loss at a 95% confidence interval (e.g., “5% loss in 24 hours”).


***4. Trading Efficiency Metrics***

(Execution quality)

•                  Slippage (%):

 Difference between intended and executed trade prices (critical in Bitcoin’s illiquid markets).

•                  Transaction Cost Ratio:

 Fees + slippage as a % of returns. Directly impacts net profitability.

•                  Execution Speed (ms):

Time between signal generation and trade execution (latency <100ms for high-frequency strategies).


***5. Strategy Robustness Metrics***
(Long-term viability)

•                  Profit Factor (Gross Profit / Gross Loss):

A ratio >1.5 indicates sustainable profitability.

•                  Expectancy per Trade:

Average profit per trade (factoring in win rate and risk-reward). Must be positive.

•                  Drawdown Duration:

 Time to recover from peak losses. Long durations signal overfitting.


***6. Benchmark Comparison***

(Competitive analysis)

•                  Alpha:

Excess return vs. Bitcoin’s price. Positive alpha = added value over passive holding.

•                  Beta:

Correlation to Bitcoin’s price movements. Beta <1 implies lower volatility.

•                  Outperformance vs. HODL:

Justifies active trading over passive investment.


***7. Operational Metrics***

(Technical reliability)

•                  Uptime (%):

% of time the bot is live. Downtime during volatility = missed profits.


•                  Latency (ms):

Delay in data processing and order execution (critical for arbitrage).


***8. Validation & Testing***

(Avoiding overfitting)

•                  Out-of-Sample Testing:

Performance on unseen data (e.g., 2023 data if trained on 2018–2022).

•                  Walk-Forward Analysis:

Rolling backtest windows to simulate live adaptability.

### Key Integration Notes.

•                  Model metrics (R², precision, F1-score) validate the
algorithm’s predictive edge, which directly impacts profitability and risk metrics.

•                  Accuracy is less useful in Bitcoin trading due to imbalanced datasets (e.g., few "buy" signals in bear markets). Prioritize precision/recall.

•                  Regression metrics (MAE, RMSE) ensure price predictions are reliable enough to justify trades.

#  Success Metrics — BTC/USD Directional Model

This model predicts one of five directional classes. Success is defined by accuracy, consistency, and deployability of signals.

---

###  **1. Class-Level Performance**

- **F1-Score (Classes 1–4)** ≥ 0.60  
  → Strong, balanced performance on directional signals.

- **Precision (Strong Bullish/Bearish: 2 & 4)** ≥ 0.65  
  → We trust the model when it signals strong moves.

- **Recall (Strong Bullish/Bearish)** ≥ 0.60  
  → Model must reliably detect big moves.

- **Directional Confusion** ≤ 10%  
  → Rarely mistakes bullish for bearish or vice versa.

---

### **2. Generalization & Balance**

- **Validation Accuracy** ≥ 55%  
  → Must outperform randomness (20%) and naïve rules.

- **Train–Val Accuracy Gap** ≤ 3%  
  → Indicates generalization, not overfitting.

- **Class F1 Variance** ≤ 0.10  
  → Performance should be consistent across classes.

---

###  **3. Signal Quality**

- **Prediction Latency** ≤ 50ms  
  → Fast enough for real-time signal generation.

- **Signal Confidence Gap (Top 1 – Top 2)** ≥ 0.15  
  → 80% of predictions should be clearly confident.

- **Signal Volume** ≥ 2,000/month  
  → The model must generate a usable number of trades.

---

### 📈 **4. Strategy Outcomes (If Traded)**

- **Annualized Return** ≥ 15%  
- **Sharpe Ratio** ≥ 1.5  
- **Max Drawdown** ≤ 25%  
- **Hit Rate (Win%)** ≥ 55%

---

## **Importing Datasets**
"""

# Loading the 4 datasets. ie M15, H1, H4, Daily

import pandas as pd

m15 = pd.read_csv("BTCUSDM15.csv")
h1 = pd.read_csv("BTCUSDH1.csv")
h4 = pd.read_csv("BTCUSDH4.csv")
daily= pd.read_csv("BTCUSD Daily.csv")

"""## **Renaming Columns**


"""

"""
Since all the datasets have Open, Low, High, Close, Volume(OHLCV) and TImestamp,
We will rename the OHLCV columns in H1, H4 and the daily.
"""
# Rename H1 columns
h1.rename(columns={
    'Open': 'H1_Open', 'High': 'H1_High', 'Low': 'H1_Low',
    'Close': 'H1_Close', 'Volume': 'H1_Volume'
}, inplace=True)

# Rename H4 columns
h4.rename(columns={
    'Open': 'H4_Open', 'High': 'H4_High', 'Low': 'H4_Low',
    'Close': 'H4_Close', 'Volume': 'H4_Volume'
}, inplace=True)

# Rename H4 columns
daily.rename(columns={
    'Open': 'Daily_Open', 'High': 'Daily_High', 'Low': 'Daily_Low',
    'Close': 'Daily_Close', 'Volume': 'Daily_Volume'
}, inplace=True)

print("✅ Column renaming complete!")

# reviewing to make sure the renaming is complete

print("M15 DATASET")

print(m15.head())

print("H1 DATASET")

print(h1.head())

print("H4 DATASET")

print(h4.head())

print("Daily DATASET")

print(daily.head())

"""As seen the renaming is complete and the dataset is ready for merging

## **Data Merging**

Since we already have M15, H1, H4 and Daily datasets the first step is to combine them into a single dataset where:

M15 is the base timeframe.

H1 values are merged into every 4 M15 rows.

H4 values are merged into every 16 M15 rows.

Daily Values are merged into every 96 m15 rows
"""

import pandas as pd

# Step 1: Ensure Timestamp is in Proper Datetime Format
def fix_timestamp(df):
    df['Timestamp'] = pd.to_datetime(df['Date'].astype(str) + " " + df['Timestamp'].astype(str), format="%Y%m%d %H:%M:%S")
    return df

# Apply to all datasets
m15 = fix_timestamp(m15)
h1 = fix_timestamp(h1)
h4 = fix_timestamp(h4)
daily = fix_timestamp(daily)

#  Step 2: Ensure DataFrames Are Sorted Before Merging
m15.sort_values("Timestamp", inplace=True)
h1.sort_values("Timestamp", inplace=True)
h4.sort_values("Timestamp", inplace=True)
daily.sort_values("Timestamp", inplace=True)

#  Step 3: Merge H1, H4, and Daily Data Using Nearest Past Timestamp
merged_df = pd.merge_asof(m15, h1, on="Timestamp", direction="backward", suffixes=('', '_H1'))
merged_df = pd.merge_asof(merged_df, h4, on="Timestamp", direction="backward", suffixes=('', '_H4'))
merged_df = pd.merge_asof(merged_df, daily, on="Timestamp", direction="backward", suffixes=('', '_D'))

#  Step 4: Final Check - Ensure Row Count Matches M15
assert len(merged_df) == len(m15), f"Row count mismatch! Expected {len(m15)}, but got {len(merged_df)}"

print(f" Merging complete! `merged_df` row count: {len(merged_df)} (Matches `m15`)")

"""# **Visualizing the Merged Dataset**

"""

merged_df.head(5)

#  Drop the first row permanently from the merged dataset so that the data starts on a new day

merged_df = merged_df.iloc[1:].reset_index(drop=True)

merged_df.head()

"""The first column has been dropped."""

merged_df.tail()

"""The dataset is merged and consistent

### **Saving the Merged Dataset**
"""

#merged_df.to_csv("merged_btc_dataset.csv", index=False)

print("✅ `merged_btc_dataset` saved successfully!")

"""# **Basic Data Structure**"""

df = pd.read_csv("/content/drive/MyDrive/BTC_Project/Data/BTCUSD CLEAN DATA FOR MODELING.csv")

desired_cols = [
    "Open", "High", "Low", "Close", "Volume",
    "H1_Open", "H1_High", "H1_Low", "H1_Close", "H1_Volume",
    "H4_Open", "H4_High", "H4_Low", "H4_Close", "H4_Volume",
    "Daily_Open", "Daily_High", "Daily_Low", "Daily_Close", "Daily_Volume"
]

df_sliced = df[desired_cols]

# 1. Overall summary of columns, non-null counts & dtypes
df.info()

# 2. Quick look at the first (and last) rows

print("first rows by default")

print(df_sliced.head())

print("last five rows by default")

print(df_sliced.tail())      # last five rows by default

"""The dataset is consistent as seen above."""

# 3. DataFrame dimensions and column names
print("Shape:", df_sliced.shape)       # (rows, columns)
print("Columns:", df_sliced.columns)   # Index of column names

"""The dataset has around 239K rows and 20 rows as of now.

This dataframe size will ofcourse increae with feature engineering.
"""

# 4. Data types of each column
print(df_sliced.dtypes)

"""The dataset columns are all of the same data type, floats."""

# 5. Basic statistical summary for numeric columns
df_sliced.describe()           # count, mean, std, min, 25%, 50%, 75%, max
df_sliced.describe(include="all")  # also shows unique/counts for non-numeric

# 6. Missing‐value counts per column
df_sliced.isnull().sum()

"""As of now, the dataset does not have any missing values.

However, as we do feature engineering there is a likelyhood of introducing NaN values.

We will recheck this and if we introduce any missing values we will be fixing them then.
"""

# 8. A random sample of rows
df_sliced.sample(5)

"""## **Feature Engineering**

We will systematically add features in stages, ensuring each contributes to model accuracy. The following is a brief of the features we intend to engineer in this section.

### **1. Technical Indicators**

To capture **momentum, trend strength, volatility, and mean reversion**, we add:

- Moving Averages (EMA, SMA)

- Relative Strength Index (RSI)

- Average True Range (ATR)

- Bollinger Bands (BB)

- MACD (Moving Average Convergence Divergence)

- Stochastic Oscillator  

 These help in **confirming trade signals and filtering noise**.  

---

### **2. Market Structure Features (ICT/SMC)**

We will generate:

- Swing Highs & Swing Lows

- Break of Structure (BOS)
  
- Change of Character (CHoCH)

- Liquidity Zones

- Fair Value Gaps (FVG)

- Order blocks

- Mitigation

- Equal Highs/Lows

- Breaker Blocks

- Swing Count / Leg Tracker

- Premium/Discount Zones

 These features **define trend changes** and **key reaction areas for price action**.  

---

### 3. Volume Spikes – Detect Institutional Activity

### 4. Historical Volatility (HV) – Past X Periods Volatility

### 5. Market Regime Classification – Detect Trending vs. Ranging Market

### 6. Candle Pattern Features- Precision features

### 7. Smart Money confluence features
  
 This ensures **we align trades with higher timeframes for better accuracy** as opposed to relying on lower timeframes that tend to have a lot of noise.

---

## **1. Technical Indicators**

### **A. Moving Averages(EMA & SMA)**

Moving averages are trend-following indicators that smooth out price data to identify direction over time.  

- **Simple Moving Average (SMA)** calculates the unweighted average of closing prices over a specific period (e.g., 10, 50, 200).

- **Exponential Moving Average (EMA)** gives more weight to recent prices, making it more responsive to new market data.
  
- Common periods like 10, 50, and 200 are used to capture short-, medium-, and long-term trends respectively.
"""

df = merged_df

df.info()

# adding the SMA & EMA

def add_moving_averages(df, short_window=10, mid_window=50, long_window=200):
    df['SMA_10'] = df['Close'].rolling(window=short_window).mean()     # 10-period SMA
    df['SMA_50'] = df['Close'].rolling(window=mid_window).mean()       # 50-period SMA
    df['SMA_200'] = df['Close'].rolling(window=long_window).mean()     # 200-period SMA

    df['EMA_10'] = df['Close'].ewm(span=short_window, adjust=False).mean()   # 10-period EMA
    df['EMA_50'] = df['Close'].ewm(span=mid_window, adjust=False).mean()     # 50-period EMA
    df['EMA_200'] = df['Close'].ewm(span=long_window, adjust=False).mean()   # 200-period EMA

    return df

# Apply it to the dataset

df = add_moving_averages(df)

print("EMA (10,50,200) and SMA(10,50,200) successfully added to the dataset")

"""### B. **Relative Strength Index (RSI)**

RSI is a momentum oscillator that measures the speed and magnitude of recent price changes to identify overbought or oversold conditions.

It ranges from 0 to 100, with values above 70 typically indicating overbought conditions, and below 30 suggesting oversold.  

RSI is calculated using average gains and losses over a set period, commonly 14 periods.

It helps traders anticipate potential price reversals or trend continuations.
"""

#adding libraries

import numpy as np
import pandas as pd

#adding rsi period 14 to the dataset

def add_rsi(df, period=14):
    delta = df['Close'].diff(1)
    gain = np.where(delta > 0, delta, 0)
    loss = np.where(delta < 0, -delta, 0)

    avg_gain = pd.Series(gain, index=df.index).rolling(window=period, min_periods=1).mean()
    avg_loss = pd.Series(loss, index=df.index).rolling(window=period, min_periods=1).mean()

    rs = avg_gain / (avg_loss + 1e-10)  # Avoid division by zero
    df['RSI_14'] = 100 - (100 / (1 + rs))
    return df


df = add_rsi(df)

print("RSI-14 added successfully to the dataset")

"""### **C. Average True Range (ATR)**

ATR is a technical indicator that measures market volatility by averaging the true range of price movements over a specified period, typically 14 periods.

The **true range**
is the greatest of:

1. Current high minus current low  
2. Absolute value of current high minus previous close  
3. Absolute value of current low minus previous close  

ATR provides insight into how much an asset typically moves, helping traders set stop-loss levels and assess risk. We shall use ATR to conduct dynamic stop loss and tp later on in this project as we do modularization.

"""

# adding ATR to the dataset


def add_atr(df, period=14):
    high_low = df['High'] - df['Low']
    high_close = np.abs(df['High'] - df['Close'].shift(1))
    low_close = np.abs(df['Low'] - df['Close'].shift(1))

    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
    df['ATR_14'] = true_range.rolling(window=period, min_periods=1).mean()
    return df

df = add_atr(df)

print("ATR successfully added to the dataset")

"""### **D. Bollinger Bands (BB)**

BB are a volatility-based technical indicator consisting of three lines plotted on a price chart:

1. **Middle Band** – a Simple Moving Average (typically 20 periods)  
2. **Upper Band** – the SMA plus a multiple (usually 2) of the standard deviation  
3. **Lower Band** – the SMA minus the same multiple of the standard deviation  

Bollinger Bands expand and contract with market volatility and help identify overbought or oversold conditions, potential breakouts, and trend reversals.
"""

def add_bollinger_bands(df, period=20, std_dev=2):
    df['BB_Mid'] = df['Close'].rolling(window=period).mean()
    rolling_std = df['Close'].rolling(window=period).std()
    df['BB_Upper'] = df['BB_Mid'] + (rolling_std * std_dev)
    df['BB_Lower'] = df['BB_Mid'] - (rolling_std * std_dev)
    return df


df = add_bollinger_bands(df)

print("BB successfully added to the dataset")

"""### **E. MACD (Moving Average Convergence Divergence)**

MACD is a momentum indicator that reveals trend direction and strength by comparing two exponential moving averages (EMAs).  

It is calculated as:  

MACD Line = 12-period EMA – 26-period EMA
    
A Signal Line (usually 9-period EMA of the MACD Line) is then used to identify crossovers.  

MACD helps traders spot trend shifts, bullish/bearish momentum, and potential entry/exit points.
"""

def add_macd(df, short_period=12, long_period=26, signal_period=9):
    short_ema = df['Close'].ewm(span=short_period, adjust=False).mean()
    long_ema = df['Close'].ewm(span=long_period, adjust=False).mean()

    df['MACD'] = short_ema - long_ema                                # MACD Line
    df['MACD_Signal'] = df['MACD'].ewm(span=signal_period, adjust=False).mean()  # Signal Line
    df['MACD_Hist'] = df['MACD'] - df['MACD_Signal']                # Histogram (Momentum)

    return df

df = add_macd(df)

print("MACD successfully added to the dataset")

"""### **F. Stochastic Oscillator**


The Stochastic Oscillator is a momentum indicator that compares a security's closing price to its price range over a specific period, typically 14 periods.  
It is calculated using the formula:

**%K = (Current Close - Lowest Low) / (Highest High - Lowest Low) × 100**

A 3-period moving average of %K, called **%D**, is often used as a signal line.

Values above 80 suggest overbought conditions, while values below 20 indicate oversold levels — helping traders anticipate potential trend reversals.
"""

def add_stochastic_oscillator(df, k_period=14, d_period=3):
    low_min = df['Low'].rolling(window=k_period, min_periods=1).min()
    high_max = df['High'].rolling(window=k_period, min_periods=1).max()

    df['Stoch_%K'] = 100 * ((df['Close'] - low_min) / (high_max - low_min + 1e-10))  # Avoid divide-by-zero
    df['Stoch_%D'] = df['Stoch_%K'].rolling(window=d_period, min_periods=1).mean()

    return df


df = add_stochastic_oscillator(df)

print("oscillator successfully added to the dataset")

# confirming all intended technical indictors have been added to the dataset

df.columns

"""The intended Technical indicators have been added to the dataset successfully

## **2. Market Structure Features**

### **A. Swing Highs & Swing Lows**

A Swing High occurs when a candle’s high is higher than the highs of the previous and next N candles.

A Swing Low occurs when a candle’s low is lower than the lows of the previous and next N candles.

These points represent local resistance and support, and are widely used to define Break of Structure (BOS), Change of Character (CHoCH), and Liquidity Zones in market structure analysis.
"""

def add_market_structure_features(df, swing_window=3, range_window=20):
    """
    Adds both pattern-based swing points and rolling high/low context levels.

    - swing_window: N candles before/after for true swing detection
    - range_window: Window size for rolling high/low
    """

    # True Swing High/Low (structure points)
    df['Swing_High'] = df['High'][(df['High'].shift(swing_window) < df['High']) &
                                  (df['High'].shift(-swing_window) < df['High'])]

    df['Swing_Low'] = df['Low'][(df['Low'].shift(swing_window) > df['Low']) &
                                (df['Low'].shift(-swing_window) > df['Low'])]

    # Rolling High/Low (context levels)
    df['Rolling_High'] = df['High'].rolling(window=range_window).max()
    df['Rolling_Low'] = df['Low'].rolling(window=range_window).min()

    return df
df = add_market_structure_features(df, swing_window=3, range_window=20)

print("swing high, swing low, rolling high and rolling low added to the dataset")

"""### **B. Break of Structure (BOS)**

Break of Structure (BOS) occurs when price breaks above a previous Swing High in an uptrend or below a previous Swing Low in a downtrend, indicating a potential continuation of the current trend.  

It confirms the formation of a higher high in bullish structure or a lower low in bearish structure, and is commonly used in price action trading to validate trend strength, trigger entries, or mark key market phases.
"""

def detect_bos(
    df,
    swing_confirmation=5,
    pip_threshold=5,
    momentum_factor=1.2,
    confirmation_candles=2,
    atr_column='ATR_14'
):
    """
    Detects Break of Structure (BOS) using validated swing highs/lows and momentum-based filters.

    Parameters:
    - swing_confirmation: int — candles before/after to confirm a swing high/low
    - pip_threshold: float — minimum breakout beyond swing level (e.g., 5 pips = 0.5 if price is 1.x)
    - momentum_factor: float — strength of the move (vs. ATR)
    - confirmation_candles: int — number of consecutive candles to confirm BOS
    - atr_column: str — name of the ATR column in the dataframe

    Returns:
    - Modified DataFrame with 'Swing_High', 'Swing_Low', 'Prev_Swing_High', 'Prev_Swing_Low',
      'Bullish_BOS', and 'Bearish_BOS' columns
    """
    import numpy as np
    import pandas as pd

    # Step 1: Detect Structure-Based Swings
    df["Swing_High"] = np.nan
    df["Swing_Low"] = np.nan

    for i in range(swing_confirmation, len(df) - swing_confirmation):
        past_highs = df.loc[i - swing_confirmation:i - 1, "High"]
        future_highs = df.loc[i + 1:i + swing_confirmation, "High"]
        current_high = df.loc[i, "High"]

        past_lows = df.loc[i - swing_confirmation:i - 1, "Low"]
        future_lows = df.loc[i + 1:i + swing_confirmation, "Low"]
        current_low = df.loc[i, "Low"]

        if current_high > past_highs.max() and current_high > future_highs.max():
            df.loc[i, "Swing_High"] = current_high

        if current_low < past_lows.min() and current_low < future_lows.min():
            df.loc[i, "Swing_Low"] = current_low

    # Step 2: Prepare Reference Swing Levels
    df["Prev_Swing_High"] = df["Swing_High"].ffill().shift(1)
    df["Prev_Swing_Low"] = df["Swing_Low"].ffill().shift(1)

    # Step 3: Initialize BOS Columns
    df["Bullish_BOS"] = 0
    df["Bearish_BOS"] = 0

    # Step 4: BOS Detection
    for i in range(confirmation_candles, len(df)):
        atr = df.loc[i, atr_column] if atr_column in df.columns else 1

        # Bullish BOS
        bullish_break = (
            df.loc[i, "Close"] > df.loc[i, "Prev_Swing_High"] + (pip_threshold / 10)
        )
        bullish_confirm = all(
            df.loc[i - k, "Close"] > df.loc[i - k, "Prev_Swing_High"]
            for k in range(1, confirmation_candles + 1)
        )
        bullish_momentum = (df.loc[i, "Close"] - df.loc[i, "Open"]) > (momentum_factor * atr)

        if bullish_break and bullish_confirm and bullish_momentum:
            df.loc[i, "Bullish_BOS"] = 1

        # Bearish BOS
        bearish_break = (
            df.loc[i, "Close"] < df.loc[i, "Prev_Swing_Low"] - (pip_threshold / 10)
        )
        bearish_confirm = all(
            df.loc[i - k, "Close"] < df.loc[i - k, "Prev_Swing_Low"]
            for k in range(1, confirmation_candles + 1)
        )
        bearish_momentum = (df.loc[i, "Open"] - df.loc[i, "Close"]) > (momentum_factor * atr)

        if bearish_break and bearish_confirm and bearish_momentum:
            df.loc[i, "Bearish_BOS"] = 1

    return df
df = detect_bos(df)
print("✅ BOS successfully added to the dataset.")

"""### **C. Change of Character (CHoCH)**

CHoCH) is a key concept in price action and smart money trading strategies that signals a potential **trend reversal**.  

It occurs when price breaks the most recent swing in the opposite direction of the prevailing trend — often following a **lower high** in an uptrend or **higher low** in a downtrend.  

While a Break of Structure (BOS) confirms **trend continuation**, **CHoCH marks a shift**, suggesting the market may be transitioning from bullish to bearish (or vice versa).

---

### Example:
- In an **uptrend**, CHoCH is triggered when price breaks below the most recent swing low → potential bearish reversal.
- In a **downtrend**, CHoCH happens when price breaks above the most recent swing high → potential bullish reversal.

"""

def detect_choch(df):
    """
    Detects Change of Character (CHoCH) in market structure.

    Assumes 'Bullish_BOS', 'Bearish_BOS', 'Swing_High', 'Swing_Low' columns already exist.

    Adds:
    - 'CHoCH_Bullish': Bullish reversal signal (break of bearish structure)
    - 'CHoCH_Bearish': Bearish reversal signal (break of bullish structure)
    """
    df['Structure_Trend'] = None  # Track current trend state
    df['CHoCH_Bullish'] = 0
    df['CHoCH_Bearish'] = 0

    last_bullish_bos_idx = None
    last_bearish_bos_idx = None

    for i in range(1, len(df)):
        # Detect active structure based on most recent BOS
        if df.loc[i, 'Bullish_BOS'] == 1:
            df.loc[i, 'Structure_Trend'] = 'bullish'
            last_bullish_bos_idx = i
        elif df.loc[i, 'Bearish_BOS'] == 1:
            df.loc[i, 'Structure_Trend'] = 'bearish'
            last_bearish_bos_idx = i
        else:
            df.loc[i, 'Structure_Trend'] = df.loc[i - 1, 'Structure_Trend']

        # Look for CHoCH: Bullish → Bearish reversal
        if (
            df.loc[i, 'Structure_Trend'] == 'bullish'
            and last_bullish_bos_idx is not None
            and df.loc[i, 'Close'] < df.loc[i, 'Swing_Low']
            and not pd.isna(df.loc[i, 'Swing_Low'])
        ):
            df.loc[i, 'CHoCH_Bearish'] = 1
            df.loc[i, 'Structure_Trend'] = 'bearish'
            last_bearish_bos_idx = i

        # Look for CHoCH: Bearish → Bullish reversal
        if (
            df.loc[i, 'Structure_Trend'] == 'bearish'
            and last_bearish_bos_idx is not None
            and df.loc[i, 'Close'] > df.loc[i, 'Swing_High']
            and not pd.isna(df.loc[i, 'Swing_High'])
        ):
            df.loc[i, 'CHoCH_Bullish'] = 1
            df.loc[i, 'Structure_Trend'] = 'bullish'
            last_bullish_bos_idx = i

    return df

df = detect_choch(df)

print("CHoCH and structure trend successfully added to the dataset")

"""### **D. Liquidity Zones(Buy side Liquidity(BSL) & Sell Side Liquidity(SSl)**

Liquidity Zones are price areas on a chart where a high volume of pending orders (buy/sell) is likely to be triggered.

These zones represent levels where liquidity is concentrated, often acting as magnetic areas for price and are key targets for institutional moves.

---

###  Typical Types of Liquidity Zones:

1. Above Swing Highs → Buy-side liquidity (stop-losses of short sellers)
2. Below Swing Lows → Sell-side liquidity (stop-losses of long buyers)
3. Consolidation Zones / Ranges → Clustered stop orders and breakouts

---

###  Why They Matter:

- Smart money (institutions) often target liquidity zones to fill large orders or engineer reversals.
- They are commonly used for:
  - Identifying stop hunts
  - Anticipating false breakouts
  - Timing entries and exits

---
"""

#  Add Liquidity Zones (target areas)

def detect_liquidity_zones(df, buffer_pips=5):
    df['Liquidity_Zone'] = np.nan
    df['Liquidity_Type'] = None
    for i in range(len(df)):
        if not pd.isna(df.loc[i, 'Swing_High']):
            df.loc[i, 'Liquidity_Zone'] = df.loc[i, 'Swing_High'] + (buffer_pips / 10)
            df.loc[i, 'Liquidity_Type'] = 'buy'
        elif not pd.isna(df.loc[i, 'Swing_Low']):
            df.loc[i, 'Liquidity_Zone'] = df.loc[i, 'Swing_Low'] - (buffer_pips / 10)
            df.loc[i, 'Liquidity_Type'] = 'sell'
    return df

#  Add BSL / SSL Detection (liquidity taken)

def detect_bsl_ssl(df, lookback=20):
    df["Prev_Highs"] = df["High"].rolling(window=lookback).max().shift(1)
    df["Prev_Lows"] = df["Low"].rolling(window=lookback).min().shift(1)
    df["BSL"] = (df["High"] > df["Prev_Highs"]).astype(int)
    df["SSL"] = (df["Low"] < df["Prev_Lows"]).astype(int)
    return df

# Add Sniper Entry Signals (liquidity sweep + CHoCH)

def add_sniper_signals(df):
    df['Buy_Sniper'] = ((df['SSL'] == 1) & (df['CHoCH_Bullish'] == 1)).astype(int)
    df['Sell_Sniper'] = ((df['BSL'] == 1) & (df['CHoCH_Bearish'] == 1)).astype(int)
    return df

df = detect_liquidity_zones(df)   # Marks target zones above swing highs / below swing lows
df = detect_bsl_ssl(df)           # Detects when liquidity is swept (BSL / SSL)
df = add_sniper_signals(df)       # Detects sniper entries: sweep + CHoCH

print(" Liquidity zones, BSL/SSL, and sniper signals successfully added.")

"""### **E. Fair Value Gaps (FVGs)**

A Fair Value Gap is a price imbalance on the chart where the market moves so aggressively in one direction that it skips over certain price levels without properly trading through them.

This creates a visible gap between candles — typically between the **high of one candle and the low of a future candle**, leaving a “void” in the middle.

FVGs represent areas where liquidity was not fully matched, and price often returns to these levels later to **“fill the gap”**, offering key opportunities for entries, re-entries, or targets — especially in smart money and institutional trading strategies.
"""

def detect_fvg(df):
    """
    Detects bullish and bearish Fair Value Gaps (FVGs) based on a 3-candle formation.
    A gap exists when the price skips between candle 1 and candle 3, leaving an imbalance.

    Adds:
    - Bullish_FVG: 1 where bullish fair value gap exists
    - Bearish_FVG: 1 where bearish fair value gap exists
    - FVG_Low, FVG_High: Boundaries of the gap zone (for plotting or filling logic)
    """
    df['Bullish_FVG'] = 0
    df['Bearish_FVG'] = 0
    df['FVG_Low'] = np.nan
    df['FVG_High'] = np.nan

    for i in range(2, len(df)):
        # Candle indices
        c1 = i - 2
        c2 = i - 1
        c3 = i

        # Bullish FVG: gap between high of candle 1 and low of candle 3
        if df.loc[c1, 'High'] < df.loc[c3, 'Low']:
            df.loc[c3, 'Bullish_FVG'] = 1
            df.loc[c3, 'FVG_Low'] = df.loc[c1, 'High']
            df.loc[c3, 'FVG_High'] = df.loc[c3, 'Low']

        # Bearish FVG: gap between low of candle 1 and high of candle 3
        elif df.loc[c1, 'Low'] > df.loc[c3, 'High']:
            df.loc[c3, 'Bearish_FVG'] = 1
            df.loc[c3, 'FVG_Low'] = df.loc[c3, 'High']
            df.loc[c3, 'FVG_High'] = df.loc[c1, 'Low']

    return df
df = detect_fvg(df)
print("FVGs successfully added to the dataset")

"""### **F. Order Blocks (OBs)**

Order Blocks are price levels or zones on a chart where institutions (smart money) have previously placed large orders, often leading to significant price moves.
They typically form before strong breakouts, indicating where accumulation (buying) or distribution (selling) occurred.

---

###  Key Concepts:
- A **Bullish Order Block** is the last bearish candle before a strong upward move.
  
- A **Bearish Order Block** is the last bullish candle before a strong downward move.

-
- These candles mark areas where institutional traders likely entered large positions, and price often returns to these zones before continuing.

---

### Why They're Important:

- Act as support/resistance
- Used for entry zones, stop placement, or re-entries
- Help validate smart money concepts alongside CHoCH, BOS, and FVGs

---

"""

def detect_order_blocks(df, body_ratio_threshold=0.6, lookahead=5):
    """
    Detects Bullish and Bearish Order Blocks based on the last opposing candle before a strong move.

    Parameters:
    - body_ratio_threshold: float — ratio to filter strong-bodied candles (e.g. 0.6 = 60% body size)
    - lookahead: int — how many candles ahead to confirm a BOS after the OB

    Adds:
    - Bullish_OB: 1 where a bullish order block is detected
    - Bearish_OB: 1 where a bearish order block is detected
    - OB_Low, OB_High: price bounds of the order block for zone visualization
    """
    df['Bullish_OB'] = 0
    df['Bearish_OB'] = 0
    df['OB_Low'] = np.nan
    df['OB_High'] = np.nan

    for i in range(len(df) - lookahead):
        open_ = df.loc[i, 'Open']
        close = df.loc[i, 'Close']
        high = df.loc[i, 'High']
        low = df.loc[i, 'Low']
        body = abs(close - open_)
        range_ = high - low

        # Skip weak-bodied candles
        if range_ == 0 or (body / range_) < body_ratio_threshold:
            continue

        is_bearish = close < open_
        is_bullish = close > open_

        # Lookahead zone to confirm a breakout
        future_highs = df.loc[i+1:i+lookahead, 'High']
        future_lows = df.loc[i+1:i+lookahead, 'Low']

        # Bullish OB logic: last bearish candle before bullish BOS
        if is_bearish and future_highs.max() > high:
            df.loc[i, 'Bullish_OB'] = 1
            df.loc[i, 'OB_Low'] = low
            df.loc[i, 'OB_High'] = high

        # Bearish OB logic: last bullish candle before bearish BOS
        if is_bullish and future_lows.min() < low:
            df.loc[i, 'Bearish_OB'] = 1
            df.loc[i, 'OB_Low'] = low
            df.loc[i, 'OB_High'] = high

    return df
df = detect_order_blocks(df)
print("OBs successfully added to the dataset")

"""### **G. Mitigation Zones**

Description:

Mitigation happens when price returns to a previous order block (OB) and respects it

— either reversing or reacting before breaking it.

It's a key confirmation that smart money is reloading from a known zone.

"""

def detect_mitigation(df):
    df['OB_Mitigated'] = 0
    for i in range(1, len(df)):
        if df.loc[i, 'Bullish_OB'] == 0 and df.loc[i, 'Low'] <= df.loc[i - 1, 'OB_High'] and df.loc[i - 1, 'Bullish_OB'] == 1:
            df.loc[i, 'OB_Mitigated'] = 1
        elif df.loc[i, 'Bearish_OB'] == 0 and df.loc[i, 'High'] >= df.loc[i - 1, 'OB_Low'] and df.loc[i - 1, 'Bearish_OB'] == 1:
            df.loc[i, 'OB_Mitigated'] = 1
    return df

df = detect_mitigation(df)

"""### **H. Equal Highs / Equal Lows**

Description:

Equal Highs or Lows are areas where price created nearly identical highs or lows.

They indicate resting liquidity and are often swept before reversals.
"""

def detect_equal_highs_lows(df, tolerance=0.0005):
    df['Equal_High'] = 0
    df['Equal_Low'] = 0
    for i in range(1, len(df)):
        if abs(df.loc[i, 'High'] - df.loc[i - 1, 'High']) <= tolerance:
            df.loc[i, 'Equal_High'] = 1
        if abs(df.loc[i, 'Low'] - df.loc[i - 1, 'Low']) <= tolerance:
            df.loc[i, 'Equal_Low'] = 1
    return df

df = detect_equal_highs_lows(df)

"""### **I. Breaker Blocks - Failed OBs that flip (used as reversal/trap zones)**

Description:

Breaker blocks are order blocks that were invalidated (price closed beyond them),

and then price flips and retests them as a reversal zone. They often act as trap areas.

"""

def detect_breaker_blocks(df):
    df['Breaker_Block'] = 0
    for i in range(1, len(df)):
        if df.loc[i - 1, 'Bullish_OB'] == 1 and df.loc[i, 'Close'] < df.loc[i - 1, 'OB_Low']:
            df.loc[i, 'Breaker_Block'] = 1
        elif df.loc[i - 1, 'Bearish_OB'] == 1 and df.loc[i, 'Close'] > df.loc[i - 1, 'OB_High']:
            df.loc[i, 'Breaker_Block'] = 1
    return df
df = detect_breaker_blocks(df)

"""### **J. Swing Count / Leg Tracker - Number of HH-HL or LL-LH legs**

Description:

Tracks the number of consecutive bullish (HH-HL) or bearish (LL-LH) swings.

Helpful for trend maturity, timing exits, or recognizing late entries.
"""

def count_swing_legs(df):
    df['Swing_Trend'] = None
    df['Swing_Leg_Count'] = 0
    count = 0
    for i in range(1, len(df)):
        if df.loc[i, 'Bullish_BOS'] == 1:
            if df.loc[i - 1, 'Swing_Trend'] == 'bullish':
                count += 1
            else:
                count = 1
            df.loc[i, 'Swing_Trend'] = 'bullish'
        elif df.loc[i, 'Bearish_BOS'] == 1:
            if df.loc[i - 1, 'Swing_Trend'] == 'bearish':
                count += 1
            else:
                count = 1
            df.loc[i, 'Swing_Trend'] = 'bearish'
        else:
            df.loc[i, 'Swing_Trend'] = df.loc[i - 1, 'Swing_Trend']
        df.loc[i, 'Swing_Leg_Count'] = count
    return df
df = count_swing_legs(df)

"""### **K. Premium/Discount Zones - Smart Money Fair Value Zones**

Description:

Divides the current range (between most recent swing high and low) in half:

- Price above the midpoint is in premium (sell zone)
-
- Price below the midpoint is in discount (buy zone)

Smart money prefers to buy at discount and sell at premium.

"""

def add_premium_discount_zone(df):
    df['Fair_Value_Mid'] = (df['Rolling_High'] + df['Rolling_Low']) / 2
    df['Is_Premium'] = (df['Close'] > df['Fair_Value_Mid']).astype(int)
    df['Is_Discount'] = (df['Close'] < df['Fair_Value_Mid']).astype(int)
    return df

df = add_premium_discount_zone(df)

df.columns

"""### **3. Volume Spikes – Detect Institutional Activity**

A volume spike is a sudden and significant increase in trading volume compared to recent candles.

Spikes often signal institutional involvement, hidden accumulation/distribution, or the start of a strong move.
"""

def detect_volume_spikes(df, lookback=20, spike_multiplier=2.0):
    """
    Flags volume spikes when volume exceeds N × rolling average volume.

    Adds:
    - Volume_Spike: 1 if volume is spiking above recent norm
    """
    df['Avg_Volume'] = df['Volume'].rolling(window=lookback).mean()
    df['Volume_Spike'] = (df['Volume'] > df['Avg_Volume'] * spike_multiplier).astype(int)
    return df
df = detect_volume_spikes(df)

"""### **4. Historical Volatility (HV)**

Historical Volatility (HV) measures the standard deviation of returns over a past window, showing how volatile the asset has been.

It helps detect periods of compression or expansion.
"""

def calculate_historical_volatility(df, window=20):
    """
    Computes rolling standard deviation of log returns over a given window.

    Adds:
    - HV: Historical volatility (%)
    """
    df['Log_Returns'] = np.log(df['Close'] / df['Close'].shift(1))
    df['HV'] = df['Log_Returns'].rolling(window=window).std() * 100  # in percentage
    return df
df = calculate_historical_volatility(df)

"""### **5. Market Regime Classification – Trending vs. Ranging**

A market regime reflects the current price environment:

Trending: clear directional movement (up or down)

Ranging: choppy, sideways consolidation

We classify this using ATR or rolling high-low range relative to recent price movement
"""

def classify_market_regime(df, range_window=20, atr_threshold=1.0):
    """
    Classifies market regime as 'trending' or 'ranging' using ATR vs. rolling price range.

    Adds:
    - Market_Regime: 'trending' or 'ranging'
    """
    df['Price_Range'] = df['High'].rolling(window=range_window).max() - df['Low'].rolling(window=range_window).min()
    df['ATR_vs_Range'] = df['ATR_14'] / (df['Price_Range'] + 1e-10)  # avoid divide-by-zero
    df['Market_Regime'] = np.where(df['ATR_vs_Range'] > atr_threshold, 'trending', 'ranging')
    return df
df = classify_market_regime(df)

df.columns

"""### **6. Candle Pattern Features – Precision Candles**

We intend to detect the following candle stick formations:



##### 1. Bullish Engulfing
A two-candle reversal pattern where a small bearish candle is followed by a larger bullish candle that completely "engulfs" the previous one.  
 Often signals a shift from selling pressure to buying strength — especially after a downtrend.

---

##### 2. Bearish Engulfing

The opposite of bullish engulfing: a small bullish candle followed by a larger bearish candle that engulfs it.  
Indicates increasing selling pressure and potential reversal to the downside after an uptrend.

---

##### 3. Doji

A candle with a very small body (open ≈ close) and long wicks, showing indecision in the market.  
Common near turning points; signals that momentum is weakening.

---

##### 4. Inside Bar  
A candle that’s completely contained within the high and low of the previous candle.  
Often forms during consolidation; can signal upcoming breakout or trap depending on context.

---

##### 5. Pin Bar (Long Wicks)

A candle with a long wick (tail) and a small body, where the wick represents a failed move in one direction.  
Suggests rejection of a price level and a possible reversal — especially powerful near OBs or FVGs.

---

"""

def add_candle_patterns(df):
    """
    Adds common smart money candlestick patterns.
    Useful near OBs, FVGs, BOS/CHoCH areas for entry precision.
    """
    body = abs(df['Close'] - df['Open'])
    wick_top = df['High'] - df[['Close', 'Open']].max(axis=1)
    wick_bottom = df[['Close', 'Open']].min(axis=1) - df['Low']
    range_ = df['High'] - df['Low']

    df['Bullish_Engulfing'] = (
        (df['Close'] > df['Open']) &
        (df['Close'].shift(1) < df['Open'].shift(1)) &
        (df['Close'] > df['Open'].shift(1)) &
        (df['Open'] < df['Close'].shift(1))
    ).astype(int)

    df['Bearish_Engulfing'] = (
        (df['Close'] < df['Open']) &
        (df['Close'].shift(1) > df['Open'].shift(1)) &
        (df['Open'] > df['Close'].shift(1)) &
        (df['Close'] < df['Open'].shift(1))
    ).astype(int)

    df['Doji'] = (body / range_ < 0.1).astype(int)

    df['Inside_Bar'] = (
        (df['High'] < df['High'].shift(1)) &
        (df['Low'] > df['Low'].shift(1))
    ).astype(int)

    df['Pin_Bar'] = (
        ((wick_top > body * 2) & (wick_bottom < body)) |  # bearish pin
        ((wick_bottom > body * 2) & (wick_top < body))    # bullish pin
    ).astype(int)

    return df
df = add_candle_patterns(df)
print("✅ Candle stick pattern  signals added successfully.")

"""### **7. Smart Money Confluence Features – Context Awareness**

We intend to do a combination of the smart money concepts to establish confluence.

The combinations will be as follows:

OB + CHoCH

OB + FVG

OB + Volume Spike

CHoCH + SSL/BSL

FVG + Volume Spike

Sniper Confluence

These help the model and strategy understand zones with stacked smart money intent.
"""

def add_smart_money_confluence(df):
    """
    Creates binary features representing high-confluence zones based on smart money concepts.
    These are modeling-ready signals with strong institutional logic.
    """

    df['OB_CHoCH'] = ((df['Bullish_OB'] == 1) & (df['CHoCH_Bullish'] == 1) |
                      (df['Bearish_OB'] == 1) & (df['CHoCH_Bearish'] == 1)).astype(int)

    df['OB_FVG'] = ((df['Bullish_OB'] == 1) & (df['Bullish_FVG'] == 1) |
                    (df['Bearish_OB'] == 1) & (df['Bearish_FVG'] == 1)).astype(int)

    df['OB_VolSpike'] = ((df['Bullish_OB'] == 1) & (df['Volume_Spike'] == 1) |
                         (df['Bearish_OB'] == 1) & (df['Volume_Spike'] == 1)).astype(int)

    df['CHoCH_Sweep'] = ((df['CHoCH_Bullish'] == 1) & (df['SSL'] == 1) |
                         (df['CHoCH_Bearish'] == 1) & (df['BSL'] == 1)).astype(int)

    df['FVG_VolSpike'] = ((df['Bullish_FVG'] == 1) & (df['Volume_Spike'] == 1) |
                          (df['Bearish_FVG'] == 1) & (df['Volume_Spike'] == 1)).astype(int)

    df['Sniper_Confluence'] = (
        (df['Buy_Sniper'] == 1) |
        (df['Sell_Sniper'] == 1) |
        (df['CHoCH_Sweep'] == 1)
    ).astype(int)

    return df
df = add_smart_money_confluence(df)
print("smart money confluence signals added successfully.")

"""### **8. Mitigation Detection**

Definition:

Mitigation occurs when price revisits and respects a previously identified Order Block (OB) or Fair Value Gap (FVG).

This retest confirms the zone’s validity and is often used for sniper entries.
"""

def detect_mitigation(df):
    """
    Flags when price revisits the OB or FVG zone.

    Adds:
    - OB_Mitigated: 1 if price tapped the OB zone
    - FVG_Mitigated: 1 if price entered the FVG zone
    """
    df['OB_Mitigated'] = (
        (df['Low'] <= df['OB_High']) & (df['High'] >= df['OB_Low'])
    ).astype(int)

    df['FVG_Mitigated'] = (
        (df['Low'] <= df['FVG_High']) & (df['High'] >= df['FVG_Low'])
    ).astype(int)

    return df
df = detect_mitigation(df)
print("Miigation detection successfully added")

"""### **9. FVG Fill Tracker**

Definition:

An FVG is considered filled once price closes inside the gap zone.
    
Tracking this helps:

Avoid targeting already filled FVGs

Understand which imbalances still offer edge
"""

def detect_fvg_fill(df):
    """
    Flags when FVG has been filled by price action.

    Adds:
    - FVG_Filled: 1 if current close is within the FVG zone
    """
    df['FVG_Filled'] = (
        (df['Close'] >= df['FVG_Low']) & (df['Close'] <= df['FVG_High'])
    ).astype(int)

    return df
df = detect_fvg_fill(df)

print("FVG fill features  successfully added to the dataset.")

df.info()

df.columns

"""All the required columns have been successfully added to the dataset

## **Target Class Labeling**

We are going to use a hybrid classification approach for this business problem. In that we are going to have the bias(buy, sell, no move) and the magnitude of the move.

#### **Why We Use Hybrid Classification for BTC/USD Prediction**

Cryptocurrency markets like BTC/USD are highly volatile, non-linear, and event-driven — which makes predicting exact prices difficult and often misleading.

Instead of forecasting price itself (regression) or just direction (binary classification), we use a **hybrid classification approach** to combine the best of both worlds.

---

###  What Hybrid Classification Means:
We **convert continuous future returns** into **discrete directional classes**, such as:

- Strongly Bullish  
- Weakly Bullish  
- Neutral  
- Weakly Bearish  
- Strongly Bearish  

This allows us to:

1. **Capture both direction and strength** of future movement  
2. Make the problem more stable and realistic in noisy financial data  
3. Align directly with trading actions (e.g., go long, go short, stay out)

---


###  Final Outcome:
Hybrid classification gives the model **clear, strategic output classes** that reflect real trading decisions — and makes it easier to optimize for **profit**, not just prediction accuracy.
"""

def assign_btc_target_class(df, lookahead=5, neutral_thresh=0.1, strong_thresh=1.0):
    """
    Assigns hybrid classification labels for BTC/USD based on % move from Open
    over the next `lookahead` candles.

    Classes:
    0 = Neutral
    1 = Weak Bullish
    2 = Strong Bearish
    3 = Weak Bearish
    4 = Strong Bullish
    """
    df = df.copy()
    df["target_class"] = 0

    for i in range(len(df) - lookahead):
        open_price = df.loc[i, "Open"]

        # Lookahead high and low
        future_high = df.loc[i+1:i+lookahead, "High"].max()
        future_low = df.loc[i+1:i+lookahead, "Low"].min()

        # Percent move calculations
        bullish_pct = ((future_high - open_price) / open_price) * 100
        bearish_pct = ((open_price - future_low) / open_price) * 100

        # Class logic
        if bullish_pct >= strong_thresh and bullish_pct > bearish_pct:
            df.loc[i, "target_class"] = 4  # Strong Bullish
        elif bullish_pct >= neutral_thresh and bullish_pct > bearish_pct:
            df.loc[i, "target_class"] = 1  # Weak Bullish
        elif bearish_pct >= strong_thresh and bearish_pct > bullish_pct:
            df.loc[i, "target_class"] = 2  # Strong Bearish
        elif bearish_pct >= neutral_thresh and bearish_pct > bullish_pct:
            df.loc[i, "target_class"] = 3  # Weak Bearish
        else:
            df.loc[i, "target_class"] = 0  # Neutral

    return df

df = assign_btc_target_class(df, lookahead=5, neutral_thresh=0.1, strong_thresh=1.0)
print("✅ BTC target class assigned with volatility-aware thresholds.")

# saving the dataset with the engineered features

df.to_csv("btc_labeled_dataset.csv", index=False)
print("✅ Dataset saved as 'btc_labeled_dataset.csv'")

import pandas as pd
df = pd.read_csv("btc_labeled_dataset.csv")

"""# **EXPLORATORY DATA ANALYSIS**

#### For us to truly understand how bitcoin price moves we will have to focus on 2 parts

## 1.Fundermental analysis

## 2.Technical analysis
"""

!pip install dash
!pip install pandas
!pip install numpy
!pip install plotly
!pip install dash
!pip install dash-bootstrap-components
!pip install matplotlib
!pip install seaborn
!pip install mplfinance

!pip install sys

# Commented out IPython magic to ensure Python compatibility.
# lets first load all our liblaries well use i creating our dashboard in one go
import pandas as pd
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import dash
from dash import dcc, html
from dash.dependencies import Input, Output
import dash_bootstrap_components as dbc
import os
from datetime import datetime, timedelta
import traceback
import warnings
import sys
warnings.filterwarnings('ignore')

# lets import  neccesry liblaries for fundurmental analysis
# %matplotlib inline
import mplfinance as mpf

# lets now import the remaining liblaries that will asist in fundermental analysis
import matplotlib.pyplot as plt
import seaborn as sns
from pandas.plotting import register_matplotlib_converters
register_matplotlib_converters()
import plotly.express as px

# lets import liblaries for ploting our technical analysis
#sys.path.append('..')  # Go up one directory
#import sys
#sys.path.append('..')  # Add parent directory to path if needed

# Import the new technical analysis module
#from python_scripts.technical_analysis import TechnicalAnalysis

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

file_path = '/content/drive/MyDrive/BTCUSD CLEAN DATA FOR MODELING.csv'
df = pd.read_csv(file_path)

print(" Dataset loaded successfully.")
print(" Shape:", df.shape)
print(" Columns:", df.columns.tolist())

# loading the combined dataset

file_path = '/content/drive/MyDrive/BTCUSD CLEAN DATA FOR MODELING.csv'
df = pd.read_csv(file_path)

print(" Dataset loaded successfully.")
print(" Shape:", df.shape)
print(" Columns:", df.columns.tolist(), "...")


# loading the daily dataset
file_path = '/content/drive/MyDrive/BTCUSD Daily.csv'
dailydf = pd.read_csv(file_path)

#loading 4 hour dataset

file_path = '/content/drive/MyDrive/BTCUSDH4.csv'
H4df = pd.read_csv(file_path)

#loading the 1 hour dataset

file_path = '/content/drive/MyDrive/BTCUSDH1.csv'
H1df = pd.read_csv(file_path)

#loading the M15 dataset

file_path = '/content/drive/MyDrive/BTCUSDM15.csv'
M15df = pd.read_csv(file_path)

dailydf.tail()

"""## 1.1.1 preprocesing the data

#### > we are going to confirm if or data is it for timeseries ananlysis and the data types
#### > lets first look at the range of our values
"""

M15df.describe().T

H1df.describe().T

dailydf.describe().T

df.describe().T

"""We have seen that most of our values are between 2017 and 2025"""

# 1. Make sure 'Date' and 'Timestamp' are strings
dailydf['Date'] = dailydf['Date'].astype(str)
dailydf['Timestamp'] = dailydf['Timestamp'].astype(str)

# 2. Combine 'Date' and 'Timestamp' into a new datetime column
dailydf['DateTime'] = pd.to_datetime(dailydf['Date'] + ' ' + dailydf['Timestamp'],
                                     format='%Y%m%d %H:%M:%S', errors='coerce')

# 3. Set 'DateTime' as the index
dailydf = dailydf.set_index('DateTime')

# 4. Sort index (optional but recommended)
dailydf = dailydf.sort_index()

dailydf.head()

"""# 1.0.0 FUNDERMENTAL ANALYSIS

#### Fundamental Analysis is a method of evaluating securities (stocks, currencies, cryptocurrencies) by examining the underlying economic, financial, and qualitative factors that influence their intrinsic value. Unlike technical analysis (which focuses on price charts and patterns), fundamental analysis looks at real-world data to determine whether an asset is overvalued or undervalued.
"""

# lets just plot the candles to see how the price moved
mpf.plot(dailydf,
         tight_layout=True,
         style='yahoo',
         figsize=(16,6))

"""### > This plot reveals several notable trends. During the COVID-19 period—from its official declaration as a pandemic on March 11, 2020, until the global health emergency concluded on May 5, 2023—the market experienced significant price fluctuations within a defined range. Additionally, the early months of the pandemic, from March until roughly mid-year, were marked by a strong bullish surge, reflecting robust upward momentum in the market."""

# lets now view the volume to see if there were any sigificant volume spikes
mpf.plot(dailydf,
         type="line",
         volume=True,
         tight_layout=True,
         figsize=(16,6),
         style='yahoo')

"""### > These observations confirm our assumptions regarding the onset of the COVID period, as demonstrated by the significant spike in trading volume.

## 1.1.1 lets plot the covid period
"""

# Define the COVID-19 period
start_date = "2020-03-11"
end_date = "2023-05-05"

# Filter the DataFrame to include only that period
covid_df = dailydf.loc[start_date:end_date]

# Plot the data
mpf.plot(covid_df,
         type="candle",
         volume=True,
         title="Bitcoin Price During COVID-19 january 30 2020 to may 5 2023",
         tight_layout=True,
         figsize=(16,6),
         style='yahoo')

"""### > This analysis indicates that at the onset of the pandemic, there was a dramatic surge in trading volume. Additionally, much of the trading activity during the crisis occurred within a defined range, signaling the early emergence of an uptrend."""

# lets zoom in on the volume spike
# Define the COVID-19 period
start_date = "2020-11-11"
end_date = "2021-05-05"

# Filter the DataFrame to include only that period
covid_df = dailydf.loc[start_date:end_date]

# Plot the data
mpf.plot(covid_df,
         type="candle",
         volume=True,
         title="Bitcoin Price During COVID-19 period november 11 2020 to may 5 2021",
         tight_layout=True,
         figsize=(16,7),
         style='yahoo')

"""### > We can observe that the majority of high-volume trades occurred within a defined price range, suggesting an intense contest between buyers and sellers for market dominance."""

def prepare_intraday_dataframe(df):
    # 1. Convert 'Date' and 'Timestamp' to string first
    df['Date'] = df['Date'].astype(str)
    df['Timestamp'] = df['Timestamp'].astype(str)

    # 2. Create 'DateTime' column by combining Date + Timestamp
    df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Timestamp'],
                                    format='%Y%m%d %H:%M:%S', errors='coerce')

    # 3. Set 'DateTime' as index
    df = df.set_index('DateTime')

    # 4. Sort by index (optional but clean)
    df = df.sort_index()

    # 5. (Optional) Drop 'Date' and 'Timestamp' columns to clean up
    df = df.drop(columns=['Date', 'Timestamp'], errors='ignore')

    return df
M15df = prepare_intraday_dataframe(M15df)
H1df = prepare_intraday_dataframe(H1df)
H4df = prepare_intraday_dataframe(H4df)

# Let's zoom in on the volume spike
start_date = "2021-1-11"
end_date = "2021-02-03"


# Filter the DataFrame to include only data within the specified period
covid_df = H1df.loc[start_date:end_date]

# Plot the candlestick chart with the volume panel
mpf.plot(covid_df,
         type="candle",
         volume=True,
         title="Bitcoin Price During Volume Spike Period (2021-1-11 to 2021-02-03)",
         tight_layout=True,
         figsize=(16,7),
         style='yahoo')

"""### > we can see that most of the volume that was trated was actualy bearish wich shows the sellers were resisting the price and they were in control"""

# lets take a deeper dive using the 15 min to confirm our analysis

# Let's zoom in on the volume spike
start_date = "2021-1-15"
end_date = "2021-01-25"


# Filter the DataFrame to include only data within the specified period
covid_df = M15df.loc[start_date:end_date]

# Plot the candlestick chart with the volume panel
mpf.plot(covid_df,
         type="candle",
         volume=True,
         title="Bitcoin Price During Volume Spike Period (2021-1-15 to 2021-01-25)",
         tight_layout=True,
         figsize=(16,7),
         style='yahoo')

"""## 1.2.0: Bitcoin's January 2021 Consolidation: A Critical Pause Before the Next Surge (January 15–25, 2021)

### During this 10-day period, Bitcoin (BTC) traded between 34,000 and 38,000  experiencing high volatility on massive volume after its explosive rally from 29,000 to 42,000 earlier in the month. This phase was crucial—it allowed the market to digest gains, shake out weak hands, and set the foundation for the next leg up.
"""



"""## 1.3.0:December 2017: Bitcoin reached its first major all-time high of 19,892
The price surge to followed by a sharp drop was driven by a combination of speculative frenzy, media hype, and key market developments.

### 1. Retail Investor Frenzy (FOMO)
Mainstream media hype: Bitcoin gained widespread attention as prices skyrocketed, with headlines like "Bitcoin Hits $10,000!" fueling Fear of Missing Out (FOMO).

New retail investors flooded in: Many first-time buyers entered the market, often using platforms like Coinbase, which saw app downloads surge to #1 on the Apple App Store.

Leveraged trading: Unregulated crypto exchanges allowed excessive margin trading (up to 100x leverage), amplifying price swings.
  
### 2. Institutional Entry via Bitcoin Futures
December 10, 2017: The CBOE (Chicago Board Options Exchange) launched the first Bitcoin futures contracts, followed by CME on December 17.

Why it mattered: Traditional investors could now bet on Bitcoin without owning it, signaling Wall Street’s growing interest.

Irony: Futures also enabled short-selling, which later contributed to the crash.


### 3. Global Market Conditions
Weak fiat currencies: Economic instability in Venezuela, Zimbabwe, and other countries drove demand for Bitcoin as a hedge.

Asian market dominance: South Korea and Japan accounted for ~50% of trading volume, with the "Kimchi Premium" (higher BTC prices on Korean exchanges) adding volatility.


### 4. Technical & Psychological Factors
Parabolic rally: Bitcoin had risen 1,900% in 12 months, attracting momentum traders.

"Greater Fool Theory": Many buyers assumed prices would keep rising indefinitely, ignoring valuation metrics.


### 5. The Crash (Why It Dropped to $13,850)
Profit-taking: Early investors cashed out near the peak.

Futures-driven shorting: Hedge funds used CME/CBOE futures to bet against Bitcoin.

Regulatory fears: China intensified its crackdown on crypto exchanges (e.g., banning ICOs in September 2017).

Exchange failures: Platforms like Bitfinex and OKEx suffered outages during peak volatility, triggering panic sells.
"""

# we saw a price dip in december 2018  lets look depper into it
start_date = "2018-12-4"
end_date = "2018-12-18"

# Filter the DataFrame to include only that period
bullrundf = M15df.loc[start_date:end_date]

# Plot the data
mpf.plot(bullrundf,
         type="candle",
         volume=True,
         title=f" December 2018: Bitcoin crashed to $3,200 (down 73% from its 2017 peak) from {start_date }to {end_date}",
         tight_layout=True,
         figsize=(16,6),
         style='yahoo')

"""## 1.4.0:Bitcoin's December 2018 Crash to $3,200: The Perfect Storm (73% drop from its 2017 all-time high of $19,892)

The collapse was driven by a combination of regulatory pressure, market psychology, and structural weaknesses in the crypto ecosystem. Below are the possible causes

### 1. Regulatory Crackdowns in China (Ongoing Pressure)
September 2017: China banned ICOs (Initial Coin Offerings), hurting speculative demand.

February 2018: Chinese authorities shut down local crypto exchanges, forcing traders offshore.

Impact: Reduced liquidity and increased fear of further bans.

### 2. Mt. Gox Bitcoin Liquidation Fears
Mt. Gox, the infamous hacked exchange (2014), still held ~166,000 BTC in bankruptcy proceedings.

Late 2018: Rumors spread that the Mt. Gox trustee would dump BTC to repay creditors.

Effect: Traders feared a massive sell-off, adding downward pressure.

### 3. The "Crypto Winter" Bear Market
After the 2017 bubble, Bitcoin entered a prolonged bear market.

Altcoin collapse: Many ICO projects (funded via Bitcoin) sold BTC reserves to stay afloat, increasing supply.

Declining interest: Google Trends for "Bitcoin" fell ~90% from December 2017 to December 2018.

### 4. Miner Capitulation (Forced Selling)

Result: Miners had to sell BTC to cover costs, worsening the downtrend.

### 5. Futures Market Manipulation?
CME & CBOE Bitcoin futures (launched Dec 2017) allowed institutions to short BTC.

Suspicions: Some believe "whales" pushed prices down to liquidate long positions.

### 6. Psychological Factors (Fear & Capitulation)
Retail investors gave up after months of decline ("I'll never buy crypto again!").

Media narrative shifted from "Bitcoin to $100K!" to "Is Bitcoin Dead?"
"""

# we saw a stip price dip in march 2020  lets look depper into it
start_date = "2020-03-11"
end_date = "2020-03-12"

# Filter the DataFrame to include only that period
bullrundf = M15df.loc[start_date:end_date]

# Plot the data
mpf.plot(bullrundf,
         type="candle",
         volume=True,
         title=f"March 12, 2020: Bitcoin's Black Thursday: The COVID-19 Crash (March 12, 2020)  from {start_date }to {end_date}",
         tight_layout=True,
         figsize=(16,6),
         style='yahoo')

"""## 1.5.0 Bitcoin's Black Thursday: The COVID-19 Crash (March 12, 2020)
### > On March 12, 2020, Bitcoin experienced one of its most brutal single-day drops in history, plunging -39% in just 24 hours. This crash mirrored a global financial panic triggered by the COVID-19 pandemic.

## Why Did Bitcoin Crash So Hard?
## 1. Global Stock Market Meltdown (Correlation with Traditional Markets)
The S&P 500 dropped -9.5%, its worst day since 1987.

Oil prices collapsed (WTI crude fell to -$37 a month later).

Investors sold everything—stocks, gold, crypto—to raise cash (a "liquidity crisis").

## 2. Bitcoin Was Still Seen as a "Risk Asset" (Not Yet a Safe Haven)
Unlike gold (which later surged), Bitcoin was still treated as a high-risk speculative asset.

Hedge funds and whales dumped BTC to cover losses in stocks.

## 3. Liquidation Cascade in Crypto Markets
Margin calls & forced selling: Traders using high leverage (up to 100x) got wiped out as exchanges liquidated positions.

Major exchanges (Binance, BitMEX, Kraken) crashed due to extreme volatility, worsening the sell-off.

Total liquidations: Over $1 billion in long positions were wiped out in 24 hours.

## 4. Miner Capitulation (Network Pressure)
Bitcoin’s price fell below the cost of mining.

Some miners shut down rigs, reducing network security (hash rate dropped -25%).

## 5. Fear & Panic Selling (Retail Investors Gave Up)
Google searches for "Bitcoin dead" spiked.

Many retail traders sold at the bottom, locking in losses.

# 1.6.0 Bitcoin's 2021 Rollercoaster
2021 was Bitcoin's most dramatic year yet—a battle between institutional adoption and regulatory crackdowns, with three major price peaks and crashes. Below is a detailed breakdown of the key events:
"""

# alot of price movement happend in 2021 lets look depper into it
start_date = "2021-02-7"
end_date = "2021-02-9"

# Filter the DataFrame to include only that period
bullrundf = M15df.loc[start_date:end_date]

# Plot the data
mpf.plot(bullrundf,
         type="candle",
         volume=True,
         title=f"Tesla’s 1.5BillionBitcoinBet from {start_date }to {end_date}",
         tight_layout=True,
         figsize=(16,6),
         style='yahoo')

"""## 1.6.1: Tesla’s 1.5BillionBitcoinBet(February8,2021)

### What Happened?

Elon Musk’s Tesla announced it had bought $1.5B worth of Bitcoin and would accept BTC for car payments.

This was the first major corporate treasury allocation to Bitcoin, validating it as a "legitimate" asset.

### Why Did It Matter?

Institutional credibility – If Tesla was buying, others (MicroStrategy, Square) would follow.

Retail FOMO – Small investors rushed in, pushing BTC to $58,000 by February 21.

Mainstream media hype – Headlines like "Bitcoin is the new gold" dominated financial news.
"""

# alot of price movement happend in 2021 lets look depper into it
start_date = "2021-04-13"
end_date = "2021-04-15"

# Filter the DataFrame to include only that period
bullrundf = M15df.loc[start_date:end_date]

# Plot the data
mpf.plot(bullrundf,
         type="candle",
         volume=True,
         title=f"Coinbase IPO from {start_date }to {end_date}",
         tight_layout=True,
         figsize=(16,6),
         style='yahoo')

"""## 1.6.2: Coinbase IPO

April 14, 2021: Coinbase Goes Public ($64,895 Peak)

Coinbase (COIN) listed on Nasdaq at a $100B valuation, a watershed moment for crypto legitimacy.

The hype briefly pushed Bitcoin to $64,895, but the rally didn’t hold.


"""

# alot of price movement happend in 2021 lets look depper into it
start_date = "2021-05-18"
end_date = "2021-05-20"

# Filter the DataFrame to include only that period
bullrundf = M15df.loc[start_date:end_date]

# Plot the data
mpf.plot(bullrundf,
         type="candle",
         volume=True,
         title=f"China’s Brutal Crackdownfrom {start_date }to {end_date}",
         tight_layout=True,
         figsize=(16,6),
         style='yahoo')

"""## 1.6.3:May–July 2021: China’s Brutal Crackdown (-50% Crash)
May 19, 2021: China banned Bitcoin mining, causing:

Hash rate drop (50% of miners went offline)

Panic selling

Elon Musk’s U-turn: Tesla stopped BTC payments, citing energy concerns, adding to the sell-off.

### Why Did It Crash So Hard?

Mining ban uncertainty – Would Bitcoin’s network recover?

Leveraged traders got wrecked – Over $10B in liquidations in May alone.

Media narrative flipped – From "Bitcoin to $100K!" to "Is Bitcoin Dead (Again)?"

# 1.7.0: One of Crypto's Biggest Black Swan Events
"""

start_date = "2022-11-6"
end_date = "2022-11-16"

# Filter the DataFrame to include only that period
bullrundf = M15df.loc[start_date:end_date]

# Plot the data
mpf.plot(bullrundf,
         type="candle",
         volume=True,
         title=f"The FTX Collapse & Bitcoin's Cras from {start_date }to {end_date}",
         tight_layout=True,
         figsize=(16,6),
         style='yahoo')

"""## 1.7.1:The FTX Collapse & Bitcoin's Crash to $16,500 (November 2022)
In November 2022, Bitcoin plunged -50% in just one month The trigger? The catastrophic collapse of FTX, then the 3rd-largest crypto exchange.


### What Was FTX & Why Did It Collapse?
FTX was a top-3 crypto exchange, founded by Sam Bankman-Fried (SBF), valued at $32B in early 2022.

November 2, 2022: A CoinDesk report revealed that Alameda Research (FTX’s sister hedge fund) held billions in FTT tokens (FTX’s own crypto) as "assets."

November 6: Binance CEO CZ announced he would dump all FTT holdings, triggering panic.

November 8: FTX faced a $6B bank run as users rushed to withdraw funds.

November 11: FTX filed for bankruptcy—customers lost $8B+ in frozen funds.

### Why Did This Crash Bitcoin?
Contagion fear – If FTX (a top exchange) could fail, who else was at risk?

Mass liquidations – Hedge funds, lenders (BlockFi, Genesis), and traders got wiped out.

Loss of trust – After FTX, many feared all centralized exchanges were unsafe.

# 1.8.0: The Banking Crisis Rally
## 1.8.1 Bitcoin's 2023 Rebound
"""

start_date = "2023-03-1"
end_date = "2023-03-31"

# Filter the DataFrame to include only that period
bullrundf = M15df.loc[start_date:end_date]

# Plot the data
mpf.plot(bullrundf,
         type="candle",
         volume=True,
         title=f"U.S. Banking Collapses (March 2023) from {start_date }to {end_date}",
         tight_layout=True,
         figsize=(16,6),
         style='yahoo')

"""
In March 2023, Bitcoin surged +40% in two weeks, rocketing from 19,900to28,500 and eventually peaking at $42,258 by mid-year. This rally was fueled by a perfect storm of banking failures, institutional demand, and macroeconomic shifts. Below is a detailed breakdown.

## 1.8.1: U.S. Banking Collapses (March 2023)
Key Bank Failures That Shook Markets
March 8: Silvergate Bank (Crypto-Friendly) Collapses

A major lender to crypto firms, its shutdown triggered panic about banking access for exchanges.

March 10: Silicon Valley Bank (SVB) Implodes

The 16th-largest U.S. bank collapsed due to a bank run, spooking investors globally.

March 12: Signature Bank Shut Down by Regulators

Another crypto-friendly bank was seized, worsening fears of a systemic banking crisis.

### Why Did This Help Bitcoin?
"Flight to Safety" Narrative: Investors feared traditional banks were unstable, so they turned to Bitcoin as "hard money" outside the banking system.

Fed Intervention Backfired: The U.S. government bailed out depositors, proving fiat systems could fail—strengthening Bitcoin’s case as an alternative."""

start_date = "2023-06-16"
end_date = "2023-06-29"

# Filter the DataFrame to include only that period
bullrundf = M15df.loc[start_date:end_date]

# Plot the data
mpf.plot(bullrundf,
         type="candle",
         volume=True,
         title=f"Institutional & Macro Drivers of the Rallyfrom {start_date }to {end_date}",
         tight_layout=True,
         figsize=(16,6),
         style='yahoo')

"""## 1.8.2: Institutional & Macro Drivers of the Rally
#### A. BlackRock’s Bitcoin ETF Filing (June 2023)
On June 15, 2023, BlackRock (the world’s largest asset manager) filed for a spot Bitcoin ETF, signaling Wall Street’s growing interest.

This sparked a wave of institutional optimism, pushing BTC to $31,000 by July.

#### B. The Fed’s Rate Hike Slowdown
After aggressive 2022 rate hikes, the Fed signaled a pause, weakening the U.S. dollar.

Bitcoin (as a risk asset) benefited from looser financial conditions.

#### C. Miner Recovery & Supply Squeeze
Many miners had gone bankrupt in 2022, but surviving firms became more efficient.

Bitcoin’s hash rate rebounded, signaling network strength.

# 1.9.0: Bitcoin's Monumental 2024

2024 was Bitcoin’s most explosive year yet, driven by unprecedented institutional adoption, supply shocks, and political tailwinds.
"""

start_date = "2024-01-8"
end_date = "2024-01-11"

# Filter the DataFrame to include only that period
bullrundf = M15df.loc[start_date:end_date]

# Plot the data
mpf.plot(bullrundf,
         type="candle",
         volume=True,
         title=f"The SEC Approves Spot Bitcoin ETFs from {start_date }to {end_date}",
         tight_layout=True,
         figsize=(16,6),
         style='yahoo')

"""## 1.9.1: January 10, 2024: The SEC Approves Spot Bitcoin ETFs

### What Happened?

The SEC approved 11 spot Bitcoin ETFs, including:

BlackRock’s IBIT (world’s largest asset manager)

Fidelity’s FBTC (trusted institutional giant)

ARK Invest’s ARKB (Cathie Wood’s crypto bet)

### Why Was This a Game-Changer?

wall Street Now Onboard – Financial advisors, retirement funds, and hedge funds could finally buy Bitcoin without self-custody.

Massive Inflows – ETFs absorbed $10B+ in net inflows in their first two months.

Price Surge – BTC skyrocketed +60% as demand overwhelmed supply.
"""

start_date = "2024-04-1"
end_date = "2024-04-29"

# Filter the DataFrame to include only that period
bullrundf = M15df.loc[start_date:end_date]

# Plot the data
mpf.plot(bullrundf,
         type="candle",
         volume=True,
         title=f"bitcoin halving from {start_date }to {end_date}",
         tight_layout=True,
         figsize=(16,6),
         style='yahoo')

"""## 1.9.3: April 19, 2024: The Fourth Bitcoin Halving

### What Did the Halving Do?

Reduced new Bitcoin supply by 50% (~$30M worth of BTC per day no longer mined).

Historically, halvings led to 12-18 month bull runs (2012, 2016, 2020 all followed this pattern).

Market Reaction

Short-term dip (to $63,821) – Traders "sold the news."

Long-term bullish – Miners held coins, anticipating higher prices.

### Why This Halving Was Different

First halving with ETFs – Institutional demand now compounded the supply shock.

Hash rate all-time highs – Miners were better prepared than in past cycles.
"""

start_date = "2024-12-1"
end_date = "2024-12-29"

# Filter the DataFrame to include only that period
bullrundf = M15df.loc[start_date:end_date]

# Plot the data
mpf.plot(bullrundf,
         type="candle",
         volume=True,
         title=f"bitcoin hits 100,000 from {start_date }to {end_date}",
         tight_layout=True,
         figsize=(16,6),
         style='yahoo')

"""## 1.9.4: December 5, 2024: Bitcoin Hits $100,000
(Fueled by U.S. Election & "Strategic Bitcoin Reserve" Talks)

### What Drove the Rally?
Trump’s Pro-Bitcoin Policies – After winning the 2024 election, he proposed:

A "Strategic Bitcoin Reserve" (like the U.S. holds gold).

Tax incentives for crypto businesses.

ETF Demand Accelerated – Daily inflows hit $1B+.

FOMO Buying – Retail investors flooded back in.

Market Psychology at $100K
Media frenzy – "Bitcoin is digital gold!" headlines dominated.

Corrections got bought aggressively – Dips to $85K were seen as "discounts."

# 2.0.0 : THECHNICAL ANALYSIS

---

# 2.1.0 : Market Structure Analysis Dashboard
---
**A Comprehensive Tool for Technical Analysis Visualization**



## Overview
I've developed an advanced dashboard to visualize critical market structure elements and technical indicators across multiple timeframes. This tool empowers traders to analyze price action with precision, focusing on key concepts like **Order Blocks**, **Fair Value Gaps (FVG)**, **Break of Structure (BOS)**, and essential technical indicators including **Moving Averages**, **RSI**, and **MACD**.



##  Key Features

###  Multi-Timeframe Analysis
- **15-Minute (M15)** - Intraday precision
- **1-Hour (H1)** - Short-term trend identification  
- **4-Hour (H4)** - Medium-term structure analysis
- **Daily** - Long-term market context

###  Core Visualizations
- **Order Blocks**: Highlight supply/demand zones with custom markers
- **FVG Zones**: Transparent overlays showing imbalance areas
- **Structure Breaks**: Star markers indicating trend reversals
- **Moving Averages**: 20/50/200-period SMAs with adaptive coloring
- **Technical Indicators**:
  - RSI with overbought/oversold levels
  - MACD histogram divergence detection

###  Performance Optimization
- **3-Month Sliding Window**: Dynamic data loading prevents memory overload
- **Smart Resampling**: Maintains detail without sacrificing performance
- **GPU-Accelerated Rendering**: Smooth chart interactions even with complex drawings

---

## 2.2.0 : Handling Multi-timeframe Data
First, let's update our data preparation code to handle this structure correctly:
"""

# Step 1: Convert 'Date' and 'Timestamp' to strings
df['Date'] = df['Date'].astype(str)
df['Timestamp'] = df['Timestamp'].astype(str)

# Step 2: Create 'DateTime' by combining 'Date' and 'Timestamp'
df['DateTime'] = pd.to_datetime(
    df['Date'] + ' ' + df['Timestamp'],
    format='%Y%m%d %H:%M:%S', errors='coerce'
)

# Step 3: Set 'DateTime' as index
df.set_index('DateTime', inplace=True)

# Step 4: Sort index
df = df.sort_index()

# Step 5: Convert selected columns to numeric if they exist
numeric_columns = [
    'SMA_10', 'SMA_50', 'SMA_200', 'BB_Mid', 'BB_Upper', 'BB_Lower',
    'Swing_High', 'Swing_Low', 'Rolling_High', 'Rolling_Low',
    'Prev_Swing_High', 'Prev_Swing_Low', 'FVG_Low', 'FVG_High',
    'OB_Low', 'OB_High', 'Fair_Value_Mid', 'Avg_Volume',
    'Log_Returns', 'HV', 'Price_Range', 'ATR_vs_Range'
]

for col in numeric_columns:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')

"""## 2.2.1 Creating Timeframe-specific Charts

### were are going to look how diffrent market structures apper in diffrent time frames


Analyzing Market Structure Across Timeframes
"""

def analyze_market_structure_by_timeframe(df):
    """
    Analyze market structure indicators and their distribution across timeframes
    """
    # Create timeframe markers
    df['is_new_h1'] = ~df['H1_Open'].duplicated(keep='first')
    df['is_new_h4'] = ~df['H4_Open'].duplicated(keep='first')
    df['is_new_daily'] = ~df['Daily_Open'].duplicated(keep='first')

    # Count structure events by timeframe
    structure_events = {
        'Bullish_BOS': [],
        'Bearish_BOS': [],
        'CHoCH_Bullish': [],
        'CHoCH_Bearish': [],
        'Bullish_FVG': [],
        'Bearish_FVG': [],
        'Bullish_OB': [],
        'Bearish_OB': []
    }

    timeframes = ['15m boundary', '1h boundary', '4h boundary', 'Daily boundary']

    # Count events at each timeframe boundary
    for event in structure_events.keys():
        # All 15m candles
        structure_events[event].append(df[event].sum())

        # Events on 1h boundaries
        structure_events[event].append(df.loc[df['is_new_h1'], event].sum())

        # Events on 4h boundaries
        structure_events[event].append(df.loc[df['is_new_h4'], event].sum())

        # Events on daily boundaries
        structure_events[event].append(df.loc[df['is_new_daily'], event].sum())

    # Plot distribution of structure events by timeframe
    fig, axes = plt.subplots(4, 2, figsize=(15, 20))
    fig.suptitle('Market Structure Events by Timeframe', fontsize=16)

    events = list(structure_events.keys())
    for i in range(8):
        row, col = i // 2, i % 2
        ax = axes[row, col]
        ax.bar(timeframes, structure_events[events[i]])
        ax.set_title(events[i].replace('_', ' '))
        ax.set_ylabel('Count')
        ax.tick_params(axis='x', rotation=45)

    plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust for suptitle
    plt.show()
    plt.savefig('structure_events_by_timeframe.png')
    plt.close()

    return structure_events

# Analyze market structure across timeframes
structure_events = analyze_market_structure_by_timeframe(df)

"""### 1. Key Observations
#### Dominance of 15-Minute Boundaries:
most of all market structure events  occurred at 15-minute boundaries—the highest frequency across timeframes.
This suggests  algorithms can dominate intraday price reactions."

#### Diminishing Impact in Higher Timeframes:
Order Blocks that appeared at daily bounderies are the lowest among all timeframes.
This implies critical reversals are rare on daily charts but carry higher significance when they occur.

#### Fair Value Gaps mostly apear as Noise:
A huge number of FVGs clustered at 15m boundaries, often fading quickly—a sign of retail-driven liquidity grabs rather than structural shifts.

### 2. Timeframe-Specific Behavior:
15-Minute charts are the most active were alot of market movement happens

### 3. Market Mechanics Insights
#### Liquidity Hierarchy:
15m boundaries attract retail liquidity pools (stops/entries), while daily boundaries see institutional order blocks (HTF banks, options expiry).

#### False Signals vs. Structural Shifts:
FVGs/BOS on 15m are noise (market makers filling orders), while 4H/daily CHoCH/BOS reflect smart money repositioning.

### 4. Strategic Implications
#### Trade Entry:
we see that its wise to use 15m BOS/FVG for scalping entries but use  4H/daily trend alignment to avoid fakeouts.

Prioritize 1H/4H CHoCH reversals as swing trade triggers—higher validity than 15m signals.

#### Risk Management:
Tighten stops on 15m-based trades  vs wider stops on daily-boundary trades

Ignore Bearish OB at 15m unless weekly charts show distribution

#### Signal Filtering:
Discard 15m FVGs without 1H BOS confirmation—retail traps.

Treat daily BOS as macro trend shifts—hold positions longer.


### 6. Caveats & Next Steps
Warning: 15m dominance may reflect overfitting to noise—always confirm with 4H+ trends.

#### Next Steps:

focus on higher timeframes market structure to avoid overfiting to noise

## 2.3.0 : Comparing Structural Events Across Timeframes
This helps understand the distribution of important structural events:
"""

def analyze_structure_timing(df):
    """
    Analyze timing of market structure events relative to HTF boundaries
    with percentage-based insights
    """
    # Add 15m boundary check for completeness
    df['is_new_15m'] = ~df['Open'].duplicated(keep='first')

    timing_buckets = {
        'at_15m_start': 0,
        'at_h1_start': 0,
        'at_h4_start': 0,
        'at_daily_start': 0,
        '1_candle_post_h1': 0,
        '2_candles_post_h1': 0,
        '1_candle_post_h4': 0,
        '2_candles_post_h4': 0,
        '1_candle_post_daily': 0,
        '2_candles_post_daily': 0,
        'no_boundary_link': 0
    }

    # Create significant event flag (using vectorized operation)
    event_cols = ['Bullish_BOS', 'Bearish_BOS', 'CHoCH_Bullish',
                'CHoCH_Bearish', 'Bullish_FVG', 'Bearish_FVG']
    df['significant_event'] = df[event_cols].max(axis=1)

    # Pre-calculate boundary shifts
    for tf in ['15m', 'h1', 'h4', 'daily']:
        for shift in [1, 2]:
            df[f'{tf}_boundary_shift{shift}'] = df[f'is_new_{tf}'].shift(shift).fillna(False)

    # Calculate percentages
    total_events = df['significant_event'].sum()

    # Count events
    timing_buckets['at_15m_start'] = df.loc[df['is_new_15m'], 'significant_event'].sum()
    timing_buckets['at_h1_start'] = df.loc[df['is_new_h1'], 'significant_event'].sum()
    timing_buckets['at_h4_start'] = df.loc[df['is_new_h4'], 'significant_event'].sum()
    timing_buckets['at_daily_start'] = df.loc[df['is_new_daily'], 'significant_event'].sum()

    for tf in ['h1', 'h4', 'daily']:
        timing_buckets[f'1_candle_post_{tf}'] = df.loc[df[f'{tf}_boundary_shift1'], 'significant_event'].sum()
        timing_buckets[f'2_candles_post_{tf}'] = df.loc[df[f'{tf}_boundary_shift2'], 'significant_event'].sum()

    # Calculate unlinked events
    counted = sum(timing_buckets.values()) - timing_buckets['no_boundary_link']
    timing_buckets['no_boundary_link'] = total_events - counted

    # Plot with percentages
    plt.figure(figsize=(14, 8))
    ax = plt.gca()

    # Order by timeframe significance
    ordered_keys = [
        'at_15m_start', 'at_h1_start', 'at_h4_start', 'at_daily_start',
        '1_candle_post_h1', '2_candles_post_h1',
        '1_candle_post_h4', '2_candles_post_h4',
        '1_candle_post_daily', '2_candles_post_daily',
        'no_boundary_link'
    ]

    values = [timing_buckets[k] for k in ordered_keys]
    percentages = [f"{(v/total_events)*100:.1f}%" for v in values]

    bars = ax.bar(ordered_keys, values)
    ax.set_title(f'Market Structure Event Timing (Total Events: {total_events})', fontsize=14)
    ax.set_ylabel('Count')
    ax.bar_label(bars, labels=percentages, padding=3)
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()
    plt.savefig('event_timing_analysis.png', dpi=300)
    plt.close()
analyze_structure_timing(df)

"""### 1. Events at h1/h4 Starts – Institutional Entry Timing

#### Observation:
concentration of events at start of h1 and h4 candles.

#### What Traders Might Be Doing:
Institutional traders tend to place bulk orders at the start of new sessions.

Many algo strategies are aligned with candle opens to reduce slippage and leverage liquidity influx.

Retail traders using ICT-style strategies often look for "power of 3" patterns (accumulation → manipulation → distribution) around these opens.

#### recomendations:
Add a feature for is_h1_open, is_h4_open, and test how your bot responds if it only executes after a fresh h1/h4 open, especially when combined with a structure shift (e.g., BOS, CHoCH).

### 2. 1 or 2 Candles After h1/h4 – Reaction Zones

#### Observation:
Events spike 1 or 2 candles after the boundary open.

#### What Traders Might Be Doing:
After institutions place their orders on h1 open, liquidity grabs or stop hunts often happen in the next 1–2 candles.

Smart money traders might wait for confirmation.

Volume usually confirms direction here.

### 3. Low Significance at 15m Open – Noise Zone

#### Observation:
Low event occurrence at 15m candle starts.

#### What Traders Might Be Doing:
High-frequency traders (HFTs) dominate here, scalping fast volatility.

Most retail swing traders ignore this timeframe as it produces a lot of noise and false signals.

Lower timeframes are used for entries, but not usually for major structure shifts.

###  4. No Boundary Link – Either Noise or News

#### Observation:
Some events happen with no tie to any boundary.

#### What Traders Might Be Doing:
Likely reacting to news, economic data, or liquidity events outside the usual cycles.

Could also be late retail FOMO or bot noise from unpredictable flows.

#### recomendations:
Consider tracking economic calendar timestamps and news events to explain these.

Create a flag like is_during_news_release or off_cycle_event.

## 2.4.0 : Technical Indicator Analysis

### lets now do analysis on our technical indicators

## 2.4.1 15 minutes
"""

!pip install ta

import matplotlib.pyplot as plt
import pandas as pd
import ta

def plot_chart(df, timeframe='15m', start_date=None, end_date=None):
    """
    Plot Close Price, RSI, and MACD for a selected timeframe and date range.
    Supports timeframes: '15m', 'H1', 'H4', 'D' (Daily).
    """

    df = df.copy()

    # 1. Make sure the index is datetime
    if not isinstance(df.index, pd.DatetimeIndex):
        df.index = pd.to_datetime(df.index, errors='coerce')

    # ✅ 2. Drop rows where index became NaT
    df = df[~df.index.isna()]

    # 3. Resample depending on timeframe
    if timeframe == '15m':
        df_tf = df.resample('15T').agg({
            'Open': 'first',
            'High': 'max',
            'Low': 'min',
            'Close': 'last',
            'Volume': 'sum'
        })
    elif timeframe == 'H1':
        df_tf = df.resample('1H').agg({
            'Open': 'first',
            'High': 'max',
            'Low': 'min',
            'Close': 'last',
            'Volume': 'sum'
        })
    elif timeframe == 'H4':
        df_tf = df.resample('4H').agg({
            'Open': 'first',
            'High': 'max',
            'Low': 'min',
            'Close': 'last',
            'Volume': 'sum'
        })
    elif timeframe == 'D' or timeframe == 'Daily':
        df_tf = df.resample('1D').agg({
            'Open': 'first',
            'High': 'max',
            'Low': 'min',
            'Close': 'last',
            'Volume': 'sum'
        })
    else:
        raise ValueError(f"Unsupported timeframe: {timeframe}")

    # Drop rows that may have become NaN after resampling
    df_tf = df_tf.dropna()

    # 4. Filter by date range if given
    if start_date and end_date:
        df_tf = df_tf.loc[start_date:end_date]

    # 5. Add indicators if not already present
    if 'RSI_14' not in df_tf.columns:
        df_tf['RSI_14'] = ta.momentum.RSIIndicator(df_tf['Close'], window=14).rsi()
    if 'MACD' not in df_tf.columns:
        macd = ta.trend.MACD(df_tf['Close'])
        df_tf['MACD'] = macd.macd()
        df_tf['MACD_Signal'] = macd.macd_signal()
        df_tf['MACD_Hist'] = macd.macd_diff()

    # 6. Plotting
    fig, axs = plt.subplots(3, 1, figsize=(16, 12), sharex=True)

    # Close Price
    axs[0].plot(df_tf.index, df_tf['Close'], label='Close Price', color='blue')
    axs[0].set_title(f'Close Price ({timeframe})')
    axs[0].legend()

    # RSI
    axs[1].plot(df_tf.index, df_tf['RSI_14'], label='RSI 14', color='orange')
    axs[1].axhline(70, color='red', linestyle='--', linewidth=1)
    axs[1].axhline(30, color='green', linestyle='--', linewidth=1)
    axs[1].set_title('RSI 14')
    axs[1].legend()

    # MACD
    axs[2].plot(df_tf.index, df_tf['MACD'], label='MACD', color='purple')
    axs[2].plot(df_tf.index, df_tf['MACD_Signal'], label='Signal Line', color='red')
    axs[2].bar(df_tf.index, df_tf['MACD_Hist'], label='MACD Histogram', color='gray', alpha=0.5)
    axs[2].set_title('MACD')
    axs[2].legend()

    plt.tight_layout()
    plt.show()

# Plot 15 minute chart
plot_chart(M15df, timeframe='15m', start_date='2023-01-01', end_date='2023-01-15')

# Plot 1 hour chart
plot_chart(H1df, timeframe='H1', start_date='2023-01-01', end_date='2023-01-15')

# Plot 4 hour chart
plot_chart(H4df, timeframe='H4', start_date='2023-01-01', end_date='2023-01-15')

# Plot daily chart
plot_chart(dailydf, timeframe='D', start_date='2023-01-01', end_date='2023-01-15')

"""### Higher Timeframe Analysis :

The 200-period SMA/EMA becomes a critical level for identifying false breakouts (fakeouts).
Prices often briefly breach it during volatile swings but fail to close beyond it, acting as a strong support/resistance zone.

The 50-period SMA/EMA shifts to the mid-range of price action (center of candles), behaving like a dynamic equilibrium point rather than a direct support/resistance.

### Lower Timeframe Analysis :

The 50-period SMA/EMA regains its role as a key support/resistance level, while the 10-period SMA/EMA acts as a short-term momentum guide.

### Strategy Implications
#### False Breakout Detection (200 SMA/EMA):

Use the 200 MA on higher timeframes to filter fakeouts.


#### 50 SMA/EMA as Mid-Range Anchor:

On higher timeframes, use the 50 MA to identify mean-reversion opportunities:


#### Multi-Timeframe Confirmation:

Align higher timeframe 200 MA signals with lower timeframe 50 MA behavior.

## 2.5.1 ploting the closing price of diffrent timeframes
"""

df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
df = df.set_index('Date')
df = df.sort_index()

close_columns = ['Close', 'H1_Close', 'H4_Close', 'Daily_Close']

for col in close_columns:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')

import matplotlib.pyplot as plt

# ✅ Select a bigger period to have enough data
sample_period = df.tail(5000)  # NOT just 500 rows — more rows for H1/H4/Daily

# ✅ Plot the close prices
plt.figure(figsize=(15, 8))

# Plot main Close (M5 or M15)
plt.plot(sample_period.index, sample_period['Close'], label='M5/M15 Close', alpha=0.7)

# Plot H1_Close if it exists
if 'H1_Close' in sample_period.columns:
    plt.plot(sample_period.index, sample_period['H1_Close'], label='H1 Close', alpha=0.7)

# Plot H4_Close if it exists
if 'H4_Close' in sample_period.columns:
    plt.plot(sample_period.index, sample_period['H4_Close'], label='H4 Close', alpha=0.7)

# Plot Daily_Close if it exists
if 'Daily_Close' in sample_period.columns:
    plt.plot(sample_period.index, sample_period['Daily_Close'], label='Daily Close', alpha=0.7)

plt.title('Multi-Timeframe Close Price Comparison')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""### Key Observations & Explanations
#### 1. 15-Minute Chart Sets Short-Term Lows/Highs
The 15m chart often defines intraday extremes (swing lows/highs) due to its granularity.

#### Why It Happens:

High-frequency noise (e.g., stop hunts, liquidity grabs) is visible here but may not reflect the broader trend.

### 2. 1-Hour Chart Smooths Intraday Noise
The 1h chart smooths out 15m volatility, showing clearer intraday trends.

### Why It Matters:

Lows/highs on the 1h chart are stronger support/resistance levels than those on the 15m.

### 3. 4-Hour Chart Anchors Medium-Term Bias
The 4h chart reflects institutional or algorithmic trading ranges (e.g., London/New York session overlaps).

### Why It Matters:

Lows/highs on the 4h chart often align with key Fibonacci retracements or VWAP levels.

### 4. Daily Chart Defines Macro Trends
The daily chart’s closing price determines the primary trend (bullish/bearish).

### Why It Matters:

Daily lows/highs are critical for swing traders and institutions. A 15m "low" during a daily uptrend is often a buy-the-dip opportunity.

# Conclusions from the EDA
### The analysis of Bitcoin's historical price action, market structure, and key events reveals several critical insights:

#### 1. Market Sensitivity to Macro Events
* Bitcoin is highly reactive to macroeconomic shocks (COVID-19, banking crises), regulatory shifts (China’s mining ban), and institutional adoption (Tesla, ETFs).

* Liquidity crises (March 2020) trigger panic selling, while supply shocks (halvings) and institutional inflows (ETFs) drive sustained rallies.

#### 2. Timeframe-Specific Dynamics
* 15-minute charts dominate intraday noise (retail liquidity grabs, algo-driven volatility).

* 1-hour/4-hour charts smooth noise and align with institutional order flow.

* Daily/weekly charts define macro trends, with key reversals ($100K breakout) validated by structural shifts.

#### 3. Structural Patterns
* Order Blocks (OB) at daily boundaries signal institutional positioning (high significance).

* Fair Value Gaps (FVG) on 15m are often retail traps but useful for scalping with higher timeframe (HTF) confirmation.

* Break of Structure (BOS) on 4H+/daily charts indicates trend reversals (like post-FTX crash recovery).

#### 4. Behavioral Insights
* Retail FOMO (fear of missing out ) amplifies volatility (2017/2021 peaks), while miner capitulation (2018/2020) marks cycle bottoms.

* Institutional participation (lackRock ETF) stabilizes price action and reduces extreme drawdowns.

# Recommendations for the Trading Model
### Based on the EDA, the model should prioritize multi-timeframe analysis, event-driven filters, and liquidity-based risk management:

#### 1. Strategy Design
##### Core Trend Alignment:

* Use daily/weekly charts to identify macro trends (bullish/bearish).

* Trade in the direction of HTF BOS (e.g., buy dips in a daily uptrend).

##### Entry Triggers:

* 15m/1H FVG/BOS: Use for entries only if aligned with 4H+ trend (15m bullish FVG + 4H OB support).

* H1/H4 Session Opens: Focus on institutional order blocks at candle opens .

##### Institutional Catalyst Filters:

* Flag events like ETF inflows/outflows, halvings, or Fed rate decisions to adjust position sizing.

#### 2. Risk Management
##### Dynamic Stop-Loss:

* 15m trades: Tight stops (0.5–1% risk) to avoid noise.

* 4H+/daily trades: Wider stops (2–3%) anchored to HTF support/resistance ( 200 SMA on daily).

##### Volatility Adjustments:
* Reduce leverage during high-impact events (like CPI reports, exchange failures).

* Use ATR (Average True Range) to scale position sizes.

#### 3. Event-Driven Overrides
#### Black Swan Protocols:

* Automatically reduce exposure during exchange collapses ( FTX-like volume spikes) or regulatory bans.

##### Halving Cycle Optimization:

* Increase long bias 6–12 months post-halving (historical bull run pattern).

* Monitor miner reserves (via Glassnode) to anticipate sell pressure.

#### 4. Add Model Features
##### Data Inputs:

* Economic calendar integration ( Fed meetings, ETF deadlines).

On-chain metrics (e.g., Miner Net Position Change, Exchange Netflow).
# Validation & Backtesting
#### Stress-Test Against Crises:

* Simulate performance during COVID-19, FTX collapse, and China FUD to assess drawdown control.

# **AI TARGETED EDA**

To build a high-performance AI-driven trading system, we move beyond traditional data exploration and adopt a specialized approach known as AI-Targeted EDA.

This methodology is designed to evaluate how effectively our engineered features support predictive modeling, particularly in a complex, event-driven environment like BTC/USD.

AI-Targeted EDA bridges the gap between smart feature design and model interpretability, allowing us to diagnose feature behavior, validate structural signals, and inform the architecture of machine learning pipelines.

#### Step 1: Feature Relevance and Predictive Power

The first objective is to assess which features contribute meaningfully to the model’s ability to classify future price behavior.

Models that rely on noisy, irrelevant, or redundant features tend to underperform or overfit.

 **Actions:**

- Compute correlation matrix between features and `target_class`
  
- Use Mutual Information Score to measure non-linear relationships


- Perform SHAP analysis to check which features drive AI predictions
---

#### Step 2: Target Class Distribution and Label Integrity

Effective modeling requires a well-balanced target variable.

In a multi-class setup, it’s essential to evaluate the distribution of directional classes (e.g., strong bullish, neutral, strong bearish).

Imbalanced classes can lead to bias, with models favoring dominant categories and failing to learn from rare but critical signals.

**Actions:**

- Check target class distribution (strong bullish, weak bullish, strong bearish, weak bearish, no move)
  
- If imbalanced, consider oversampling/undersampling strategies like SMOTE

---

#### Step 3: Smart Money Signal Validation

It is critical to validate whether market structure signals—such as BOS, CHoCH, OBs, FVGs, and liquidity sweeps—align with future directional outcomes.

These features form the foundation of our system’s intelligence, and their statistical relevance must be confirmed.

We’ll evaluate feature behavior by plotting distributions across different target classes, conducting signal precision tests, and checking for feature redundancy.
For instance, if both SMA and EMA of the same length provide overlapping signals, one may be removed to simplify the model.

 **Actions:**

- Compare feature values before bullish/bearish moves to check if they align with expected behavior
    
- Box plots and histograms to show distribution of BOS, liquidity levels, etc.

- Feature clustering to detect redundancies (e.g., do we need both `SMA_10` and `EMA_10`?)  

---

#### Step 4: Temporal Pattern Analysis and Lag Optimization

Since price movements are inherently temporal, we analyze how far back in time our model should look to capture useful signals.

This involves studying autocorrelations, partial autocorrelations, and lagged feature behavior. The goal is to determine optimal time windows that provide predictive value without introducing noise.

By analyzing features like MACD, BSL, or ATR across various lags, we identify whether short-term or longer-term historical information contributes more to accurate forecasting.

This insight directly informs model design—especially for architectures like LSTMs and Transformers that depend on sequential context.

 **Actions:**

- Autocorrelation function (ACF) and Partial Autocorrelation (PACF) to analyze dependencies
  
- Lagged correlations to detect how far back we should look for patterns

-
---

We will be going deeper into each step and conducting the analysis and noting the outcomes in preparation for modeling

---

## **Step 1: Feature Relevance and Predictive Power**

### **A: Correlation Analysis**
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Select numeric features only
numeric_df = df.select_dtypes(include=["number"])

# Compute correlation with target_class
corr_matrix = numeric_df.corr()["target_class"].sort_values(ascending=False)

# Plot heatmap of correlations
plt.figure(figsize=(12, 8))
sns.heatmap(numeric_df[corr_matrix.index].corr(), annot=False, cmap="coolwarm", fmt=".2f")
plt.title("Feature Correlation Heatmap")
plt.show()

# View top and bottom correlations with target_class
print(" Top Correlated Features with Target Class:")
print(corr_matrix.head(20))

print("\n Least Correlated Features with Target Class:")
print(corr_matrix.tail(20))

"""#### Key Outcomes:

- **HV**, **ATR_14**, and **Price_Range** had the highest correlations, confirming that **volatility** is a major driver of directional price movement.
  
- Features like **BSL**, **Bullish_BOS**, and **Bullish_OB** had moderate but positive correlation, supporting their relevance.

- Volume-based features (e.g., `H1_Volume`, `Daily_Volume`) consistently showed relevance.

- Low or negative correlation was seen with **Swing_Low**, **Bearish_OB**, and all `Date_` columns — these are not useful independently.

#### Recommendation:

- Prioritize volatility and volume features in early modeling.
  
- Flag event-driven features (like BOS, OB) for further validation via SHAP or modeling performance.

- Drop or ignore date fields unless used to derive session or cyclic behavior.

---

## **Part B: Mutual Information Analysis**

Goal:

Capture non-linear dependencies between features and target_class that correlation might miss.
"""

from sklearn.feature_selection import mutual_info_classif
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Select only numeric features
numeric_df = df.select_dtypes(include=["number"])

# Define features (X) and target (y)
X_mi = numeric_df.drop(columns=["target_class"])
y_mi = numeric_df["target_class"]

# Fill NaNs with column means (MI requires no missing values)
X_mi = X_mi.fillna(X_mi.mean())

# Compute mutual information scores
mi_scores = mutual_info_classif(X_mi, y_mi, discrete_features=False, random_state=42)

# Organize into DataFrame for easier viewing
mi_df = pd.DataFrame({
    "Feature": X_mi.columns,
    "Mutual_Information": mi_scores
}).sort_values(by="Mutual_Information", ascending=False)

# Display top 20 MI features
print("\n Top 20 Features by Mutual Information Score:")
display(mi_df.head(20))

# Optional: Visualize top MI scores
plt.figure(figsize=(12, 6))
sns.barplot(x="Mutual_Information", y="Feature", data=mi_df.head(20), palette="viridis")
plt.title("Top 20 Features by Mutual Information Score")
plt.xlabel("Mutual Information Score")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

"""#### Key Outcomes:
- **H1 and H4 OHLCV** features were among the most informative, suggesting that intraday price action holds strong predictive power.
  
- **Prev_Swing_High/Low**, **Rolling_High/Low**, and **Fair_Value_Mid** ranked very high — confirming that your **smart money market structure** features carry true signal.

- MI revealed that even moderately correlated features (like `Prev_Lows`) are **conditionally important**, especially around directional pivots.

#### Recommendation:

- Retain all top H1/H4 price and volume features.
  
- Treat engineered structure features (Swing levels, Fair Value zones) as high-value inputs.

- Avoid premature removal of features based only on correlation — MI confirms hidden predictive potential.

---

## **C. SHAP VALUE ANALYSIS**
"""



"""## **Step 2: Target Class Distribution and Label Integrity**

"""

import matplotlib.pyplot as plt
import seaborn as sns

# Compute class distribution
class_counts = df['target_class'].value_counts().sort_index()
class_percent = (class_counts / class_counts.sum()) * 100

# Print details
print(" Target Class Distribution:")
print(class_counts)
print("\n Class Distribution (%):")
print(class_percent.round(2))

# Clean bar plot (FutureWarning fix applied)
plt.figure(figsize=(8, 5))
sns.barplot(x=class_counts.index, y=class_counts.values, hue=class_counts.index, palette="Set2", legend=False)
plt.title("Target Class Distribution")
plt.xlabel("Class")
plt.ylabel("Count")
plt.grid(True, axis='y', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

"""###  **Key Outcomes:**

- **Class 1 (Weak Bullish)** and **Class 3 (Weak Bearish)** dominate the dataset, making up **~69%** combined.

   
 This is expected in most markets — small directional moves happen more frequently than large ones.
  
- **Class 2 (Strong Bearish)** and **Class 4 (Strong Bullish)** each represent around **15%**, which is a healthy level for strong signal classes.

  
   These are less common but still well-represented — a good sign.

- **Class 0 (Neutral)** appears in less than **1%** of the dataset.

   
   These cases likely reflect periods with little to no movement — rare and not dominant, which aligns with a volatile market like BTC.

---

###  **Recommendations:**

- **No need for resampling**: The class distribution is well-balanced across actionable labels. The model can learn from this naturally.
  
- **Retain Class 0**: Even though neutral candles are rare, keeping them adds realism and helps the model recognize “no-trade” zones.

- **Consider class weighting** during model training (especially for strong classes like 2 and 4) to ensure they aren’t overshadowed by weaker moves.

- **No class is critically underrepresented**: Modeling can proceed with this structure without major imbalance corrections.

---

## **Step 3: Smart Money Signal Validation**

This phase is focused on validating whether our **engineered trading signals**—like **Break of Structure (BOS)**, **Liquidity Sweeps (BSL/SSL)**, **Order Blocks (OB)**, and **Fair Value Gaps (FVG)**—are truly aligned with real price direction. These features represent the foundation of smart money concepts, and now we want to assess if the data agrees with the theory.

---

### Why We're Doing It:

Anyone can generate structural labels like “bullish BOS” or “liquidity sweep,” but that doesn't guarantee they’re useful.

This step helps answer key questions:

- Do these signals consistently appear before bullish or bearish moves?

- Are some signals stronger than others in anticipating large moves?

- Are any features firing too often without contributing meaningfully?

By quantifying their alignment with actual outcomes (`target_class`), we identify which signals are worth trusting—and which might need refining or discarding.

---

### What We’ll Look At:

- The average outcome (target class) when each signal is triggered
   
- The distribution of bullish vs. bearish outcomes per signal

- The activation frequency to understand how rare or common each setup is

This gives us insight into the **precision**, **directional bias**, and **strategic usefulness** of each smart money feature.

---
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# List of binary smart money features to evaluate
signal_features = [
    'Bullish_BOS', 'Bearish_BOS', 'BSL', 'SSL', 'Bullish_OB', 'Bearish_OB',
    'Bullish_FVG', 'Bearish_FVG', 'OB_Mitigated', 'FVG_Mitigated',
    'OB_CHoCH', 'Sniper_Confluence', 'CHoCH_Bullish', 'CHoCH_Bearish'
]

# Create a results DataFrame
results = []

for feature in signal_features:
    if feature in df.columns:
        subset = df[df[feature] == 1]
        counts = subset['target_class'].value_counts(normalize=True) * 100
        avg_class = subset['target_class'].mean()
        results.append({
            'Feature': feature,
            'Avg_Target_Class': round(avg_class, 2),
            'Strong Bullish (%)': round(counts.get(4, 0), 2),
            'Weak Bullish (%)': round(counts.get(1, 0), 2),
            'Neutral (%)': round(counts.get(0, 0), 2),
            'Weak Bearish (%)': round(counts.get(3, 0), 2),
            'Strong Bearish (%)': round(counts.get(2, 0), 2),
            'Total Activations': len(subset)
        })

signal_df = pd.DataFrame(results).sort_values(by='Avg_Target_Class', ascending=False)

# Display results
pd.set_option('display.max_columns', None)
display(signal_df)

# Optional: Visualize feature strength
plt.figure(figsize=(12, 6))
sns.barplot(data=signal_df, x='Feature', y='Avg_Target_Class', palette="coolwarm")
plt.title("Average Target Class by Smart Money Feature")
plt.ylabel("Mean Target Class (0=Neutral, 4=Strong Bullish)")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""###  **Key Outcomes**

- **Bullish_BOS** stands out as the strongest individual signal. It has an average target class of **2.91**, with over **60%** of its activations leading to strong bullish outcomes.

Despite being relatively rare (1,562 activations), it is precise and should be treated as a **high-conviction entry trigger**.

- **BSL (Buy-side Liquidity Sweeps)** and **Bullish_OB (Order Blocks)** also show strong bullish bias with average target classes above **2.4**, activating frequently (over 22,000 times each). These are strong supporting signals and work well in high-confluence scenarios.

- **SSL (Sell-side Liquidity Sweeps)** has a slightly lower directional score (2.34), but still favors bullish movement — likely because price sweeps sell-side liquidity before rebounding.

- **Bearish_BOS** works inversely — with 65% of its activations followed by strong bearish moves. Its average class score is **2.33**, slightly below Bullish_BOS, but it’s effective for identifying strong downside setups.

- **FVG and OB mitigations** (both bullish and bearish) have more mixed profiles. Their activations are common, but directional precision is moderate. They may be more useful as contextual filters rather than standalone triggers.

- **Bearish_OB** interestingly shows a weaker directional edge — it leans slightly bearish but also activates heavily before weak bullish movements (48% weak bullish). May need refinement or context.

- **CHoCH, Sniper Confluence, and OB_CHoCH** show no activations in this dataset. This could be due to:

  - Extremely rare firing conditions
  

---

It is evident this features were fairly calibrated and they are not too tight or oftenly misfiring. We will proceed as is, train the baseline model and later on the tuned model with feature selection.

However we will keep in mind this outcomes to make sure feature selection keeps in mind the outcome of this process.
"""



"""## **Step 4: Temporal Pattern and Lag Analysis**

In this step, we evaluate how far back a feature’s historical values remain relevant to predicting the current target_class.

Some indicators may lose predictive power quickly, while others — especially structural or volatility-based signals — may influence outcomes several bars later.

Rather than blindly generating lag features, we’ll first identify which lag intervals actually hold predictive value.

This allows us to create only the most meaningful lagged features and avoid bloating the dataset with noise.

### **A. Lagged Correlation Analysis**

What We’ll Do:

1. Select core features: e.g., Close, ATR_14, HV, MACD, BSL, SSL.

These features were chosen based on performance in earlier EDA steps (correlation, MI, and smart money signal validation), and because they represent diverse but high-impact categories:

2. Generate lagged versions: lag_1, lag_3, lag_5, lag_10

3. Measure correlation between each lag and target_class

4. Visualize lag impact to determine optimal historical window
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Define core features to test
key_features = ["Close", "ATR_14", "HV", "MACD", "BSL", "SSL"]

# Define lags to evaluate
lags = [1, 3, 5, 10]

# Store correlation results
lagged_corrs = {"Feature": [], "Lag": [], "Correlation": []}

# Calculate lagged correlations
for feature in key_features:
    for lag in lags:
        lagged_col = f"{feature}_lag{lag}"
        df[lagged_col] = df[feature].shift(lag)
        correlation = df[lagged_col].corr(df["target_class"])
        lagged_corrs["Feature"].append(feature)
        lagged_corrs["Lag"].append(lag)
        lagged_corrs["Correlation"].append(correlation)

# Convert to DataFrame
lag_corr_df = pd.DataFrame(lagged_corrs)

# Plot lag correlation lines
plt.figure(figsize=(12, 6))
for feature in key_features:
    subset = lag_corr_df[lag_corr_df["Feature"] == feature]
    plt.plot(subset["Lag"], subset["Correlation"], marker="o", label=feature)

plt.axhline(y=0, color="gray", linestyle="--", linewidth=1)
plt.title("Lagged Correlation with Target Class")
plt.xlabel("Lag (Bars)")
plt.ylabel("Correlation")
plt.legend()
plt.grid(True, linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

"""###  **Observations:**

- **HV (Historical Volatility)** shows the strongest and most consistent correlation (~0.18 down to ~0.165). This indicates volatility maintains predictive power over several bars — it's safe to retain lags like `HV_lag1`, `lag3`, and even `lag5`.

- **ATR_14** also maintains a relatively strong and stable correlation (just under 0.10). It decays slowly, suggesting that **recent volatility range** is still influential for up to 5–10 bars.

- **BSL** shows a more modest correlation (~0.05 to ~0.025), but still shows usable predictive value. A shorter lag (e.g., `lag1`, `lag3`) might be worth keeping.

- **MACD** and **SSL** show very low correlation across all lags — their signal is likely better captured **as-is** (not lagged), or they may require other types of interaction (like combined conditions or directionality).

- **Close** has virtually no correlation with the target at any lag — consistent with expectations, since raw price doesn’t inform direction by itself in a normalized target setup.

---

###  **Recommendations:**

-  **We should include**:
  
  - `HV_lag1`, `HV_lag3`, `HV_lag5`

  - `ATR_14_lag1`, `ATR_14_lag3`

  - `BSL_lag1`, `BSL_lag3`

-  **We should skip**:

  - `Close_lags`: non-informative for this classification setup

  - `SSL_lags`, `MACD_lags`: too weak to justify unless used in a different form

---

### **Adding the confirmed lagged features to the dataset**
"""

import pandas as pd
import numpy as np

newdf = pd.read_csv("btc_labeled_dataset.csv")

# Start with a fresh copy
df = newdf.copy()

# Define lagged features to create
lag_features_to_add = {
    'HV': [1, 3, 5],
    'ATR_14': [1, 3],
    'BSL': [1, 3]
}

# Generate lagged columns
for feature, lags in lag_features_to_add.items():
    for lag in lags:
        df[f"{feature}_lag{lag}"] = df[feature].shift(lag)

print(" Lagged features created and added.")
print(" Current dataset shape:", df.shape)



"""## **4B: ACF & PACF Analysis (Temporal Memory)**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# ✅ Define correct features to analyze
features_to_check = ["Close", "ATR_14", "HV", "MACD", "BSL", "SSL"]

# ✅ Loop through each feature and plot ACF & PACF
for feature in features_to_check:
    fig, axes = plt.subplots(1, 2, figsize=(14, 4))

    # Plot ACF
    plot_acf(newdf[feature].dropna(), ax=axes[0], lags=30)
    axes[0].set_title(f"ACF for {feature}")

    # Plot PACF
    plot_pacf(newdf[feature].dropna(), ax=axes[1], lags=30, method='ywm')
    axes[1].set_title(f"PACF for {feature}")

    plt.tight_layout()
    plt.show()

"""## ACF & PACF Interpretation — Sequence Memory Profile

This analysis examines the **temporal memory** of key engineered features using ACF and PACF.

These tools allow us to quantify how far back historical values continue to exert influence, a critical input when setting the **sequence length** for deep learning models such as LSTM, GRU, or Transformer architectures.

---

###  **Close**
The ACF reveals near-perfect autocorrelation across all 30 lags, confirming that `Close` is a **non-stationary series** — its current value is heavily dependent on past prices.

However, the PACF sharply cuts off after lag 1, indicating that any direct predictive power comes almost entirely from the most recent observation.

**Implication**: `Close` should not be used directly in sequence models. Instead, its **first-differenced form** (e.g., log returns) is a more appropriate input for capturing directional change.

---

###  **ATR_14** (Average True Range) & **HV** (Historical Volatility)

Both features display **strong and persistent autocorrelation**, gradually decaying but remaining statistically significant through at least 25 lags. This indicates a high degree of temporal memory — recent volatility remains relevant across time.

PACF confirms that these are **not just short-memory indicators**; they have meaningful independent contributions at several lag intervals, especially for HV (spike again around lag 20).

**Implication**: These are ideal for sequence models. Use with a **sequence length of 20–25 time steps** to fully capture their informational footprint.

---

###  **MACD**
MACD’s ACF shows a consistent decay — slower than BSL/SSL, faster than HV — suggesting **medium-term memory**. The PACF is strongly concentrated at the first 2 lags, after which it flattens, showing diminishing marginal contribution.

**Implication**: MACD offers solid short-to-mid-term predictive structure. A **sequence window of 15–20 bars** is optimal for extracting this signal in sequential models.

---

###  **BSL & SSL** (Liquidity Sweeps)
These smart money features display the expected short memory profile. ACF drops sharply after lag 1 but maintains low positive autocorrelation through lag 10. PACF confirms that only the **first one or two lags** offer direct predictive power.

**Implication**: These event-driven signals trigger short-term movement. They can be embedded into models with shorter memory windows (e.g., 10–12 bars) or incorporated into **attention modules** in Transformer models for recent regime shifts.

---

##  Final Guidance: Sequence Length for LSTM / GRU / Transformer

- **Recommended Range**: **20–25 bars**
- Captures full volatility structure (HV, ATR)
- Balances medium-term memory (MACD)
- Includes event recency (BSL/SSL)

> This length ensures sufficient context for volatility persistence and market regime transitions while avoiding excessive noise from older lags.

---

#  **Final AI-Targeted EDA Summary**


### **Step 1: Feature Relevance and Predictive Power**

**Tools Used**: Correlation, Mutual Information, SHAP

- **Top Linear Correlations**: `HV`, `ATR_14`, `Price_Range`, and `Log_Returns` show strongest relationships to `target_class`.
- **High Mutual Information**: H1/H4 OHLCV data, `Rolling_High`, `Prev_Swing_High`, `Fair_Value_Mid`, and smart money levels provide strong non-linear dependencies.
- **SHAP (attempted)**: Technical limitations prevented final summary extraction, but MI + Corr cross-validation confirms top signal sources.

✅ **Outcome**: Identified top contributors to be retained: volatility features, structural swing levels, and multi-timeframe context. Weak or flat features (e.g., `Date_*`, `Sniper_*`) deprioritized.

---

### **Step 2: Target Class Distribution and Label Integrity**

**Class Mapping**:  
- `0`: Neutral (rare)  
- `1`: Weak Bullish  
- `2`: Strong Bearish  
- `3`: Weak Bearish  
- `4`: Strong Bullish

**Class Balance**:
- Majority: Weak Bullish (34.6%) and Weak Bearish (34%)
- Strong Bullish/Bearish: ~15% each — solid representation
- Neutral: ~0.8% — rare, but valid

✅ **Outcome**: No need for resampling. Balanced enough for modeling. Class weights may be used to slightly reinforce underrepresented strong move classes.

---

### **Step 3: Smart Money Signal Validation**

**Features Analyzed**: `Bullish_BOS`, `Bearish_BOS`, `BSL`, `SSL`, `Bullish_OB`, `FVG`, `Mitigation`, etc.

**Findings**:
- `Bullish_BOS` was the most precise trigger (Avg Target Class = 2.91, 60% → Strong Bullish).
- `BSL` and `Bullish_OB` activated frequently with strong directional follow-through.
- `SSL` and `Bearish_BOS` effective in bearish setups but with lower precision.
- `CHoCH`, `Sniper_Confluence` did not activate — logic needs review.

✅ **Outcome**: Top-performing signals retained. Others flagged for refinement. BOS, OB, and FVG-based triggers confirmed valid.

---

### **Step 4A: Lagged Feature Analysis**

**Approach**: Correlation of lag1, lag3, lag5, lag10 for key features

**Findings**:
- `HV`, `ATR_14`, and `BSL` showed meaningful lagged correlation.
- `Close`, `MACD`, and `SSL` showed low predictive power through lag.

✅ **Outcome**: Selected lagged features (`HV_lag1/3/5`, `ATR_14_lag1/3`, `BSL_lag1/3`) added to modeling dataset for XGBoost-type models.

---

### **Step 4B: ACF & PACF Analysis (Temporal Memory)**

**Features Analyzed**: `Close`, `ATR_14`, `HV`, `MACD`, `BSL`, `SSL`

**Findings**:
- `HV` and `ATR_14`: High memory — decayed slowly. Significant up to 25 lags.
- `MACD`: Mid-term memory. Effective through 15–20 lags.
- `BSL/SSL`: Short-term signals — useful for 5–10 lags.
- `Close`: Highly autocorrelated. Differencing required (use `Log_Returns` instead).

✅ **Outcome**: Recommended **sequence length for LSTM/GRU/Transformer**: **20–25 bars**, balancing volatility structure and recent signal context.

---

## ✅ Final Modeling Guidance

- **Traditional ML Models** (e.g., XGBoost): Use final engineered dataset with lag features.
- **Sequential Models** (e.g., LSTM/GRU): Structure input sequences with a lookback window of **20–25 time steps**, include `HV`, `ATR_14`, `MACD`, `BSL`, `SSL`, and structural flags (e.g., `Bullish_BOS`) as features.
- **Neutral Class**: Retain for completeness, but prioritize directional classes in evaluation metrics.

---

## **Data Cleaning**


Due to feature Engineering there is a high likelyhood we have NaN values in our dataset. We will inspect that and fix accordingly
"""

# Check total missing values per column
missing_counts = df.isna().sum()

# Filter only columns with missing values
missing = missing_counts[missing_counts > 0].sort_values(ascending=False)

# Display results
print(" Columns with missing values :")
print(missing)

# Structure_Trend, Swing_Trend

#We will use forward fill to propagate the last known structure

df['Structure_Trend'] = df['Structure_Trend'].ffill()
df['Swing_Trend'] = df['Swing_Trend'].ffill()

#SMA_200, SMA_50, SMA_10, BB_Upper, BB_Mid, BB_Lower

# we will use backfill then forward fill

smoothing_cols = ['SMA_200', 'SMA_50', 'SMA_10', 'BB_Upper', 'BB_Mid', 'BB_Lower']
df[smoothing_cols] = df[smoothing_cols].bfill().ffill()

#HV_lag1, HV_lag3, HV_lag5, ATR_14_lag1, ATR_14_lag3, BSL_lag1, BSL_lag3
#nan are natural due to shift(). We will use bfill

lag_cols = ['HV_lag1', 'HV_lag3', 'HV_lag5', 'ATR_14_lag1', 'ATR_14_lag3', 'BSL_lag1', 'BSL_lag3']
df[lag_cols] = df[lag_cols].bfill()

#HV, Avg_Volume, ATR_vs_Range, Fair_Value_Mid, Rolling_High, Rolling_Low, Price_Range, Log_Returns

# use bfill

volatility_related = ['HV', 'Avg_Volume', 'ATR_vs_Range', 'Fair_Value_Mid', 'Rolling_High',
                      'Rolling_Low', 'Price_Range', 'Log_Returns']
df[volatility_related] = df[volatility_related].bfill()

#Prev_Highs, Prev_Lows, Prev_Swing_High, Prev_Swing_Low
#use ffill

structure_levels = ['Prev_Highs', 'Prev_Lows', 'Prev_Swing_High', 'Prev_Swing_Low']
df[structure_levels] = df[structure_levels].ffill()

"""We confirm that the columns that had minimal NaN values have been fixed:"""

# Check total missing values per column
missing_counts = df.isna().sum()

# Filter only columns with missing values
missing = missing_counts[missing_counts > 0].sort_values(ascending=False)

# Display results
print(" Columns with missing values :")
print(missing)

# We will proceed to drop the NaNs in the columns

# Define structural columns to check
structure_cols = ['Structure_Trend', 'Swing_Trend', 'Prev_Swing_High', 'Prev_Swing_Low', 'Prev_Highs', 'Prev_Lows']

# Drop rows where any of these columns have missing values
df.dropna(subset=structure_cols, inplace=True)

# Confirm result
print(" Dropped rows with NaNs in structural columns.")
print(" New dataset shape:", df.shape)

# Check total missing values per column
missing_counts = df.isna().sum()

# Filter only columns with missing values
missing = missing_counts[missing_counts > 0].sort_values(ascending=False)

# Display results
print(" Columns with missing values :")
print(missing)

import numpy as np
import pandas as pd

# ✅ Step 1: Fix Swing High and Swing Low using market-aware logic
def fill_swing_values(series, reference_series, window=20):
    rolling_ref = reference_series.rolling(window=window, min_periods=1).max()
    series = series.fillna(rolling_ref)
    series = series.ffill(limit=10).bfill(limit=10)
    series = series.interpolate(method="linear", limit_direction="both")
    return series

df["Swing_High"] = fill_swing_values(df["Swing_High"], df["High"])
df["Swing_Low"] = fill_swing_values(df["Swing_Low"], df["Low"])

# ✅ Step 2: Safely regenerate Liquidity Zone and Type using .iloc
def detect_liquidity_zones(df, buffer_pips=5):
    df['Liquidity_Zone'] = np.nan
    df['Liquidity_Type'] = None

    for i in range(len(df)):
        if not pd.isna(df.iloc[i]['Swing_High']):
            df.iloc[i, df.columns.get_loc('Liquidity_Zone')] = df.iloc[i]['Swing_High'] + (buffer_pips / 10)
            df.iloc[i, df.columns.get_loc('Liquidity_Type')] = 'buy'
        elif not pd.isna(df.iloc[i]['Swing_Low']):
            df.iloc[i, df.columns.get_loc('Liquidity_Zone')] = df.iloc[i]['Swing_Low'] - (buffer_pips / 10)
            df.iloc[i, df.columns.get_loc('Liquidity_Type')] = 'sell'

    return df

df = detect_liquidity_zones(df)

# ✅ Step 3: Forward-fill OB and FVG features
df["OB_Low"] = df["OB_Low"].ffill()
df["OB_High"] = df["OB_High"].ffill()
df["FVG_Low"] = df["FVG_Low"].ffill()
df["FVG_High"] = df["FVG_High"].ffill()

# ✅ Step 4: Confirm all key missing values are handled
print("\n Remaining NaNs after fix:")
print(df[[
    "Swing_High", "Swing_Low",
    "Liquidity_Zone", "Liquidity_Type",
    "OB_Low", "OB_High", "FVG_Low", "FVG_High"
]].isna().sum())

# Drop rows where either FVG_Low or FVG_High is still NaN
df.dropna(subset=["FVG_Low", "FVG_High"], inplace=True)

# Confirm cleanup
print("\n✅ Final NaNs removed from FVG columns.")
print(df[["FVG_Low", "FVG_High"]].isna().sum())

# confirming the entire dataset does not have missing values

# Check for total missing values
total_missing = df.isna().sum().sum()

if total_missing == 0:
    print("✅ The dataset is completely clean — no NaN values found.")
else:
    print(f"⚠️ Dataset contains {total_missing} missing values.")
    print("\n📌 Columns with NaNs:")
    print(df.isna().sum()[df.isna().sum() > 0])

"""We will proceed to save the dataset in preparation for modeling"""

df.info()

"""## **SAVING THE DATASET FOR MODELING**"""

# Save cleaned dataset to CSV
df.to_csv("BTCUSD CLEAN DATA FOR MODELING.csv", index=False)

print(" Dataset saved as 'BTCUSD CLEAN DATA FOR MODELING.csv'")



"""# **Baseline Models**

We will use xgboost and lstm as the baseline models.

# **Baseline XGBOOST Model**
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

file_path = '/content/drive/MyDrive/BTCUSD CLEAN DATA FOR MODELING.csv'
df = pd.read_csv(file_path)

print(" Dataset loaded successfully.")
print(" Shape:", df.shape)
print(" Columns:", df.columns.tolist())

"""
Step 1: Define modeling objective

Step 2: Split dataset into features and target

Step 3: Train-test split with stratification

Step 4: Initialize and train a baseline XGBoost model

Step 5: Make predictions and evaluate performance"""

# Step 1: Confirm target column
target_column = 'target_class'

# Step 2: Create X (features) and y (target)
X = df.drop(columns=[target_column])
y = df[target_column]

print(" Features and target defined.")
print(" X shape:", X.shape)
print(" y distribution:")
print(y.value_counts(normalize=True).round(3))

# List of object/categorical columns to drop for baseline
non_numeric_cols = ['Timestamp', 'Structure_Trend', 'Liquidity_Type', 'Swing_Trend', 'Market_Regime']

# Drop them from X
X = X.drop(columns=non_numeric_cols, errors='ignore')

# Re-split the cleaned features
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    stratify=y,
    random_state=42
)

print(" Non-numeric columns removed and data re-split.")
print(" X_train shape:", X_train.shape)

import xgboost as xgb

# Initialize XGBoost Classifier
baseline_model = xgb.XGBClassifier(
    objective='multi:softprob',     # for multi-class probabilities
    num_class=5,                    # number of target classes
    eval_metric='mlogloss',         # suitable for multi-class
    use_label_encoder=False,        # avoid unnecessary warnings
    random_state=42,
    n_jobs=-1                       # use all cores for faster training
)

# Train the model
baseline_model.fit(X_train, y_train)

print("✅ XGBoost model training complete.")

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Make predictions
y_pred = baseline_model.predict(X_test)

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f" Accuracy: {accuracy:.4f}")

# Classification Report
print("\n Classification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)

# Plot Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=range(5), yticklabels=range(5))
plt.title(" Confusion Matrix")
plt.xlabel("Predicted Class")
plt.ylabel("True Class")
plt.show()

"""## **Analysis of the Baseline XGBOOST Model**

We've successfully built a baseline XGBoost model to classify BTC/USD price movements into five classes: Neutral, Weak Bullish, Strong Bullish, Weak Bearish, and Strong Bearish.

This baseline serves as a foundational benchmark to assess future improvements in modeling strategy.

---

### Overall Performance

- **Accuracy**: **60.9%**

- This is well above random chance (20%) for a 5-class problem, and confirms that the model is learning real patterns from the data.

- It's a strong starting point considering we haven’t applied advanced tuning, feature selection, or temporal context yet.

---

###  Class-Level Breakdown

| Class            | Precision | Recall | F1-Score | Interpretation |
|------------------|-----------|--------|----------|----------------|
| Neutral (0)      | 0.57      | 0.26   | 0.36     | Rare class, under-predicted — model struggles due to limited examples. |
| Weak Bullish (1) | 0.59      | 0.66   | 0.63     | Most common class, well predicted. |
| Strong Bearish (2)| 0.66     | 0.51   | 0.57     | Model can identify strong bearish moves fairly well. |
| Weak Bearish (3) | 0.59      | 0.66   | 0.63     | Mirrors performance of class 1 — directionally accurate. |
| Strong Bullish (4)| 0.66     | 0.49   | 0.56     | Captures strong upward moves, but recall could improve. |

---

###  Interpretation

- The model is directionally solid: It’s correctly identifying whether the market is going up or down most of the time.

- It struggles more with intensity — i.e., distinguishing between strong and weak moves.

- It performs best on common classes (1 and 3), which is expected due to higher representation in the dataset.

---

###  Weak Spot: Class 0 (Neutral)

- Low recall (26%) suggests the model almost ignores neutral movement.

- This class is rare and possibly noisy. It may benefit from relabeling, down-weighting, or being dropped entirely depending on your trading logic.

---

# **Baseline LSTM Model**

Step 1: Data Preparation for LSTM

We’ll:

Normalize the features

Create rolling sequences of 20 time steps

Align them with their future target_class label

Split into X_train, X_test, y_train, y_test
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

# ✅ Drop non-numeric columns (e.g., Timestamp, Structure_Trend, etc.)
df_lstm = df.select_dtypes(include=['float64', 'int64', 'bool'])

# ✅ Define sequence length
SEQ_LEN = 20

# ✅ Separate features and target
features = df_lstm.drop(columns=['target_class']).values
labels = df_lstm['target_class'].values

# ✅ Normalize features
scaler = MinMaxScaler()
features_scaled = scaler.fit_transform(features)

# ✅ Create rolling sequences
X_seq = []
y_seq = []

for i in range(SEQ_LEN, len(features_scaled)):
    X_seq.append(features_scaled[i-SEQ_LEN:i])
    y_seq.append(labels[i])

X_seq = np.array(X_seq)
y_seq = np.array(y_seq)

# ✅ Split into train/test sets (without stratification)
X_train, X_test, y_train, y_test = train_test_split(
    X_seq, y_seq, test_size=0.2, random_state=42
)

# ✅ Summary
print(" LSTM data preparation complete.")
print(" X shape (samples, timesteps, features):", X_seq.shape)
print(" y shape:", y_seq.shape)
print("X_train shape:", X_train.shape, " | y_train shape:", y_train.shape)
print(" X_test shape:", X_test.shape, " | y_test shape:", y_test.shape)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# ✅ Build the enhanced LSTM model
model = Sequential([
    LSTM(64, activation='relu', return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),
    LSTM(50, activation='relu'),
    Dropout(0.2),
    Dense(50, activation='relu'),
    Dense(len(np.unique(y_train)), activation='softmax')  # Multi-class output
])

# ✅ Compile the model
model.compile(
    optimizer=Adam(learning_rate=1e-4),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# ✅ Add Early Stopping
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

# ✅ Train the model
history = model.fit(
    X_train, y_train,
    epochs=300,
    batch_size=32,
    validation_data=(X_test, y_test),
    callbacks=[early_stop],
    verbose=1
)

# ✅ Print the number of epochs used
print(f" Training stopped at epoch: {len(history.history['loss'])}")

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

#  Predict class probabilities
y_pred_probs = model.predict(X_test)

#  Predicted classes
y_pred = np.argmax(y_pred_probs, axis=1)

#  Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f" Test Accuracy: {accuracy:.4f}")

#  Classification Report
print("\n Classification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
plt.title(" Confusion Matrix - LSTM")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

#  Confidence Gap Analysis
conf_gap = np.sort(np.max(y_pred_probs, axis=1) - np.partition(y_pred_probs, -2, axis=1)[:, -2])
plt.figure(figsize=(10, 4))
plt.plot(conf_gap)
plt.title(" Confidence Gap (Top 1 - Top 2 Probability)")
plt.xlabel("Test Samples (sorted)")
plt.ylabel("Confidence Gap")
plt.grid(True)
plt.show()

#  Training History
plt.figure(figsize=(12, 4))

# Loss Curve
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title("Loss Curve")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()

# Accuracy Curve
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.title("Accuracy Curve")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()

plt.tight_layout()
plt.show()



"""---

### LSTM Model Evaluation – First Benchmark Attempt

We’ve successfully trained and evaluated the first LSTM model designed to classify Bitcoin’s short-term price direction into five classes: neutral, weak bullish, strong bullish, weak bearish, and strong bearish.

With a test accuracy of approximately **57.5%**, the model demonstrates that it has learned meaningful directional patterns in the data. While the result is well above random chance (which would be 20% for five equally likely classes), it also reveals several key strengths and weaknesses that will guide further development.

---

### Strengths

The model performs reliably on the most common directional classes — weak bullish (class 1) and weak bearish (class 3). These classes represent a significant portion of the dataset, and the model has learned to recognize them with good balance between precision and recall. This suggests it has captured recurring patterns associated with softer market trends, which are often subtle and noisy in nature.

Additionally, both strong trend classes — strong bullish (class 4) and strong bearish (class 2) — show solid precision and recall. While not yet optimal, the model is already beginning to distinguish between stronger directional moves and weaker ones. This is promising, as these are the classes most relevant for active trading decisions.

---

### Weaknesses

The most noticeable weakness lies in the model's treatment of the neutral class (class 0). Despite having relatively high precision, the recall is extremely low, meaning that the model rarely identifies neutral conditions when they occur. Instead, it tends to assign these cases to trending classes — particularly weak bullish or weak bearish. This may be due to class imbalance, weak definition of neutrality, or a lack of distinguishing signals in neutral bars.

Another area for improvement is the confusion between strong and weak versions of the same directional move. The model sometimes predicts a strong bullish signal when the actual label is weak bullish, and vice versa. This reflects a challenge in distinguishing trend strength, even when the directional bias is correct.

---

### Interpretation

Overall, this model shows strong potential as a starting point. It understands the direction of price movement and correctly classifies a majority of the signals. While its ability to distinguish between trend intensity needs refinement, it already demonstrates usable predictive behavior for directional trading.

This model’s performance forms a valuable baseline from which we can iterate. Future improvements will focus on addressing the neutral class, tightening the boundary between weak and strong signals, and experimenting with architecture enhancements or loss reweighting to boost performance on underrepresented classes.

In short, this first LSTM is not perfect — but it is clearly learning, clearly directional, and a solid foundation for everything that comes next.

# **Baseline Gated Recurrent Unit**
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# Your dataframe 'df' is assumed to be loaded

# Drop rows with missing values
df.dropna(inplace=True)

# Define features: remove identifiers, timestamps, dates, and target variable
cols_to_drop = ['Date', 'Timestamp', 'Date_H1', 'Date_H4', 'Date_D', 'target_class']
X_df = df.drop(columns=cols_to_drop)
y_df = df['target_class'].astype(int)

# Encode categorical columns
non_numeric_cols = X_df.select_dtypes(include=['object']).columns
for col in non_numeric_cols:
    le = LabelEncoder()
    X_df[col] = le.fit_transform(X_df[col])

# Scale features
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X_df)

# Reshape for GRU
X = X_scaled.reshape(X_scaled.shape[0], 1, X_scaled.shape[1])
y = y_df.values

# Train-test split (time series)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, shuffle=False
)

# GRU model for multiclass classification
model = Sequential([
    GRU(64, activation='tanh', input_shape=(X_train.shape[1], X_train.shape[2])),
    Dense(5, activation='softmax')  # 5 classes (0 to 4)
])

# Compile model
model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Early stopping
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train the model
history = model.fit(
    X_train, y_train,
    epochs=200,
    batch_size=32,
    validation_split=0.1,
    callbacks=[early_stop],
    verbose=1
)

# Evaluate
loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f'Baseline GRU Test Accuracy: {accuracy:.4f}')

# Predictions
y_pred_probs = model.predict(X_test)
y_pred_classes = np.argmax(y_pred_probs, axis=1)

# Classification Report
print("Classification Report:\n", classification_report(y_test, y_pred_classes))

# Confusion Matrix
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred_classes)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('GRU Confusion Matrix')
plt.xlabel('Predicted Class')
plt.ylabel('Actual Class')
plt.show()

"""## **Analysis of the Baseline GRU model**

The GRU model demonstrates moderate overall performance with an accuracy of **62%**, but its effectiveness varies significantly across classes. Most notably, the model **completely fails to recognize Class 0**, suggesting a severe imbalance or lack of distinctive patterns for that class. This should be addressed, as it may lead to biased or unreliable predictions in production settings.

Class 3 emerges as the dominant prediction, which aligns with its strong recall — the model tends to overpredict this class, possibly due to its temporal dominance in the training data. In contrast, Class 4 shows a high precision but low recall, meaning the model is very accurate when it does predict Class 4, but often misses real instances of it.

Classes 1 and 2 perform reasonably well, with balanced precision and recall, but still show confusion with nearby classes, especially Class 3.

Overall, the model leans heavily on dominant patterns while neglecting minority classes. Enhancements like **class balancing**, **longer sequence inputs**, or incorporating **SHAP-selected features** could significantly boost its fairness and robustness.

---

# **Building the Advanced models**

We will incorporate feature selection , sorting class imbalance and hyperparameter tuning to try and improve the 2 baseline models

## **Feature Selection**

## **Feature Selection using RFE**
"""

# Core libraries
import pandas as pd
import numpy as np

# RFE and model selection
from sklearn.feature_selection import RFE
from sklearn.model_selection import train_test_split

# A model to base RFE on (commonly used: XGBoost or Logistic Regression or RandomForest)
from xgboost import XGBClassifier

from sklearn.feature_selection import RFE
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split

# Step 1: Prepare data
X = df.drop(columns=["target_class", "Timestamp", "Market_Regime", "Structure_Trend", "Liquidity_Type", "Swing_Trend"], errors='ignore')
X = X.select_dtypes(include=['number'])  # Drop any remaining non-numeric columns
y = df["target_class"]

# Step 2: Train-test split
X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Step 3: Initialize XGBoost model
model = XGBClassifier(
    use_label_encoder=False,
    eval_metric='mlogloss',
    random_state=42,
    verbosity=0
)

# Step 4: Apply RFE
rfe_selector = RFE(estimator=model, n_features_to_select=50, step=1, verbose=1)
rfe_selector.fit(X_train, y_train)

# Step 5: Get selected features
rfe_features = X_train.columns[rfe_selector.support_].tolist()

# Step 6: Display selected features
print("\n✅ Top 50 RFE-Selected Features:")
for i, feature in enumerate(rfe_features, 1):
    print(f"{i}. {feature}")

rfe_features = [
    'Open',
    'High',
    'Low',
    'Close',
    'H1_Open',
    'H1_Low',
    'H1_Close',
    'H1_Volume',
    'H4_High',
    'H4_Low',
    'H4_Close',
    'H4_Volume',
    'Daily_Open',
    'Daily_High',
    'Daily_Low',
    'Daily_Close',
    'Daily_Volume',
    'SMA_10',
    'SMA_200',
    'EMA_10',
    'EMA_50',
    'EMA_200',
    'ATR_14',
    'BB_Upper',
    'BB_Lower',
    'MACD',
    'Swing_Low',
    'Rolling_High',
    'Rolling_Low',
    'Prev_Swing_High',
    'Prev_Swing_Low',
    'Prev_Highs',
    'Prev_Lows',
    'FVG_Low',
    'FVG_High',
    'Bullish_OB',
    'Bearish_OB',
    'OB_Low',
    'OB_High',
    'OB_Mitigated',
    'Breaker_Block',
    'Fair_Value_Mid',
    'Is_Premium',
    'Is_Discount',
    'Avg_Volume',
    'Log_Returns',
    'HV',
    'Doji',
    'HV_lag1'
]

"""## **Feature Selection using SHAP analysis**"""

# ✅ Import Required Libraries
import pandas as pd
import numpy as np
import shap
import xgboost as xgb
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# ✅ Step 1: Prepare Dataset
# If 'newdf' isn't defined already, use your main df copy
newdf = df.copy()

# Drop non-numeric/identifier columns if they exist
cols_to_drop = ["Date", "Timestamp", "Date_H1", "Date_H4", "Date_D", "Market_Regime"]
newdf = newdf.drop(columns=cols_to_drop, errors='ignore')

# Encode object/categorical columns
for col in newdf.select_dtypes(include='object').columns:
    newdf[col] = LabelEncoder().fit_transform(newdf[col].astype(str))

# Convert all remaining columns to numeric, just in case
newdf = newdf.apply(pd.to_numeric, errors='coerce')

# Drop rows with missing values
newdf = newdf.dropna()

#  Step 2: Define Features (X) & Target Variable (y)
X = newdf.drop(columns=["target_class"])
y = newdf["target_class"].astype(int)

#  Step 3: Reduce Dataset Size to Prevent Crashes (Colab/SaaS Limitations)
if len(X) > 350000:
    X, _, y, _ = train_test_split(X, y, train_size=350000, random_state=42, stratify=y)

#  Step 4: Train a Lightweight XGBoost Model
model = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.01,
    use_label_encoder=False,
    eval_metric='mlogloss',
    random_state=42
)
model.fit(X, y)

#  Step 5: Compute SHAP Values using TreeExplainer (better for tree models)
explainer = shap.TreeExplainer(model)
shap_values = explainer(X)

#  Step 6: Handle Multiclass (SHAP values shape: [samples, features, classes])
if len(np.shape(shap_values.values)) == 3:
    selected_class = 1  # You can change this to any class index 0–4
    shap_values_single_class = shap_values[:, :, selected_class]
else:
    shap_values_single_class = shap_values

#  Step 7: Compute Mean SHAP Importance
shap_importance = np.abs(shap_values_single_class.values).mean(axis=0)
feature_names = X.columns.tolist()

#  Step 8: Build DataFrame with Importance Scores
shap_feature_importance = pd.DataFrame({
    'Feature': feature_names,
    'SHAP Importance': shap_importance
}).sort_values(by='SHAP Importance', ascending=False)

#  Step 9: Print & Save Top 50 Features
shap_features = shap_feature_importance.iloc[:50]['Feature'].tolist()
print("Top 50 SHAP-Selected Features:\n", shap_features)

shap_features=  ['Log_Returns', 'HV', 'OB_Mitigated', 'Breaker_Block', 'Bullish_OB', 'H4_Volume', 'ATR_vs_Range', 'HV_lag1', 'HV_lag3', 'Stoch_%D', 'Daily_Volume', 'MACD', 'Bearish_OB', 'ATR_14', 'H1_Volume', 'HV_lag5', 'ATR_14_lag1', 'Volume', 'Bearish_FVG', 'Doji', 'Daily_Low', 'Daily_High', 'High', 'Open', 'H1_Open', 'Close', 'H1_Close', 'H4_Open', 'H4_Low', 'H4_High', 'H4_Close', 'H1_High', 'H1_Low', 'Low', 'MACD_Hist', 'MACD_Signal', 'BB_Lower', 'BB_Upper', 'RSI_14', 'BB_Mid', 'EMA_50', 'EMA_200', 'SMA_50', 'SMA_10', 'EMA_10', 'SMA_200', 'Daily_Close', 'Daily_Open', 'Swing_High', 'Stoch_%K']

"""
## **Creating Combined Features**

We have selected features using SHAP and RFE.

Previously we conducted AI targeted EDA that revealed there are features that must actually assist in predicting the target class.

We will create a combined features list from all thee and experiment on the models to see the eventual impact on perfomance"""

# Provided lists
both = [
    'Log_Returns', 'HV', 'OB_Mitigated', 'Breaker_Block', 'Bullish_OB',
    'H4_Volume', 'ATR_vs_Range', 'HV_lag1', 'Stoch_%D', 'Daily_Volume',
    'MACD', 'Bearish_OB', 'ATR_14', 'H1_Volume', 'Volume', 'Doji',
    'Daily_Low', 'Daily_High', 'High', 'Open', 'H1_Open', 'Close',
    'H1_Close', 'H4_Open', 'H4_Low', 'H4_High', 'H4_Close', 'H1_High',
    'H1_Low', 'Low', 'MACD_Hist', 'MACD_Signal', 'BB_Lower', 'BB_Upper',
    'RSI_14', 'BB_Mid', 'EMA_50', 'EMA_200', 'SMA_50', 'SMA_10', 'EMA_10',
    'SMA_200', 'Daily_Close', 'Daily_Open', 'Swing_High'
]

eda_additions = [
    'CHoCH_Bullish', 'CHoCH_Bearish',
    'Bullish_BOS', 'Bearish_BOS',
    'BSL', 'SSL',
    'Bullish_FVG', 'Bearish_FVG', 'FVG_Mitigated',
    'Sniper_Confluence',
    'Fair_Value_Mid',
    'Is_Premium', 'Is_Discount',
    'Close_lag1', 'Close_lag2', 'Close_lag3', 'Close_lag4', 'Close_lag5',
    'ATR_14_lag2', 'ATR_14_lag3', 'ATR_14_lag4', 'ATR_14_lag5',
    'HV_lag2', 'HV_lag4',
    'MACD_lag1', 'MACD_lag2', 'MACD_lag3', 'MACD_lag4', 'MACD_lag5',
    'BSL_lag1', 'SSL_lag1'
]

# Combine and filter based on columns in df
raw_combined = both + [f for f in eda_additions if f not in both]
combined_features = [f for f in raw_combined if f in df.columns]

# Print result
print(f"✅ Final Combined Feature Count: {len(combined_features)}\n")
print(combined_features)

"""# **Tuned XGBOOST Model**"""

shap_features =  ['Log_Returns', 'HV', 'OB_Mitigated', 'Breaker_Block', 'Bullish_OB', 'H4_Volume', 'ATR_vs_Range', 'HV_lag1', 'HV_lag3', 'Stoch_%D', 'Daily_Volume', 'MACD', 'Bearish_OB', 'ATR_14', 'H1_Volume', 'HV_lag5', 'ATR_14_lag1', 'Volume', 'Bearish_FVG', 'Doji', 'Daily_Low', 'Daily_High', 'High', 'Open', 'H1_Open', 'Close', 'H1_Close', 'H4_Open', 'H4_Low', 'H4_High', 'H4_Close', 'H1_High', 'H1_Low', 'Low', 'MACD_Hist', 'MACD_Signal', 'BB_Lower', 'BB_Upper', 'RSI_14', 'BB_Mid', 'EMA_50', 'EMA_200', 'SMA_50', 'SMA_10', 'EMA_10', 'SMA_200', 'Daily_Close', 'Daily_Open', 'Swing_High', 'Stoch_%K']

"""# **TUNED XGBOOST MODEL with RFE FEATURES**"""

from sklearn.model_selection import RandomizedSearchCV
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# ✅ Use RFE-50 selected features
X = df[rfe_features]
y = df['target_class']

# ✅ Split data (same as before)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# ✅ Define parameter distribution for Randomized Search
param_dist = {
    'n_estimators': [100, 200, 300, 500],
    'max_depth': [3, 5, 7, 10],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'gamma': [0, 0.1, 0.2, 0.3],
    'reg_lambda': [0.1, 1, 10]
}

# ✅ Initialize base XGBoost model
base_model = XGBClassifier(
    objective='multi:softmax',
    num_class=len(np.unique(y)),
    eval_metric='mlogloss',
    use_label_encoder=False,
    verbosity=0
)

# ✅ Perform Randomized Search with CV
random_search = RandomizedSearchCV(
    estimator=base_model,
    param_distributions=param_dist,
    n_iter=100,  # Number of random combinations to try
    scoring='accuracy',
    cv=5,
    verbose=2,
    n_jobs=-1,
    random_state=42
)

# ✅ Fit the randomized search
random_search.fit(X_train, y_train)

# ✅ Best model and parameters
best_model = random_search.best_estimator_
print("\n✅ Best Hyperparameters Found:")
print(random_search.best_params_)

# ✅ Evaluate best model
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"\n✅ XGBOOST Tuned Accuracy WITH RFE: {accuracy:.4f}")
print("\n📋 Classification Report:")
print(classification_report(y_test, y_pred))

# ✅ Confusion Matrix Plot
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.title('📊 XGBOOST Confusion Matrix with RFE Features')
plt.xlabel('Predicted Class')
plt.ylabel('Actual Class')
plt.show()

"""###  **XGBOOST Tuned Model (RFE Features + Randomized Search) — Analysis**

This XGBoost model achieved an accuracy of **69.15%**, indicating strong and consistent learning across the dataset.

- **Class 1, 2, 3, and 4** show solid precision and recall, with f1-scores around **69–70%**, which suggests the model understands the main structure of the market behavior.

- **Class 0** is more challenging. The precision was good at **65%**, but the recall was lower (**32%**), meaning the model finds it harder to catch all instances of this rare class.

- The confusion matrix shows a healthy concentration along the diagonal, confirming that most predictions align correctly with the actual classes.
- Some mild confusion exists between neighboring classes, which is common, but no severe collapse into a wrong class is seen.

 **Summary:**  
> "This XGBoost model is strong, balanced across major classes, and handles the dataset well overall. Class 0 detection could still improve, but performance is reliable."

---

# **Tuned XGboost with feature selection &  optuna**
"""

!pip install optuna

#  Required Libraries
import optuna
from xgboost import XGBClassifier
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings("ignore")

#  Prepare Data (use RFE-50)
X = df[rfe_features]
y = df['target_class']

# Encode target if not numeric
if y.dtype == 'object':
    le = LabelEncoder()
    y = le.fit_transform(y)

#  Split (for final testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

#  Define Optuna Objective Function
def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 500),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'gamma': trial.suggest_float('gamma', 0, 0.5),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10.0),
        'objective': 'multi:softmax',
        'num_class': len(np.unique(y)),
        'eval_metric': 'mlogloss',
        'use_label_encoder': False,
        'verbosity': 0
    }

    model = XGBClassifier(**params)
    scores = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy')
    return scores.mean()

#  Run Optuna Study
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=30, show_progress_bar=True)

#  Print Best Results
print("\n Best Accuracy:", study.best_value)
print(" Best Hyperparameters:")
for key, val in study.best_params.items():
    print(f"  {key}: {val}")

#  Train Final Model with Best Params
best_model = XGBClassifier(**study.best_params,
                           objective='multi:softmax',
                           num_class=len(np.unique(y)),
                           eval_metric='mlogloss',
                           use_label_encoder=False,
                           verbosity=0)

best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

#  Final Evaluation
print(f"\n Final Accuracy on Test Set: {accuracy_score(y_test, y_pred):.4f}")
print("\n📋 Classification Report:")
print(classification_report(y_test, y_pred))

#  Confusion Matrix Plot (Well-Labeled)
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.title(" Confusion Matrix: XGBoost + Optuna (RFE Features)")
plt.xlabel("Predicted Class")
plt.ylabel("Actual Class")
plt.show()



"""###  **XGBOOST Model — Tuned with Optuna (RFE Features) — Analysis**

This XGBoost model achieved a **final accuracy of 70.25%**, showing a solid and consistent fit to the data.

- **Classes 1, 2, 3, and 4** are handled very well, with f1-scores around **70–72%**, indicating that the model captures the dominant market movements with good balance between precision and recall.

- **Class 0** remains harder to predict, with precision around **64%** but a lower recall at **31%**, meaning the model finds it challenging to detect all occurrences of rare market conditions.

- The confusion matrix shows a clean structure, with strong diagonals and minimal major misclassifications, suggesting the model understands the classification boundaries well.

- Some overlap still exists between adjacent classes, but this is normal in multi-class trading setups where signals can overlap naturally.

 **Summary:**  
> "This XGBoost model is robust, tuned well with Optuna, and offers reliable predictions across all major classes, making it a strong candidate for deployment."

---

## **Tuned XGBOOST Model with SHAP features and Optuna**
"""

#  Required Libraries
import optuna
from xgboost import XGBClassifier
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings("ignore")

#  Prepare Data (use RFE-50)
X = df[shap_features]
y = df['target_class']

# Encode target if not numeric
if y.dtype == 'object':
    le = LabelEncoder()
    y = le.fit_transform(y)

#  Split (for final testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

#  Define Optuna Objective Function
def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 500),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'gamma': trial.suggest_float('gamma', 0, 0.5),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10.0),
        'objective': 'multi:softmax',
        'num_class': len(np.unique(y)),
        'eval_metric': 'mlogloss',
        'use_label_encoder': False,
        'verbosity': 0
    }

    model = XGBClassifier(**params)
    scores = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy')
    return scores.mean()

#  Run Optuna Study
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=30, show_progress_bar=True)

#  Print Best Results
print("\n Best Accuracy:", study.best_value)
print(" Best Hyperparameters:")
for key, val in study.best_params.items():
    print(f"  {key}: {val}")

#  Train Final Model with Best Params
best_model = XGBClassifier(**study.best_params,
                           objective='multi:softmax',
                           num_class=len(np.unique(y)),
                           eval_metric='mlogloss',
                           use_label_encoder=False,
                           verbosity=0)

best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

#  Final Evaluation
print(f"\n Final Accuracy on Test Set: {accuracy_score(y_test, y_pred):.4f}")
print("\n Classification Report:")
print(classification_report(y_test, y_pred))

#  Confusion Matrix Plot (Well-Labeled)
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.title(" Confusion Matrix: XGBoost + Optuna (shap Features)")
plt.xlabel("Predicted Class")
plt.ylabel("Actual Class")
plt.show()

"""### **XGBOOST Model — Tuned with Optuna (SHAP Features) — Analysis**

This XGBoost model achieved a **final accuracy of 67.47%**, showing a decent but slightly lower performance compared to the earlier tuned models.

- **Classes 1, 2, 3, and 4** are still handled reasonably, but their f1-scores are a bit lower, around **67–68%**, suggesting a slight drop in model confidence and precision.
- **Class 0** remains a struggle, with a precision of **68%** but a very low recall of **29%**, meaning the model often misses these rare class cases.
- The confusion matrix shows a generally correct structure, but with more spread across wrong classes compared to previous models, indicating a little more confusion.
- Particularly for Class 2 and Class 4, there is slightly more overlap with neighboring classes, which weakens the boundary separation.

 **Summary:**  
> "This XGBoost model using SHAP-selected features performs fairly well but shows a bit more instability and slightly weaker boundary definition compared to others. It is decent, but not as robust."

---

## **XGBOOST with Combined Features and Optuna**
"""

#  Required Libraries
import optuna
from xgboost import XGBClassifier
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings("ignore")

#  Prepare Data (use RFE-50)
X = df[combined_features]
y = df['target_class']

# Encode target if not numeric
if y.dtype == 'object':
    le = LabelEncoder()
    y = le.fit_transform(y)

#  Split (for final testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

#  Define Optuna Objective Function
def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 500),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'gamma': trial.suggest_float('gamma', 0, 0.5),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10.0),
        'objective': 'multi:softmax',
        'num_class': len(np.unique(y)),
        'eval_metric': 'mlogloss',
        'use_label_encoder': False,
        'verbosity': 0
    }

    model = XGBClassifier(**params)
    scores = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy')
    return scores.mean()

#  Run Optuna Study
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=30, show_progress_bar=True)

#  Print Best Results
print("\n Best Accuracy:", study.best_value)
print(" Best Hyperparameters:")
for key, val in study.best_params.items():
    print(f"  {key}: {val}")

#  Train Final Model with Best Params
best_model = XGBClassifier(**study.best_params,
                           objective='multi:softmax',
                           num_class=len(np.unique(y)),
                           eval_metric='mlogloss',
                           use_label_encoder=False,
                           verbosity=0)

best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

#  Final Evaluation
print(f"\n Final Accuracy on Test Set: {accuracy_score(y_test, y_pred):.4f}")
print("\n Classification Report:")
print(classification_report(y_test, y_pred))

#  Confusion Matrix Plot (Well-Labeled)
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.title(" Confusion Matrix: XGBoost + Optuna (Combined Features)")
plt.xlabel("Predicted Class")
plt.ylabel("Actual Class")
plt.show()

"""###  **XGBOOST Model — Tuned with Optuna (Combined Features) — Analysis**

This XGBoost model achieved a **final accuracy of 67.92%**, showing reasonable performance but slightly below the strongest tuned versions.

- **Classes 1, 2, 3, and 4** were predicted quite well, with f1-scores mostly around **67–69%**, showing the model can handle the dominant classes with decent balance between precision and recall.

- **Class 0** remains very difficult, with precision **66%** but recall **30%**, indicating that the model detects class 0 sometimes but misses many of them.

- The confusion matrix looks fairly organized with dominant predictions along the diagonal, but there is a bit more spread between neighboring classes compared to stronger models.

- Some confusion especially happens between Class 1 and Class 3, which can affect decision making slightly.

 **Summary:**  
> "This XGBoost model trained on combined features performs reliably but shows a slight loss of sharpness compared to other fine-tuned models. It remains usable, but slightly weaker."

---

## **Final Verdict from the Tuned XGBOOST Models**

Among all tuned XGBoost models:

- **XGBoost + Optuna (RFE Features)** performed the strongest.
- It achieved a **final accuracy of 70.25%**, with the most stable balance between precision, recall, and f1-scores across all major classes.

- It also showed the cleanest confusion matrix structure, with minimal unnecessary confusion between classes.

**Chosen Model for Next Steps:**  

> **XGBoost + Optuna (RFE Features)**

---

####  **Next Step:**

We will proceed with it for ensembling later on in this project.

# **Advanced LSTM Model with RFE**
"""

# ✅ Import Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ✅ Prepare Data
X = df[rfe_features]
y = df['target_class']

# Encode target if it's categorical
if y.dtype == 'object':
    from sklearn.preprocessing import LabelEncoder
    y = LabelEncoder().fit_transform(y)

# ✅ Scale Features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ✅ Reshape for LSTM (timesteps = 1)
X_lstm = X_scaled.reshape(X_scaled.shape[0], 1, X_scaled.shape[1])

# ✅ Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X_lstm, y, test_size=0.2, random_state=42, stratify=y
)

print(f"✅ Training Set Shape: {X_train.shape}, Testing Set Shape: {X_test.shape}")

# ✅ Build LSTM Model
model = Sequential([
    LSTM(64, activation='relu', return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),
    LSTM(50, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),
    Dense(50, activation='relu'),
    BatchNormalization(),
    Dense(len(np.unique(y)), activation='softmax')  # Corrected: softmax for multiclass
])

# ✅ Compile Model
model.compile(
    optimizer=Adam(learning_rate=0.0001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# ✅ Callbacks
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.5, verbose=1)

# ✅ Train Model
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=700,
    batch_size=32,
    callbacks=[early_stop, reduce_lr],
    verbose=1
)

print(f"✅ Training stopped at epoch: {len(history.history['loss'])}")

# ✅ Evaluation
y_pred = np.argmax(model.predict(X_test), axis=1)
accuracy = accuracy_score(y_test, y_pred)
print(f"\n LSTM Accuracy with rfe_features: {accuracy:.4f}")

print("\n Classification Report:")
print(classification_report(y_test, y_pred))

# ✅ Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(7, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.xlabel("Predicted Class")
plt.ylabel("Actual Class")
plt.title(" Confusion Matrix - Optimized LSTM Model with RFE")
plt.show()

"""### **Optimized LSTM Model — RFE Features — Analysis**

This LSTM model achieved a **final accuracy of 68.15%**, showing decent learning performance.

- **Classes 1, 2, 3, and 4** are handled fairly well, with f1-scores around **67–69%**, indicating good general behavior prediction for the majority classes.

- **Class 0** remains weak, with a low f1-score of **22%**, mainly due to very poor recall (**14%**), meaning the model rarely catches the rare class correctly.

- The confusion matrix shows clean diagonals for strong classes but a lot of spread for Class 0 and slight overlaps between Classes 1, 3, and 4.
- General trend shows the model understands the dominant market movements but struggles with rare signals.

 **Summary:**  
> "This LSTM model captures the major class patterns well but still struggles significantly with rare events (Class 0). Overall, it is reasonably strong and reliable."

---

## **Advanced LSTM with SHAP Features**
"""

# ✅ Import Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ✅ Prepare Data
X = df[shap_features]
y = df['target_class']

# Encode target if it's categorical
if y.dtype == 'object':
    from sklearn.preprocessing import LabelEncoder
    y = LabelEncoder().fit_transform(y)

# ✅ Scale Features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ✅ Reshape for LSTM (timesteps = 1)
X_lstm = X_scaled.reshape(X_scaled.shape[0], 1, X_scaled.shape[1])

# ✅ Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X_lstm, y, test_size=0.2, random_state=42, stratify=y
)

print(f"✅ Training Set Shape: {X_train.shape}, Testing Set Shape: {X_test.shape}")

# ✅ Build LSTM Model
model = Sequential([
    LSTM(64, activation='relu', return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),
    LSTM(50, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),
    Dense(50, activation='relu'),
    BatchNormalization(),
    Dense(len(np.unique(y)), activation='softmax')  # Corrected: softmax for multiclass
])

# ✅ Compile Model
model.compile(
    optimizer=Adam(learning_rate=0.0001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# ✅ Callbacks
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.5, verbose=1)

# ✅ Train Model
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=700,
    batch_size=32,
    callbacks=[early_stop, reduce_lr],
    verbose=1
)

print(f"✅ Training stopped at epoch: {len(history.history['loss'])}")

# ✅ Evaluation
y_pred = np.argmax(model.predict(X_test), axis=1)
accuracy = accuracy_score(y_test, y_pred)
print(f"\n✅ LSTM Accuracy with shap_features: {accuracy:.4f}")

print("\n📋 Classification Report:")
print(classification_report(y_test, y_pred))

# ✅ Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(7, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.xlabel("Predicted Class")
plt.ylabel("Actual Class")
plt.title(" Confusion Matrix - Optimized LSTM Model with shap features")
plt.show()

"""###  **Optimized LSTM Model — SHAP Features — Analysis**

This LSTM model achieved a **final accuracy of 68.04%**, which is consistent but not stronger than the best runs.

- **Classes 1, 2, 3, and 4** were predicted fairly well, with f1-scores around **67–69%**, showing that the model maintains decent handling of majority classes.

- **Class 0** struggled heavily, with a very low f1-score of **19%**, mainly because of very poor recall (**12%**), indicating the model fails to detect most rare cases.

- The confusion matrix reveals a good concentration along the diagonal for strong classes, but heavy spread and misclassification around Class 0.

- Overall learning was stable, but feature selection limited the model's full ability to differentiate finer edges.

 **Summary:**  
> "This LSTM model using SHAP features is reasonably stable for main classes but very weak for minority detection, making it less reliable overall."

---

## **Advanced LSTM model with Combined Features**
"""

# ✅ Import Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ✅ Prepare Data
X = df[combined_features]
y = df['target_class']

# Encode target if it's categorical
if y.dtype == 'object':
    from sklearn.preprocessing import LabelEncoder
    y = LabelEncoder().fit_transform(y)

# ✅ Scale Features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ✅ Reshape for LSTM (timesteps = 1)
X_lstm = X_scaled.reshape(X_scaled.shape[0], 1, X_scaled.shape[1])

# ✅ Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X_lstm, y, test_size=0.2, random_state=42, stratify=y
)

print(f"✅ Training Set Shape: {X_train.shape}, Testing Set Shape: {X_test.shape}")

# ✅ Build LSTM Model
model = Sequential([
    LSTM(64, activation='relu', return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),
    LSTM(50, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),
    Dense(50, activation='relu'),
    BatchNormalization(),
    Dense(len(np.unique(y)), activation='softmax')  # Corrected: softmax for multiclass
])

# ✅ Compile Model
model.compile(
    optimizer=Adam(learning_rate=0.0001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# ✅ Callbacks
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.5, verbose=1)

# ✅ Train Model
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=700,
    batch_size=32,
    callbacks=[early_stop, reduce_lr],
    verbose=1
)

print(f"✅ Training stopped at epoch: {len(history.history['loss'])}")

# ✅ Evaluation
y_pred = np.argmax(model.predict(X_test), axis=1)
accuracy = accuracy_score(y_test, y_pred)
print(f"\n✅ LSTM Accuracy with combined_features: {accuracy:.4f}")

print("\n Classification Report:")
print(classification_report(y_test, y_pred))

# ✅ Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(7, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.xlabel("Predicted Class")
plt.ylabel("Actual Class")
plt.title(" Confusion Matrix - Optimized LSTM Model with combined features")
plt.show()

"""###   **Optimized LSTM Model — Combined Features — Analysis**

This LSTM model achieved a **final accuracy of 67.78%**, placing it slightly below the best performing LSTM runs.

- **Classes 1, 2, 3, and 4** showed consistent f1-scores around **66–69%**, indicating the model handles dominant classes steadily.

- **Class 0** was poorly predicted, with an extremely low f1-score of **18%**, driven by a recall of only **11%**, meaning rare class detection was very weak.

- The confusion matrix reveals a fair diagonal structure for strong classes but an almost complete miss on Class 0 behavior.

- Overall, the model retains good consistency for larger classes but sacrifices rare event recognition significantly.

 **Summary:**  
> "This LSTM model using combined features is stable for main market classes but notably weak at identifying rare signals, making it less reliable for sensitive decision-making."

---

### **Final Verdict — Tuned LSTM Models**

Among all the tuned LSTM models:

- **Optimized LSTM (RFE Features)** performed the strongest.

- It achieved the **highest final accuracy of 68.15%** and maintained the most stable f1-scores across Classes 1, 2, 3, and 4.

- While rare class (Class 0) detection was still weak, this model handled the main market movements more consistently and showed the cleanest confusion matrix structure among the LSTM variations.

✅ **Chosen Model for Next Steps:**  
> **Optimized LSTM (RFE Features)**

---

## **Advanced GRU Model**

## **Advanced GRU with RFE (No weighting))**
"""

# ✅ Import Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ✅ Prepare Data
X = df[rfe_features]
y = df['target_class']

# Encode target if needed
if y.dtype == 'object':
    le = LabelEncoder()
    y = le.fit_transform(y)

# ✅ Scale Features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ✅ Reshape for GRU (timesteps = 1)
X_gru = X_scaled.reshape(X_scaled.shape[0], 1, X_scaled.shape[1])

# ✅ Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X_gru, y, test_size=0.2, stratify=y, random_state=42
)

print(f"✅ X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")

# ✅ Build Advanced GRU Model
model = Sequential([
    GRU(128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]), activation='tanh'),
    Dropout(0.3),
    BatchNormalization(),

    GRU(64, return_sequences=False, activation='tanh'),
    Dropout(0.3),
    BatchNormalization(),

    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(len(np.unique(y)), activation='softmax')
])

# ✅ Compile Model
model.compile(
    optimizer=Adam(learning_rate=0.0001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# ✅ Callbacks
early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=7, factor=0.5, verbose=1)

# ✅ Train Model (WITHOUT class_weight)
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=300,
    batch_size=32,
    callbacks=[early_stop, reduce_lr],
    verbose=1
)

# ✅ Evaluation
y_pred = np.argmax(model.predict(X_test), axis=1)
accuracy = accuracy_score(y_test, y_pred)
print(f"\n Advanced GRU Accuracy + RFE (No Class Weights): {accuracy:.4f}")
print("\n Classification Report:")
print(classification_report(y_test, y_pred))

# ✅ Confusion Matrix
plt.figure(figsize=(7, 6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.title(" Confusion Matrix - Advanced GRU Model  + RFE (No Class Weights)")
plt.xlabel("Predicted Class")
plt.ylabel("Actual Class")
plt.show()

"""### **Advanced GRU Model — RFE Features (No Class Weights) — Analysis**

This GRU model achieved a **final accuracy of 68.12%**, showing strong consistency with the dominant class structures.

- **Classes 1, 2, 3, and 4** achieved f1-scores around **67–69%**, reflecting solid pattern recognition for the major market movements.

- **Class 0** remained extremely weak, with an f1-score of only **15%**, driven by very low recall (**9%**), meaning the model rarely identifies rare market behavior.

- The confusion matrix shows clear strong diagonals for the dominant classes, but rare events are largely missed or misclassified into adjacent classes.

- Overall, the model demonstrates stability for main classes but sacrifices rare event sensitivity.

 **Summary:**  
> "This GRU model without class balancing is steady for regular market classes but extremely weak at catching rare signals, limiting its reliability for rare-event forecasting."

---

## **Advanced GRU with RFE features & class weighting**
"""

# ✅ Import Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight

# ✅ Prepare Data
X = df[rfe_features]
y = df['target_class']

# Encode target if needed
if y.dtype == 'object':
    le = LabelEncoder()
    y = le.fit_transform(y)

# ✅ Handle Class Imbalance
class_weights = dict(zip(
    np.unique(y),
    compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)
))
print("✅ Computed class weights:", class_weights)

# ✅ Scale Features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ✅ Reshape for GRU (timesteps = 1)
X_gru = X_scaled.reshape(X_scaled.shape[0], 1, X_scaled.shape[1])

# ✅ Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X_gru, y, test_size=0.2, stratify=y, random_state=42
)

print(f"✅ X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")

# ✅ Build Advanced GRU Model
model = Sequential([
    GRU(128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]), activation='tanh'),
    Dropout(0.3),
    BatchNormalization(),

    GRU(64, return_sequences=False, activation='tanh'),
    Dropout(0.3),
    BatchNormalization(),

    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(len(np.unique(y)), activation='softmax')
])

# ✅ Compile Model
model.compile(
    optimizer=Adam(learning_rate=0.0001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# ✅ Callbacks
early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=7, factor=0.5, verbose=1)

# ✅ Train Model
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=300,
    batch_size=32,
    class_weight=class_weights,
    callbacks=[early_stop, reduce_lr],
    verbose=1
)

# ✅ Evaluation
y_pred = np.argmax(model.predict(X_test), axis=1)
accuracy = accuracy_score(y_test, y_pred)
print(f"\n Advanced GRU Accuracy + weighting + RFE: {accuracy:.4f}")
print("\n Classification Report:")
print(classification_report(y_test, y_pred))

# ✅ Confusion Matrix
plt.figure(figsize=(7, 6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.title("📊 Confusion Matrix - Advanced GRU Model with RFE + weighting")
plt.xlabel("Predicted Class")
plt.ylabel("Actual Class")
plt.show()

"""### **Advanced GRU Model — RFE Features + Class Weighting — Analysis**

This GRU model achieved a **final accuracy of 49.94%**, representing a major drop in overall performance.

- **Class 0** recall improved massively (91%), meaning the model now catches most rare events, but at a huge cost to overall accuracy and balance.

- **Classes 1, 2, 3, and 4** f1-scores dropped heavily, sitting around **50–53%**, meaning the model's ability to predict dominant classes collapsed.

- The confusion matrix shows heavy confusion across all major classes, with weak separation and loss of the strong diagonal structure seen earlier.

- Overall, the model became overly sensitive to rare classes but poor for general market prediction.

 **Summary:**  

> "Applying class weighting severely damaged the model’s balance. While rare class detection improved, the overall classification quality across main classes deteriorated badly."

---

## **Advanced GRU + SHAP no weighting**
"""

# ✅ Import Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ✅ Prepare Data
X = df[shap_features]
y = df['target_class']

# Encode target if needed
if y.dtype == 'object':
    le = LabelEncoder()
    y = le.fit_transform(y)

# ✅ Scale Features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ✅ Reshape for GRU (timesteps = 1)
X_gru = X_scaled.reshape(X_scaled.shape[0], 1, X_scaled.shape[1])

# ✅ Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X_gru, y, test_size=0.2, stratify=y, random_state=42
)

print(f"✅ X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")

# ✅ Build Advanced GRU Model
model = Sequential([
    GRU(128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]), activation='tanh'),
    Dropout(0.3),
    BatchNormalization(),

    GRU(64, return_sequences=False, activation='tanh'),
    Dropout(0.3),
    BatchNormalization(),

    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(len(np.unique(y)), activation='softmax')
])

# ✅ Compile Model
model.compile(
    optimizer=Adam(learning_rate=0.0001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# ✅ Callbacks
early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=7, factor=0.5, verbose=1)

# ✅ Train Model (WITHOUT class_weight)
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=300,
    batch_size=32,
    callbacks=[early_stop, reduce_lr],
    verbose=1
)

# ✅ Evaluation
y_pred = np.argmax(model.predict(X_test), axis=1)
accuracy = accuracy_score(y_test, y_pred)
print(f"\n Advanced GRU Accuracy + SHAP (No Class Weights): {accuracy:.4f}")
print("\n Classification Report:")
print(classification_report(y_test, y_pred))

# ✅ Confusion Matrix
plt.figure(figsize=(7, 6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.title(" Confusion Matrix - Advanced GRU Model  + SHAP (No Class Weights)")
plt.xlabel("Predicted Class")
plt.ylabel("Actual Class")
plt.show()

"""### **Advanced GRU Model — SHAP Features (No Class Weights) — Analysis**

This GRU model achieved a **final accuracy of 69.14%**, marking a strong and consistent performance.

- **Classes 1, 2, 3, and 4** maintained f1-scores around **68–70%**, showing the model reliably captures the main structure of the market behavior.

- **Class 0** remains a difficult case, with a low f1-score of **19%** and recall at **12%**, meaning rare event prediction is still weak.

- The confusion matrix is clean and shows dominant diagonals for majority classes, confirming stability in regular class predictions.

- The model retained very good balance between precision and recall for the major classes without being overly sensitive or overly conservative.

**Summary:**  

> "This GRU model tuned with SHAP-selected features is stable, balanced, and handles major classes well. It is strong and reliable for main market structures, despite weakness in rare event detection."

---

###  **Final Verdict — Tuned GRU Models**

Among all the GRU models evaluated:

- **Advanced GRU Model + SHAP Features (No Class Weights)** performed the best.

- It achieved the **highest accuracy of 69.14%**, with stable and consistent f1-scores across Classes 1, 2, 3, and 4.

- The confusion matrix showed the clearest separation with minimal major class confusion.

- It maintained strong general market structure prediction without sacrificing performance for rare classes.

**Chosen Model for Next Steps:**  

> **Advanced GRU Model + SHAP Features (No Class Weights)**

---

# **Building N-beats Classifier Model**
"""

# ✅ Import libraries
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# ✅ Prepare data
X = df[rfe_features]  # or full feature list you prefer
y = df['target_class']

# Encode target if needed
if y.dtype == 'object':
    le = LabelEncoder()
    y = le.fit_transform(y)

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y, random_state=42)

print(f"✅ X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")

# ✅ Define N-BEATS Block
def NBeatsBlock(input_dim, hidden_dim=128):
    model = tf.keras.Sequential([
        layers.Dense(hidden_dim, activation='relu'),
        layers.Dense(hidden_dim, activation='relu'),
        layers.Dense(input_dim, activation='linear')
    ])
    return model

# ✅ Build N-BEATS Classifier
input_layer = layers.Input(shape=(X_train.shape[1],))

# Stack multiple N-BEATS blocks
x = NBeatsBlock(X_train.shape[1])(input_layer)
x = NBeatsBlock(X_train.shape[1])(x)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.3)(x)
x = layers.Dense(64, activation='relu')(x)

# Output
output_layer = layers.Dense(len(np.unique(y)), activation='softmax')(x)

model = models.Model(inputs=input_layer, outputs=output_layer)

# ✅ Compile
model.compile(
    optimizer=optimizers.Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# ✅ Callbacks
early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)
reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', patience=7, factor=0.5, verbose=1)

# ✅ Train
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=300,
    batch_size=64,
    callbacks=[early_stop, reduce_lr],
    verbose=1
)

# ✅ Evaluate
y_pred = np.argmax(model.predict(X_test), axis=1)
accuracy = accuracy_score(y_test, y_pred)

print(f"\n✅ N-BEATS Classifier Accuracy: {accuracy:.4f}")
print("\n📋 Classification Report:")
print(classification_report(y_test, y_pred))

# ✅ Confusion Matrix
plt.figure(figsize=(7,6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.xlabel('Predicted Class')
plt.ylabel('Actual Class')
plt.title('N-BEATS Classifier Confusion Matrix')
plt.tight_layout()
plt.show()

"""### **N-BEATS Classifier — Analysis**

This N-BEATS model achieved a **final accuracy of 67.83%**, showing good overall structure recognition.

- **Classes 1, 2, 3, and 4** achieved decent f1-scores between **67–69%**, proving that the model captured dominant market movements relatively well.

- **Class 0** remained a significant weak spot, with an f1-score of **24%**, mainly due to a low recall of **16%**, meaning rare event detection was still very poor.

- The confusion matrix structure looks organized but reveals a slight spread and overlap, particularly between neighboring classes.

- The model was generally reliable for major classes but lacked the sharpness seen in the best tuned GRU and LSTM models.

**Summary:**  
> "The N-BEATS classifier is stable for dominant classes but struggles with rare events, offering good but not outstanding overall performance."

---

## **Building an Advanced N-beats classifier model**
"""

# ✅ Import libraries
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# ✅ Prepare your data
X = df[rfe_features]  # or the features you want
y = df['target_class']

# Encode target if needed
if y.dtype == 'object':
    le = LabelEncoder()
    y = le.fit_transform(y)

# Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, stratify=y, random_state=42
)

print(f"✅ X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")

# ✅ Define an improved N-BEATS block
def ImprovedNBeatsBlock(input_dim, hidden_dim=256):
    x = layers.Input(shape=(input_dim,))
    y = layers.Dense(hidden_dim, activation='relu')(x)
    y = layers.BatchNormalization()(y)
    y = layers.Dropout(0.3)(y)
    y = layers.Dense(hidden_dim, activation='relu')(y)
    y = layers.BatchNormalization()(y)
    y = layers.Dense(input_dim, activation='linear')(y)
    return models.Model(inputs=x, outputs=y)

# ✅ Build the improved N-BEATS Classifier
input_layer = layers.Input(shape=(X_train.shape[1],))

# Stack multiple blocks
x = ImprovedNBeatsBlock(X_train.shape[1])(input_layer)
x = ImprovedNBeatsBlock(X_train.shape[1])(x)
x = ImprovedNBeatsBlock(X_train.shape[1])(x)

# Final dense layers
x = layers.Dense(256, activation='relu')(x)
x = layers.BatchNormalization()(x)
x = layers.Dropout(0.4)(x)
x = layers.Dense(128, activation='relu')(x)

# Output
output_layer = layers.Dense(len(np.unique(y)), activation='softmax')(x)

# Define the model
model = models.Model(inputs=input_layer, outputs=output_layer)

# ✅ Compile
model.compile(
    optimizer=optimizers.Adam(learning_rate=0.0005),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# ✅ Callbacks
early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)
reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', patience=10, factor=0.5, verbose=1)

# ✅ Train the model
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=400,
    batch_size=128,
    callbacks=[early_stop, reduce_lr],
    verbose=1
)

# ✅ Evaluate
y_pred = np.argmax(model.predict(X_test), axis=1)
accuracy = accuracy_score(y_test, y_pred)

print(f"\n✅ Improved N-BEATS Classifier Accuracy: {accuracy:.4f}")
print("\n📋 Classification Report:")
print(classification_report(y_test, y_pred))

# ✅ Confusion Matrix
plt.figure(figsize=(7,6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.xlabel('Predicted Class')
plt.ylabel('Actual Class')
plt.title('Improved N-BEATS Classifier Confusion Matrix')
plt.tight_layout()
plt.show()

"""### **Improved N-BEATS Classifier — Analysis**

The improved N-BEATS model achieved a **final accuracy of 69.66%**, showing clear gains in learning more complex market structures.

- **Classes 1, 2, 3, and 4** improved notably, reaching f1-scores around **68–71%**, demonstrating a stronger understanding of major market behaviors compared to the baseline.

- **Class 0** detection remained weak, with an f1-score of **22%**, slightly lower than desired but still consistent with its rarity in the dataset.

- The confusion matrix exhibits tighter clustering around the correct predictions, indicating a reduction in severe misclassifications, particularly between Classes 1, 3, and 4.

- Overall, this upgraded version displays better stability, sharper decision boundaries, and more confident handling of dominant classes compared to the original baseline.

**Summary:**  
> "The improved N-BEATS model substantially boosts classification strength, offering more precise predictions across major classes while still struggling with rare anomalies."

---

## 🏆 Final Verdict: N-BEATS Classifiers

- The **baseline N-BEATS model** reached **67.83%** accuracy, delivering reliable results on dominant classes but visibly struggled with rare class detection and slightly wider confusion across neighboring categories.

- The **improved N-BEATS model** achieved a much stronger **69.66%** accuracy. It delivered **better precision, recall, and f1-scores across all major classes (1–4)** while also slightly tightening the error spread seen in the confusion matrix.

- **Class 0** detection (rare events) remained difficult in both models but slightly better handled structurally in the improved model.

### **Final Decision:**

> The Improved N-BEATS model is clearly superior and will be the **chosen N-BEATS variant** for any ensembling or stacking approaches moving forward.

---

## **Building the Baseline Transformer Classifier Model**
"""

# Import Libraries
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Prepare Data
X = df[rfe_features]  # Use RFE features or any feature set you choose
y = df['target_class']

# Encode target if needed
if y.dtype == 'object':
    le = LabelEncoder()
    y = le.fit_transform(y)

# Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, stratify=y, random_state=42
)

print(f"✅ X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")

# Define Transformer Encoder Block
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    # Multi-Head Attention
    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)
    x = layers.Dropout(dropout)(x)
    x = layers.LayerNormalization(epsilon=1e-6)(x)
    res = x + inputs

    # Feed Forward Part
    x = layers.Dense(ff_dim, activation="relu")(res)
    x = layers.Dense(inputs.shape[-1])(x)
    x = layers.Dropout(dropout)(x)
    return layers.LayerNormalization(epsilon=1e-6)(x + res)

# Build the Transformer Classifier
input_layer = layers.Input(shape=(X_train.shape[1],))
x = layers.Reshape((1, X_train.shape[1]))(input_layer)  # Reshape for attention mechanism

x = transformer_encoder(x, head_size=64, num_heads=2, ff_dim=128, dropout=0.1)
x = layers.GlobalAveragePooling1D()(x)
x = layers.Dense(128, activation="relu")(x)
x = layers.Dropout(0.2)(x)
output_layer = layers.Dense(len(np.unique(y)), activation="softmax")(x)

model = models.Model(inputs=input_layer, outputs=output_layer)

# Compile Model
model.compile(
    optimizer=optimizers.Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Callbacks
early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)
reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, verbose=1)

# Train Model
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=200,
    batch_size=128,
    callbacks=[early_stop, reduce_lr],
    verbose=1
)

# Evaluate Model
y_pred = np.argmax(model.predict(X_test), axis=1)
accuracy = accuracy_score(y_test, y_pred)

print(f"\n✅ Baseline Transformer Classifier Accuracy: {accuracy:.4f}")
print("\n📋 Classification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
plt.figure(figsize=(7,6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.xlabel('Predicted Class')
plt.ylabel('Actual Class')
plt.title('Baseline Transformer Classifier Confusion Matrix')
plt.tight_layout()
plt.show()



"""### **Baseline Transformer Classifier — Analysis**

This first Transformer model achieved a **final accuracy of 68.78%**, indicating a strong early grasp of the dominant classes.

- **Classes 1, 2, 3, and 4** achieved solid f1-scores around **68–69%**, meaning the model was already fairly capable of predicting the major structure of the target classes.

- **Class 0** showed the usual fragility, with a low f1-score of **28%**, driven by poor recall at **19%**, meaning that rare or abnormal events remained difficult for the model to identify properly.

- The confusion matrix shows a relatively clear block diagonal, although there’s still meaningful class confusion, especially between Classes 1-3, showing room for better separation.

- Overall, the model performed **consistently well** out of the box, but **lacked deep precision and tight class boundary control** expected in highly tuned models.

**Summary:**  
> "The baseline Transformer model is a strong starting point, capturing major class behaviors well, but still leaves opportunity for tuning to sharpen precision and rare event detection."

---

## **Advanced Transformer Model**
"""

# ✅ Import libraries
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# ✅ Assume df and rfe_features are already loaded

# Prepare data
X = df[rfe_features]
y = df['target_class']

# Encode target if needed
if y.dtype == 'object':
    le = LabelEncoder()
    y = le.fit_transform(y)

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, stratify=y, random_state=42
)

# ✅ Define a Transformer Encoder Block
def TransformerEncoderBlock(embed_dim, num_heads, ff_dim, rate=0.1):
    inputs = layers.Input(shape=(embed_dim,))
    x = layers.Reshape((1, embed_dim))(inputs)

    attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x, x)
    attn_output = layers.Dropout(rate)(attn_output)
    out1 = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)

    ffn = layers.Dense(ff_dim, activation='relu')(out1)
    ffn = layers.Dense(embed_dim)(ffn)
    ffn = layers.Dropout(rate)(ffn)
    out2 = layers.LayerNormalization(epsilon=1e-6)(out1 + ffn)

    out2 = layers.Flatten()(out2)
    return models.Model(inputs, out2)

# ✅ Build the Advanced Transformer Classifier
embed_dim = X_train.shape[1]
num_heads = 8
ff_dim = 512

inputs = layers.Input(shape=(embed_dim,))
x = TransformerEncoderBlock(embed_dim, num_heads, ff_dim)(inputs)
x = layers.Dense(512, activation='relu')(x)
x = layers.BatchNormalization()(x)
x = layers.Dropout(0.4)(x)
x = layers.Dense(256, activation='relu')(x)
x = layers.BatchNormalization()(x)
x = layers.Dropout(0.3)(x)
outputs = layers.Dense(len(np.unique(y)), activation='softmax')(x)

model = models.Model(inputs, outputs)

# ✅ Compile the model
model.compile(
    optimizer=optimizers.Adam(learning_rate=0.0003),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# ✅ Callbacks
early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)
reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', patience=10, factor=0.5, verbose=1)

# ✅ Train the model
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=400,
    batch_size=128,
    callbacks=[early_stop, reduce_lr],
    verbose=1
)

# ✅ Evaluate
y_pred = np.argmax(model.predict(X_test), axis=1)
accuracy = accuracy_score(y_test, y_pred)

print(f"\n✅ Advanced Transformer Classifier Accuracy: {accuracy:.4f}")
print("\n📋 Classification Report:")
print(classification_report(y_test, y_pred))

# ✅ Confusion Matrix
plt.figure(figsize=(7,6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.xlabel('Predicted Class')
plt.ylabel('Actual Class')
plt.title('Advanced Transformer Classifier Confusion Matrix')
plt.tight_layout()
plt.show()

"""### **Advanced Transformer Classifier — Analysis**

The advanced Transformer model achieved a **final accuracy of 72.39%**, clearly outperforming the baseline version and moving into strong performance territory.

- **Classes 1, 2, 3, and 4** all posted **f1-scores between 71–73%**, reflecting excellent structure recognition across major market patterns.
  
- **Class 0** remained challenging, with an f1-score of **30%** — slightly better than previous models but still demonstrating the difficulty in capturing rare edge cases.

- The **confusion matrix** shows a very tight clustering, especially for dominant classes, with reduced confusion spillovers compared to earlier models.

- The model demonstrates **stable generalization** and better adaptation across market regimes, making it very promising for next steps like stacking or ensembling.

**Summary:**  
> "The advanced Transformer is a top-performing model, with consistent strength across main classes and only minor struggles on rare events. A strong candidate for further ensembling and deployment."

---

## Final Verdict: Best Individual Model

### 1. **Advanced Transformer Classifier**
- **Accuracy**: **72.39%**
- **Strengths**:  
  - Strong precision, recall, and f1-scores across **all classes**.
  - Best recall/f1 for **Class 0 (rare events)** among all models so far.
  - Minimal class confusion and very organized confusion matrix.
- **Summary**:  
  > *"The Advanced Transformer model provides the most balanced and powerful performance, especially on both dominant and rare classes. It clearly stands out."*

---

### Runner-Up Models

| Model | Accuracy | Notes |
|:---|:---|:---|
| **Improved N-BEATS** | 69.66% | Strong on major classes but slightly weaker recall on rare events. |
| **XGBoost Optuna (RFE Features)** | 70.25% | Very reliable on major classes but weak on rare classes compared to Transformer. |
| **GRU Advanced (SHAP No Weights)** | 69.14% | Consistent, but not as sharp on dominant classes. |
| **LSTM (RFE Features)** | 68.15% | Good generalization, but relatively weaker on rare classes and Class 0 detection. |

---

#  Final Call:
 **Advanced Transformer Classifier** is the best overall foundation for further work:
- Best **accuracy**,
- Best **class balance**,
- Best **rare event sensitivity**.

---

# **Building the Ensemble Models**

# **Hybrid Advanced n-beats + Advanced  Transformer Classifier +  Tuned XGboost**
"""

!pip install optuna

# ✅ Import all libraries
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import optuna
from xgboost import XGBClassifier
import warnings
warnings.filterwarnings("ignore")

# ✅ Prepare Data
X = df[rfe_features]
y = df['target_class']

# Encode if needed
if y.dtype == 'object':
    le = LabelEncoder()
    y = le.fit_transform(y)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, stratify=y, random_state=42
)

# ✅ Build and Train Advanced Transformer
def build_advanced_transformer(input_dim):
    def TransformerEncoderBlock(embed_dim, num_heads, ff_dim, rate=0.1):
        inputs = layers.Input(shape=(embed_dim,))
        x = layers.Reshape((1, embed_dim))(inputs)
        attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x, x)
        attn_output = layers.Dropout(rate)(attn_output)
        out1 = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)
        ffn = layers.Dense(ff_dim, activation='relu')(out1)
        ffn = layers.Dense(embed_dim)(ffn)
        ffn = layers.Dropout(rate)(ffn)
        out2 = layers.LayerNormalization(epsilon=1e-6)(out1 + ffn)
        out2 = layers.Flatten()(out2)
        return models.Model(inputs, out2)

    inputs = layers.Input(shape=(input_dim,))
    x = TransformerEncoderBlock(input_dim, num_heads=8, ff_dim=512)(inputs)
    x = layers.Dense(512, activation='relu')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.4)(x)
    x = layers.Dense(256, activation='relu')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.3)(x)
    outputs = layers.Dense(len(np.unique(y)), activation='softmax')(x)

    model = models.Model(inputs, outputs)
    return model

transformer_model = build_advanced_transformer(X_train.shape[1])
transformer_model.compile(optimizer=optimizers.Adam(learning_rate=0.0003),
                          loss='sparse_categorical_crossentropy', metrics=['accuracy'])
transformer_model.fit(X_train, y_train, validation_data=(X_test, y_test),
                      epochs=400, batch_size=128,
                      callbacks=[
                          callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1),
                          callbacks.ReduceLROnPlateau(monitor='val_loss', patience=10, factor=0.5, verbose=1)
                      ], verbose=1)
y_pred_transformer_probs = transformer_model.predict(X_test)

# ✅ Build and Train Improved N-BEATS
def build_improved_nbeats(input_dim):
    def ImprovedNBeatsBlock(input_dim, hidden_dim=256):
        x = layers.Input(shape=(input_dim,))
        y = layers.Dense(hidden_dim, activation='relu')(x)
        y = layers.BatchNormalization()(y)
        y = layers.Dropout(0.3)(y)
        y = layers.Dense(hidden_dim, activation='relu')(y)
        y = layers.BatchNormalization()(y)
        y = layers.Dense(input_dim, activation='linear')(y)
        return models.Model(inputs=x, outputs=y)

    input_layer = layers.Input(shape=(input_dim,))
    x = ImprovedNBeatsBlock(input_dim)(input_layer)
    x = ImprovedNBeatsBlock(input_dim)(x)
    x = ImprovedNBeatsBlock(input_dim)(x)
    x = layers.Dense(256, activation='relu')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.4)(x)
    x = layers.Dense(128, activation='relu')(x)
    output_layer = layers.Dense(len(np.unique(y)), activation='softmax')(x)
    model = models.Model(inputs=input_layer, outputs=output_layer)
    return model

nbeats_model = build_improved_nbeats(X_train.shape[1])
nbeats_model.compile(optimizer=optimizers.Adam(learning_rate=0.0005),
                     loss='sparse_categorical_crossentropy', metrics=['accuracy'])
nbeats_model.fit(X_train, y_train, validation_data=(X_test, y_test),
                 epochs=400, batch_size=128,
                 callbacks=[
                     callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1),
                     callbacks.ReduceLROnPlateau(monitor='val_loss', patience=10, factor=0.5, verbose=1)
                 ], verbose=1)
y_pred_nbeats_probs = nbeats_model.predict(X_test)

# ✅ Train the best tuned XGBoost (optuna params manually given)
best_xgb_params = {
    'n_estimators': 375,
    'max_depth': 10,
    'learning_rate': 0.25277271258993694,
    'subsample': 0.9110499713882658,
    'colsample_bytree': 0.7739441441525933,
    'gamma': 0.0027904051194214538,
    'reg_lambda': 1.2336444977023753,
    'objective': 'multi:softprob',  # IMPORTANT for probability output
    'num_class': len(np.unique(y)),
    'eval_metric': 'mlogloss',
    'use_label_encoder': False,
    'verbosity': 0
}
xgb_model = XGBClassifier(**best_xgb_params)
xgb_model.fit(X_train, y_train)
y_pred_xgb_probs = xgb_model.predict_proba(X_test)

# ✅ Weighted Ensemble
ensemble_probs = (
    (0.5 * y_pred_transformer_probs) +
    (0.3 * y_pred_nbeats_probs) +
    (0.2 * y_pred_xgb_probs)
)
y_pred_ensemble = np.argmax(ensemble_probs, axis=1)

# ✅ Evaluate Ensemble
ensemble_accuracy = accuracy_score(y_test, y_pred_ensemble)
print(f"\n✅ Final Ensemble Classifier Accuracy: {ensemble_accuracy:.4f}")

print("\n📋 Final Ensemble Classification Report:")
print(classification_report(y_test, y_pred_ensemble))

# ✅ Confusion Matrix
plt.figure(figsize=(8,6))
cm = confusion_matrix(y_test, y_pred_ensemble)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
plt.xlabel('Predicted Class')
plt.ylabel('Actual Class')
plt.title('📊 Final Ensemble Model Confusion Matrix')
plt.tight_layout()
plt.show()

"""## **Final Ensemble Model — Performance Analysis**

This ensemble model achieves a **strong final test accuracy of 73.65%**, which is the **highest among all previously analyzed models** — signaling robust predictive capability after integration of Transformer, N-BEATS, and XGBoost.

####  **Key Observations:**

- **Class 0 (Rare Events)**:  
  - Precision: 0.65  
  - Recall: 0.22  
  - F1-Score: 0.33  
   Still remains the weakest class. The model detects some rare events but misses most due to class imbalance and subtle patterns.

- **Core Classes (1–4)**:  
  - **Class 1**: High precision (0.73) and recall (0.77). Most frequently occurring pattern, and the model learned it well.  
  - **Class 2**: Strong balance (F1 = 0.73), meaning both long and short-term directional shifts are well captured.  
  - **Class 3 & 4**: Consistently high performance, especially for class 3 with 12,400 true positives.

- **Macro Avg (Unweighted)**:  
  - Precision: 0.73  
  - Recall: 0.63  
  - F1-score: 0.65  
   Shows the model handles imbalance moderately well, but could still improve recall on underrepresented classes.

- **Weighted Avg (Support-Adjusted)**:  
  - All metrics ~0.74  
   Shows stability across class volumes.

#### **Confusion Matrix Insights:**

- Clear diagonal dominance, especially for classes 1, 3, and 4 — the model rarely misclassifies those.
- Class 0 is often confused with 1 and 3 — likely due to visual or structural overlap in short-term price reactions.
- Class 2 is fairly stable, though there's leakage into 3 (which can happen when reversals fade into continuation).

---

###  **Summary:**
> This final ensemble model represents the strongest version so far. It combines the contextual understanding of the Transformer, the deep time modeling of N-BEATS, and the structured learning of XGBoost.

Though rare event detection (class 0) still suffers, the model handles dominant classes with impressive consistency — ideal for forward testing and live validation.

---

# **Backtesting the Ensemble Models using Backtrader**
"""

!pip install backtrader xgboost

# ✅ Imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm

# ✅ Assume df is already loaded and you already have:
# transformer_model, nbeats_model, xgb_model, scaler, rfe_features, ensemble_weights

# ✅ Prepare features properly
X_scaled = scaler.transform(df[rfe_features])

# ✅ Prepare inputs
X_lstm = X_scaled.reshape(X_scaled.shape[0], 1, X_scaled.shape[1])

# ✅ Predict from models
transformer_probs = transformer_model.predict(X_scaled, verbose=0)
nbeats_probs = nbeats_model.predict(X_scaled, verbose=0)
xgb_probs = xgb_model.predict_proba(X_scaled)

# ✅ Weighted Ensemble
ensemble_probs = (
    (ensemble_weights['transformer'] * transformer_probs) +
    (ensemble_weights['nbeats'] * nbeats_probs) +
    (ensemble_weights['xgboost'] * xgb_probs)
)

predicted_classes = np.argmax(ensemble_probs, axis=1)
confidences = np.max(ensemble_probs, axis=1)

# ✅ Backtesting Parameters
lookahead = 5
account_balance = 100000
equity_curve = [account_balance]
trade_returns = []

# Risk Management
risk_per_trade = 0.01  # Risk 1% per trade
confidence_threshold = 0.55  # Ignore weak signals
min_atr = 50
max_atr = 1500

# ✅ Make sure ATR exists
if 'ATR' not in df.columns:
    df['ATR'] = df['High'].rolling(14).max() - df['Low'].rolling(14).min()
    df = df.dropna().reset_index(drop=True)

print("🔁 Running FINAL corrected backtest with strong risk management...")
for i in tqdm(range(len(df) - lookahead)):
    pred = predicted_classes[i]
    confidence = confidences[i]
    atr = df.loc[i, 'ATR']

    if confidence < confidence_threshold or atr < min_atr or atr > max_atr:
        continue

    entry_price = df.loc[i, "Open"]
    sl_distance = atr * 1.5  # Stop loss distance in $ terms

    risk_amount = account_balance * risk_per_trade
    pip_value = entry_price / 10000  # BTC/USD pip value estimate

    if pip_value == 0:
        continue

    lot_size = risk_amount / (sl_distance * pip_value)
    lot_size = min(lot_size, 2.0)  # Cap position size to 2 lots

    # Future candles
    future = df.loc[i+1:i+lookahead]
    high, low, close = future['High'].max(), future['Low'].min(), future.iloc[-1]['Close']

    pnl = 0

    if pred in [1, 4]:  # BUY signal
        up = (high - entry_price) * 10000
        down = (entry_price - low) * 10000
        if down >= sl_distance * 10000:
            pnl -= risk_amount
        elif up >= sl_distance * 2 * 10000:
            pnl += risk_amount * 2
        else:
            pnl += (close - entry_price) * lot_size * 10000

    elif pred in [2, 3]:  # SELL signal
        down = (entry_price - low) * 10000
        up = (high - entry_price) * 10000
        if up >= sl_distance * 10000:
            pnl -= risk_amount
        elif down >= sl_distance * 2 * 10000:
            pnl += risk_amount * 2
        else:
            pnl += (entry_price - close) * lot_size * 10000

    account_balance += pnl
    equity_curve.append(account_balance)
    trade_returns.append(pnl)

# ✅ Final Metrics
returns = np.array(trade_returns)
win_rate = np.sum(returns > 0) / len(returns) if len(returns) else 0
cum_returns = np.cumsum(returns)
max_dd = np.max(np.maximum.accumulate(cum_returns) - cum_returns)
sharpe = (np.mean(returns) / np.std(returns)) * np.sqrt(252) if np.std(returns) else 0

print("\nBacktest Results:")
print(f"Total Trades: {len(returns)}")
print(f"Win Rate: {win_rate:.2%}")

"""The backtesting results display a very promising result.

A win rate of 82% indicates a very good potential perfomance. Especially if coupled with good rik management.

#### We will save the models in preparation for forward testing and later deployment

# **Saving the ensemble Models**
"""

# ✅ First, saving everything again properly
import json
import joblib
import os
import pickle
import shutil
from google.colab import files

# ✅ Save directory
save_dir = '/content/drive/MyDrive/BTC_Project/Models/'
os.makedirs(save_dir, exist_ok=True)

# ✅ Save Transformer and N-BEATS models
transformer_model.save(os.path.join(save_dir, 'transformer_model.h5'))
nbeats_model.save(os.path.join(save_dir, 'nbeats_model.h5'))

# ✅ Save XGBoost model as JSON
xgb_model.save_model(os.path.join(save_dir, 'xgboost_model.json'))

# ✅ Save scaler manually to JSON
scaler_json = {
    'mean_': scaler.mean_.tolist(),
    'scale_': scaler.scale_.tolist(),
    'var_': scaler.var_.tolist(),
    'n_features_in_': int(scaler.n_features_in_),
    'feature_names_in_': list(scaler.feature_names_in_) if hasattr(scaler, 'feature_names_in_') else None
}
with open(os.path.join(save_dir, 'scaler.json'), 'w') as f:
    json.dump(scaler_json, f)

# ✅ Save rfe_features
with open(os.path.join(save_dir, 'rfe_features.json'), 'w') as f:
    json.dump(rfe_features, f)

# ✅ Save ensemble weights
ensemble_weights = {'transformer': 0.5, 'nbeats': 0.3, 'xgboost': 0.2}
with open(os.path.join(save_dir, 'ensemble_weights.json'), 'w') as f:
    json.dump(ensemble_weights, f)

print("\n✅ All models and preprocessors successfully saved to Drive.")

# ================================
# ✅ Now ZIP everything to prepare download
# ================================
zip_filename = '/content/models_backup.zip'
shutil.make_archive('/content/models_backup', 'zip', save_dir)

print("\n✅ Successfully zipped all models into models_backup.zip.")

# ================================
# ✅ Initiate download to local machine
# ================================
files.download(zip_filename)

"""# **Resaving our best Models in the right format**

## **Tuned XGBOOST With Feature Selection**
"""

# ✅ Imports
import optuna
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder
from google.colab import drive
import os

# ✅ Mount Google Drive
drive.mount('/content/drive')

# ✅ Load your data (assuming df and rfe_features are already loaded)
X = df[rfe_features]
y = df['target_class']

# Encode if needed
if y.dtype == 'object':
    le = LabelEncoder()
    y = le.fit_transform(y)

# ✅ Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, test_size=0.2, random_state=42
)

# ✅ Best Optuna parameters with correct objective
best_params = {
    'n_estimators': 375,
    'max_depth': 10,
    'learning_rate': 0.25277271258993694,
    'subsample': 0.9110499713882658,
    'colsample_bytree': 0.7739441441525933,
    'gamma': 0.0027904051194214538,
    'reg_lambda': 1.2336444977023753,
    'objective': 'multi:softprob',  # <-- IMPORTANT
    'num_class': len(np.unique(y)),
    'eval_metric': 'mlogloss',
    'use_label_encoder': False,
    'verbosity': 0
}

# ✅ Train model
best_model = XGBClassifier(**best_params)
best_model.fit(X_train, y_train)

# ✅ Predict probabilities and classes
y_proba = best_model.predict_proba(X_test)  # <-- PROBABILITIES
y_pred = np.argmax(y_proba, axis=1)

# ✅ Evaluate
print(f"\n Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print("\n Classification Report:")
print(classification_report(y_test, y_pred))

# ✅ Confusion matrix
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.title("Confusion Matrix: XGBoost (softprob)")
plt.xlabel("Predicted Class")
plt.ylabel("Actual Class")
plt.tight_layout()
plt.show()
"""
# ✅ Save model to Google Drive and local download
drive_path = "/content/drive/MyDrive/BTC_Project/newmodels"
os.makedirs(drive_path, exist_ok=True)
model_drive_path = os.path.join(drive_path, "xgboost_model.json")
best_model.save_model(model_drive_path)
print(f"✅ Model saved to: {model_drive_path}")

# ✅ Also download locally
local_path = "/content/xgboost_model.json"
best_model.save_model(local_path)

from google.colab import files
files.download(local_path)
"""

"""## **Checking for Overfitting in the Tuned XGBOOST Model**"""

# ✅ Imports
import numpy as np
import pandas as pd
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report

# ✅ Load data
df = pd.read_csv("/content/drive/MyDrive/BTC_Project/Data/BTCUSD CLEAN DATA FOR MODELING.csv")

# ✅ Create features and target
X = df[rfe_features]
y = df['target_class']

# ✅ Encode target if needed
if y.dtype == 'object':
    le = LabelEncoder()
    y = le.fit_transform(y)

# ✅ Recreate the same train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, test_size=0.2, random_state=42
)

# ✅ Load the saved XGBoost model
model_path = "/content/drive/MyDrive/BTC_Project/newmodels/xgboost_model.json"
loaded_model = XGBClassifier()
loaded_model.load_model(model_path)

# ✅ Predict on training set
y_train_proba = loaded_model.predict_proba(X_train)
y_train_pred = np.argmax(y_train_proba, axis=1)

# ✅ Evaluate training performance
print(f"\n Training Accuracy: {accuracy_score(y_train, y_train_pred):.4f}")
print("\n Training Classification Report:")
print(classification_report(y_train, y_train_pred))

"""## **Observation**

The tuned XGBoost is overffitted due to the huge difference between the test accuracy of 70.30% and the Training accuracy of 98.33%.

We will proceed to adjust inorder to fix the overfitting.

## **Overfitting-Resistant XGBoost Optimization with Optuna**
"""

!pip install optuna

!pip install xgboost==1.7.5

import optuna
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, log_loss
from google.colab import drive
import os

# Mount Google Drive
drive.mount('/content/drive')

# Load dataset
df = pd.read_csv("/content/drive/MyDrive/BTC_Project/Data/BTCUSD CLEAN DATA FOR MODELING.csv")

# Prepare features and target
X = df[rfe_features]
y = df['target_class']
if y.dtype == 'object':
    le = LabelEncoder()
    y = le.fit_transform(y)

# Train-test split (for final evaluation)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, test_size=0.2, random_state=42
)

# Optuna objective with early stopping
def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 500),
        'max_depth': trial.suggest_int('max_depth', 3, 8),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),
        'subsample': trial.suggest_float('subsample', 0.7, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),
        'gamma': trial.suggest_float('gamma', 0, 5),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.5, 5),
        'reg_alpha': trial.suggest_float('reg_alpha', 0, 2),
        'objective': 'multi:softprob',
        'num_class': len(np.unique(y)),
        'use_label_encoder': False,
        'verbosity': 0
    }

    model = XGBClassifier(**params)

    # hold out 20% of training data for early stopping
    X_tr, X_val, y_tr, y_val = train_test_split(
        X_train, y_train, test_size=0.2, stratify=y_train, random_state=42
    )

    model.fit(
        X_tr, y_tr,
        eval_set=[(X_val, y_val)],
        early_stopping_rounds=20,
        eval_metric='mlogloss',
        verbose=False
    )

    y_val_proba = model.predict_proba(X_val)
    return log_loss(y_val, y_val_proba)

# Run Optuna
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=50)

print("Best parameters:", study.best_params)

# Train final model on full training set with best parameters
final_model = XGBClassifier(**study.best_params)
final_model.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    early_stopping_rounds=20,
    eval_metric='mlogloss',
    verbose=False
)

# Evaluate on training set
y_train_proba = final_model.predict_proba(X_train)
y_train_pred = np.argmax(y_train_proba, axis=1)
train_acc = accuracy_score(y_train, y_train_pred)
print(f"\nTraining Accuracy: {train_acc:.4f}")
print("Training Classification Report:")
print(classification_report(y_train, y_train_pred))

# Evaluate on test set
y_test_proba = final_model.predict_proba(X_test)
y_test_pred = np.argmax(y_test_proba, axis=1)
test_acc = accuracy_score(y_test, y_test_pred)
print(f"\nTest Accuracy: {test_acc:.4f}")
print("Test Classification Report:")
print(classification_report(y_test, y_test_pred))

# Confusion matrix for test set
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_test_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.title("Confusion Matrix: Test Set")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()

"""We’ve successfully pulled the training accuracy down from ~98 % to ~82 %, while the test accuracy sits at ~67.6 %.

That gives you a gap of about 15 pp (82 − 67.6), versus ~28 pp before—so we’re definitely reducing over-fit.
"""

save_path = "/content/drive/MyDrive/BTC_Project/newmodels"
os.makedirs(save_path, exist_ok=True)
final_model_path = os.path.join(save_path, "xgboost_model_tuned.json")
final_model.save_model(final_model_path)
print(f"Final model saved to: {final_model_path}")

"""## **Advanced N-beats Model**"""

#  Import libraries
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive
import os

#  Mount Google Drive
#drive.mount('/content/drive')

#  Prepare data
X = df[rfe_features]
y = df['target_class']

# Encode target
if y.dtype == 'object':
    le = LabelEncoder()
    y = le.fit_transform(y)

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, stratify=y, random_state=42
)

#  Improved N-BEATS block
def ImprovedNBeatsBlock(input_dim, hidden_dim=256):
    x = layers.Input(shape=(input_dim,))
    y = layers.Dense(hidden_dim, activation='relu')(x)
    y = layers.BatchNormalization()(y)
    y = layers.Dropout(0.3)(y)
    y = layers.Dense(hidden_dim, activation='relu')(y)
    y = layers.BatchNormalization()(y)
    y = layers.Dense(input_dim, activation='linear')(y)
    return models.Model(inputs=x, outputs=y)

# ✅ Build the model
input_layer = layers.Input(shape=(X_train.shape[1],))
x = ImprovedNBeatsBlock(X_train.shape[1])(input_layer)
x = ImprovedNBeatsBlock(X_train.shape[1])(x)
x = ImprovedNBeatsBlock(X_train.shape[1])(x)
x = layers.Dense(256, activation='relu')(x)
x = layers.BatchNormalization()(x)
x = layers.Dropout(0.4)(x)
x = layers.Dense(128, activation='relu')(x)
output_layer = layers.Dense(len(np.unique(y)), activation='softmax')(x)

model = models.Model(inputs=input_layer, outputs=output_layer)

# ✅ Compile
model.compile(
    optimizer=optimizers.Adam(learning_rate=0.0005),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# ✅ Callbacks
early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)
reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', patience=10, factor=0.5, verbose=1)

# ✅ Train
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=400,
    batch_size=128,
    callbacks=[early_stop, reduce_lr],
    verbose=1
)

# ✅ Evaluate
y_pred = np.argmax(model.predict(X_test), axis=1)
accuracy = accuracy_score(y_test, y_pred)
print(f"\n Final Accuracy: {accuracy:.4f}")
print("\n Classification Report:")
print(classification_report(y_test, y_pred))

#  Confusion Matrix
plt.figure(figsize=(7,6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.xlabel('Predicted Class')
plt.ylabel('Actual Class')
plt.title('Improved N-BEATS Classifier Confusion Matrix')
plt.tight_layout()
plt.show()

#  Save model to Google Drive
drive_path = "/content/drive/MyDrive/BTC_Project/newmodels"
os.makedirs(drive_path, exist_ok=True)
model.save(os.path.join(drive_path, "nbeats_model.keras"))
print(f" Model saved to Google Drive at: {drive_path}/nbeats_model.keras")

# Save for local download
model.save("nbeats_model.keras")

from google.colab import files
files.download("nbeats_model.keras")

"""## **Checking for Overfitting in the N-beats Model**"""

import os
import json
import joblib
import numpy as np
import pandas as pd
from tensorflow import keras
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# --- CONFIGURATION ---
DRIVE_MODEL_PATH = "/content/drive/MyDrive/BTC_Project/newmodels/nbeats_model.keras"
DATA_PATH        = "/content/drive/MyDrive/BTC_Project/Data/BTCUSD CLEAN DATA FOR MODELING.csv"
FEATURES_PATH    = "/content/drive/MyDrive/BTC_Project/newmodels/rfe_features.json"
SCALER_PATH      = "/content/drive/MyDrive/BTC_Project/newmodels/scaler.pkl"
TEST_SIZE        = 0.2
RANDOM_STATE     = 42

# 1. Load data and RFE feature list
df = pd.read_csv(DATA_PATH)
with open(FEATURES_PATH, "r") as f:
    rfe_features = json.load(f)

X = df[rfe_features]
y = df["target_class"]

# 2. Encode target if needed
if y.dtype == "object":
    le = LabelEncoder()
    y = le.fit_transform(y)

# 3. Load the same scaler used during training
scaler = joblib.load(SCALER_PATH)
X_scaled = scaler.transform(X)

# 4. Re-create the train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=TEST_SIZE,
    stratify=y, random_state=RANDOM_STATE
)

# 5. Load your saved N-BEATS Keras model
model = keras.models.load_model(DRIVE_MODEL_PATH)

# 6. Evaluate on the **training** set
y_train_pred = np.argmax(model.predict(X_train), axis=1)
train_acc   = accuracy_score(y_train, y_train_pred)
print(f"Training Accuracy: {train_acc:.4f}")
print("\nTraining Classification Report:")
print(classification_report(y_train, y_train_pred, digits=4))

# 7. Evaluate on the **test** set (for comparison)
y_test_pred = np.argmax(model.predict(X_test), axis=1)
test_acc    = accuracy_score(y_test, y_test_pred)
print(f"\nTest Accuracy: {test_acc:.4f}")
print("\nTest Classification Report:")
print(classification_report(y_test, y_test_pred, digits=4))

"""Training Accuracy: 0.7069

Test Accuracy: 0.7004

The near-parity of these two numbers (a gap of only ~0.6 percentage points) indicates that the model is not grossly overfitting.

Both precision/recall across classes are very similar between train and test.

Conclusion:

The model can be processed further as is.

# **Advanced Transformer Model**
"""

# ✅ Import libraries
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import os

# ✅ Setup save path
SAVE_DIR = "/content/drive/MyDrive/BTC_Project/newmodels"
os.makedirs(SAVE_DIR, exist_ok=True)

# ✅ Prepare your data (assuming df and rfe_features are loaded)
X = df[rfe_features]
y = df['target_class']

# Encode target if needed
if y.dtype == 'object':
    le = LabelEncoder()
    y = le.fit_transform(y)

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ✅ Save scaler
scaler_path = os.path.join(SAVE_DIR, "scaler.pkl")
joblib.dump(scaler, scaler_path)
print(f"✅ Scaler saved to: {scaler_path}")

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, stratify=y, random_state=42
)

# ✅ Define Transformer Encoder Block
def TransformerEncoderBlock(embed_dim, num_heads=8, ff_dim=512, rate=0.1):
    inputs = layers.Input(shape=(embed_dim,))
    x = layers.Reshape((1, embed_dim))(inputs)

    attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x, x)
    attn_output = layers.Dropout(rate)(attn_output)
    out1 = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)

    ffn = layers.Dense(ff_dim, activation='relu')(out1)
    ffn = layers.Dense(embed_dim)(ffn)
    ffn = layers.Dropout(rate)(ffn)
    out2 = layers.LayerNormalization(epsilon=1e-6)(out1 + ffn)

    out2 = layers.Flatten()(out2)
    return models.Model(inputs, out2)

# ✅ Build the Advanced Transformer Classifier
embed_dim = X_train.shape[1]
inputs = layers.Input(shape=(embed_dim,))
x = TransformerEncoderBlock(embed_dim)(inputs)
x = layers.Dense(512, activation='relu')(x)
x = layers.BatchNormalization()(x)
x = layers.Dropout(0.4)(x)
x = layers.Dense(256, activation='relu')(x)
x = layers.BatchNormalization()(x)
x = layers.Dropout(0.3)(x)
outputs = layers.Dense(len(np.unique(y)), activation='softmax')(x)

model = models.Model(inputs, outputs)

# ✅ Compile
model.compile(
    optimizer=optimizers.Adam(learning_rate=0.0003),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# ✅ Callbacks
early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)
reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', patience=10, factor=0.5, verbose=1)

# ✅ Train
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=400,
    batch_size=128,
    callbacks=[early_stop, reduce_lr],
    verbose=1
)

# ✅ Save model
model_path = os.path.join(SAVE_DIR, "transformer_model.keras")
model.save(model_path)
print(f"✅ Transformer model saved to: {model_path}")

# ✅ Evaluate
y_pred = np.argmax(model.predict(X_test), axis=1)
accuracy = accuracy_score(y_test, y_pred)
print(f"\n✅ Accuracy: {accuracy:.4f}")
print("\n📋 Classification Report:")
print(classification_report(y_test, y_pred))

# ✅ Confusion Matrix
plt.figure(figsize=(7,6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.xlabel('Predicted Class')
plt.ylabel('Actual Class')
plt.title('Advanced Transformer Classifier Confusion Matrix')
plt.tight_layout()
plt.show()



"""# **Checking for Overfitting in the Advanced Transformer model**"""

import json
import joblib
import numpy as np
import pandas as pd
from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ─── CONFIG ────────────────────────────────────────────────────────────────────
BASE_DIR      = "/content/drive/MyDrive/BTC_Project/newmodels"
SCALER_PATH   = f"{BASE_DIR}/scaler.pkl"
FEATURES_PATH = f"{BASE_DIR}/rfe_features.json"
MODEL_PATH    = f"{BASE_DIR}/transformer_model.keras"
DATA_PATH     = "/content/drive/MyDrive/BTC_Project/Data/BTCUSD CLEAN DATA FOR MODELING.csv"
# ────────────────────────────────────────────────────────────────────────────────

# 1. Load raw data
df = pd.read_csv(DATA_PATH)

# 2. Load RFE feature list
with open(FEATURES_PATH, "r") as f:
    features = json.load(f)

# 3. Prepare X and y
X = df[features]
y = df["target_class"]
if y.dtype == "object":
    # if you ever encoded as strings
    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    y = le.fit_transform(y)

# 4. Load and apply scaler
scaler = joblib.load(SCALER_PATH)
X_scaled = scaler.transform(X)

# 5. Recreate train/test split (must match your original random_state & test_size)
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, stratify=y, random_state=42
)

# 6. Load your saved Transformer model
model = keras.models.load_model(MODEL_PATH)

# 7. Inference only: no .fit() calls here
y_train_pred = np.argmax(model.predict(X_train), axis=1)
y_test_pred  = np.argmax(model.predict(X_test ), axis=1)

# 8. Compute metrics
train_acc = accuracy_score(y_train, y_train_pred)
test_acc  = accuracy_score(y_test,  y_test_pred)
print(f"Training Accuracy: {train_acc:.4f}")
print(classification_report(y_train, y_train_pred))
print(f"\nTest Accuracy:     {test_acc:.4f}")
print(classification_report(y_test,  y_test_pred))

# 9. (Optional) Confusion matrix
import matplotlib.pyplot as plt
import seaborn as sns
cm = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(6,6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.title("Transformer: Test Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()



"""The Transformer shows solid generalization:

* **Training vs. Test Accuracy**
  • Training: 73.24%
  • Test:     72.32%

A gap of less than 1% indicates minimal overfitting—the model is learning real patterns, not memorizing noise.

* **Per-Class Performance**
  • Classes 1–4 all achieve \~70–75% precision and recall on both sets.
  • Class 0 (neutral) remains the toughest: recall ≈20–22%, suggesting fewer or noisier examples for “neutral” moves.

**Bottom Line**
The Transformer is well‐balanced: it generalizes almost as well on unseen data as it does on its training set.

# **Final Conclusions and Recommendations**

**Executive Summary**

We’ve engineered, trained, and deployed a sophisticated, multi-model AI trading system for BTC/USD.

Rigorous data analysis, feature design, and model optimization have yielded strong predictive power, and our automated execution logic ensures disciplined risk management.
 Below, weI present a concise, high-impact review of our findings, followed by clear action items to elevate performance further.

---

## 1. Data Insights & Feature Engineering

* **Class Distribution & Market Regimes**

  * *Imbalance:* Neutral periods (class 0) represent only \~2% of observations, while directional moves dominate.

  * *Volatility Clustering:* Price swings follow volatility clusters—calm stretches punctuated by bursts. Our handcrafted volatility bands and momentum indicators capture these regimes effectively.

* **Temporal Lead Structure**

  * Price trends persist for 3–7 fifteen-minute bars before mean-reverting, validating our 5-bar lookahead and hybrid bullish/bearish classification.

* **Correlation Patterns**

  * Strong positive correlations among short-term momentum features; moderate negative correlations between short vs. long-term volatility measures. This suggests combining cross-scale signals (e.g., divergence between M15 and H4 vol) adds unique predictive value.

---

## 2. Model Performance & Robustness

| Model            | Train Accuracy | Test Accuracy | Train–Test Gap |
| ---------------- | -------------- | ------------- | -------------- |
| XGBoost          | 82%          | 67.8%         | 28.0 pp        |
| Transformer      | 73.2%          | 72.3%         | 0.9 pp         |
| Improved N-BEATS | 70.7%          | 70.0%         | 0.7 pp         |
| **Ensemble**     | —              | **\~73%**     | —              |

**Generalization**

  * Transformer and N-BEATS show minimal overfitting (<1 pp gap). XGBoost exhibits higher training accuracy but moderate test performance, indicating potential model complexity—although its soft-prob outputs enrich our ensemble.

* **Neutral Class**

  * Recall on “no-move” windows remains below 20%, leading to occasional entries in choppy markets. Addressing this gap is critical to reducing false signals and whipsaw trades.

---

## 3. Trading Infrastructure & Execution

* **Entry Filters**

  * **Spread Control:** Enforced a 70 USD maximum spread for BTC/USD, avoiding shallow liquidity periods.

  * **Cooldown & Position Limits:** 15-minute cooldown and up to five concurrent trades preserve capital and prevent overexposure.

* **Stop Loss & Take Profit**

  * Fixed USD stops (±1 000 USD) align with typical BTC range activity. In live testing, we confirm FOK filling mode is required; IOC and RETURN modes are unsupported by our broker.

  * **Partial Exit Logic:** TP1 auto-close and SL-breakeven rules executed by `live_monitor.py` lock in gains and curtail run-on losses.

* **Logging & Monitoring**

  * Unified `utils/logger.py` maintains a single JSON log, capturing both entry and exit details—timestamps, PnL, TP/SL flags—for every trade.
  * Streamlit dashboards render cumulative PnL curves, outcome breakdowns, and directional performance in real time.

---

## 4. Strategic Roadmap & Recommendations

1. **Enhance Neutral-Move Detection**

   * **Data Augmentation:** Synthetically oversample flat regimes to bolster class 0 representation.
   * **Feature Innovation:** Integrate order-book imbalance, micro-structure (tick-level) volatility, or sentiment indicators to distinguish quiet markets.

2. **Adaptive Risk Management**

   * **Volatility-Based Stops:** Replace fixed USD stops with ATR- or percentile-based bands that scale to changing market volatility.
   * **Dynamic Position Sizing:** Implement Kelly-criterion or fixed-fractional sizing to allocate risk proportionally to account equity.

3. **Model Ensemble Evolution**

   * **Periodic Re-Calibration:** Automate monthly retraining and Optuna re-optimization to adapt ensemble weights and hyperparameters to fresh data.
   * **Confidence-Weighted Execution:** Scale lot sizes based on ensemble confidence, allowing stronger signals to carry greater weight.

4. **Operational Resilience**

   * **Broker Constraints Automation:** At startup, query `symbol_info()` for `stops_level`, `min_volume`, and supported filling modes to auto-configure SL/TP and order parameters.
   * **Health-Check Alerts:** Build automated monitoring for MT5 connectivity, unexpected spread spikes, and log-write failures, with real-time notifications via Slack or email.

5. **Comprehensive Backtesting & Simulation**

   * Develop a dedicated `utils/backtester.py` that simulates historical trades—including slippage, commission, and spread—for rigorous performance validation before live deployment.

6. **Scalable Deployment**

   * **Containerization:** Dockerize the entire stack (Python environment, MT5 bridge, models, dashboard) for seamless VPS or cloud deployment.
   * **CI/CD Pipeline:** Incorporate pre-deployment checks (MT5 connectivity, model load, log integrity) and automatic updates to production.

---

**Conclusion**

Our BTC/USD AI trading framework is operationally solid: advanced feature engineering, high-quality ensemble models, disciplined execution logic, and real-time monitoring. By sharpening neutral-move detection, embracing adaptive risk sizing, and automating continuous learning, we will unlock additional alpha and reduce drawdowns creating a world class algo.
"""

